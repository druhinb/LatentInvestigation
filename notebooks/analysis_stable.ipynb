{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Analysis: 3D Understanding in Vision Transformers\n",
    "\n",
    "This notebook provides a comprehensive analysis of our interpretability experiments on Vision Transformers (DINOv2 and I-JEPA) for 3D understanding tasks:\n",
    "- **3D Voxel Reconstruction**: How well can different layers reconstruct 3D shape from 2D views?\n",
    "- **Viewpoint Estimation**: How accurately can models predict camera viewpoint parameters?\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Loading](#1-setup-and-data-loading)\n",
    "2. [Voxel Reconstruction Analysis](#2-voxel-reconstruction-analysis)\n",
    "3. [Viewpoint Estimation Analysis](#3-viewpoint-estimation-analysis)\n",
    "4. [Advanced Feature Analysis](#4-advanced-feature-analysis)\n",
    "5. [Synthesis and Conclusions](#5-synthesis-and-conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import timm  # Added import\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Project imports\n",
    "from src.probing.probes import create_probe, VoxelProbe, LinearProbe, MLPProbe\n",
    "from src.datasets.shapenet_voxel_meshes import create_3dr2n2_reconstruction_dataloaders\n",
    "from src.datasets.shapenet_3dr2n2 import create_3dr2n2_dataloaders\n",
    "from src.models.model_loader import load_model_and_preprocessor\n",
    "from src.analysis.layer_analysis import LayerWiseAnalyzer\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Loading Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"Simple container for experiment results\"\"\"\n",
    "    name: str\n",
    "    model: str  # dinov2, ijepa, supervised_vit\n",
    "    task: str   # viewpoint_estimation, voxel_reconstruction\n",
    "    results: Dict[str, Any]\n",
    "    cache_dir: Path\n",
    "    results_dir: Path\n",
    "\n",
    "\n",
    "class SimpleExperimentLoader:\n",
    "    \"\"\"Simplified experiment results loader based on known directory structure\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = \".\"):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.cache_path = self.base_path / \"cache\"\n",
    "        self.results_path = self.base_path / \"results\"\n",
    "        \n",
    "    def load_all_experiments(self) -> Dict[str, ExperimentResult]:\n",
    "        \"\"\"Load all experiments from results directory\"\"\"\n",
    "        experiments = {}\n",
    "        \n",
    "        # Scan results directory for experiment folders\n",
    "        for results_dir in self.results_path.iterdir():\n",
    "            if not results_dir.is_dir() or results_dir.name.startswith('.'):\n",
    "                continue\n",
    "                \n",
    "            exp_name = results_dir.name\n",
    "            \n",
    "            # Parse experiment name to extract model and task\n",
    "            model, task = self._parse_experiment_name(exp_name)\n",
    "            \n",
    "            # Load results JSON\n",
    "            results_file = results_dir / \"results.json\"\n",
    "            if not results_file.exists():\n",
    "                print(f\"Warning: No results.json found for {exp_name}\")\n",
    "                continue\n",
    "                \n",
    "            with open(results_file, 'r') as f:\n",
    "                results_data = json.load(f)\n",
    "            \n",
    "            # Find corresponding cache directory\n",
    "            cache_dir = self._find_cache_dir(exp_name)\n",
    "            \n",
    "            experiments[exp_name] = ExperimentResult(\n",
    "                name=exp_name,\n",
    "                model=model,\n",
    "                task=task,\n",
    "                results=results_data,\n",
    "                cache_dir=cache_dir,\n",
    "                results_dir=results_dir\n",
    "            )\n",
    "            \n",
    "        return experiments\n",
    "    \n",
    "    def _parse_experiment_name(self, exp_name: str) -> Tuple[str, str]:\n",
    "        \"\"\"Extract model and task from experiment name using simple rules\"\"\"\n",
    "        # Extract model\n",
    "        if 'dinov2' in exp_name.lower():\n",
    "            model = 'dinov2'\n",
    "        elif 'ijepa' in exp_name.lower():\n",
    "            model = 'ijepa'\n",
    "        elif 'supervised' in exp_name.lower():\n",
    "            model = 'supervised_vit'\n",
    "        else:\n",
    "            model = 'unknown'\n",
    "            \n",
    "        # Extract task\n",
    "        if 'viewpoint' in exp_name.lower():\n",
    "            task = 'viewpoint_estimation'\n",
    "        elif 'voxel' in exp_name.lower() or 'reconstruction' in exp_name.lower():\n",
    "            task = 'voxel_reconstruction'\n",
    "        else:\n",
    "            task = 'unknown'\n",
    "            \n",
    "        return model, task\n",
    "    \n",
    "    def _find_cache_dir(self, exp_name: str) -> Path:\n",
    "        \"\"\"Find corresponding cache directory for experiment\"\"\"\n",
    "        # Try exact match first\n",
    "        cache_dir = self.cache_path / exp_name\n",
    "        if cache_dir.exists():\n",
    "            return cache_dir\n",
    "            \n",
    "        # Try alternative names for cache directories\n",
    "        alternatives = [\n",
    "            exp_name.replace('phase1_', '').replace('phase2_', ''),\n",
    "            f\"ijepa_{exp_name}\" if 'ijepa' in exp_name else None,\n",
    "            exp_name.replace('_probing', ''),\n",
    "            exp_name.replace('_reconstruction', '')\n",
    "        ]\n",
    "        \n",
    "        for alt_name in alternatives:\n",
    "            if alt_name and (self.cache_path / alt_name).exists():\n",
    "                return self.cache_path / alt_name\n",
    "                \n",
    "        # Return non-existent path if not found\n",
    "        return self.cache_path / exp_name\n",
    "    \n",
    "    def get_probe_files(self, experiment: ExperimentResult) -> Dict[str, Dict[int, Path]]:\n",
    "        \"\"\"Get probe files organized by type and layer\"\"\"\n",
    "        probes = {}\n",
    "        probes_dir = experiment.cache_dir / \"probes\"\n",
    "        \n",
    "        if not probes_dir.exists():\n",
    "            return probes\n",
    "            \n",
    "        for probe_file in probes_dir.glob(\"*.pth\"):\n",
    "            # Parse different filename patterns:\n",
    "            # viewpoint: linear_layer_11.pth, mlp_layer_2.pth\n",
    "            # voxel: dinov2_voxel_layer_11.pth\n",
    "            \n",
    "            filename = probe_file.stem\n",
    "            \n",
    "            if experiment.task == 'viewpoint_estimation':\n",
    "                # Format: {probe_type}_layer_{layer_num}\n",
    "                if '_layer_' in filename:\n",
    "                    parts = filename.split('_layer_')\n",
    "                    if len(parts) == 2 and parts[1].isdigit():\n",
    "                        probe_type = parts[0]  # linear or mlp\n",
    "                        layer_num = int(parts[1])\n",
    "                        \n",
    "                        if probe_type not in probes:\n",
    "                            probes[probe_type] = {}\n",
    "                        probes[probe_type][layer_num] = probe_file\n",
    "                        \n",
    "            elif experiment.task == 'voxel_reconstruction':\n",
    "                # Format: {model}_voxel_layer_{layer_num}\n",
    "                if '_voxel_layer_' in filename:\n",
    "                    layer_part = filename.split('_voxel_layer_')[-1]\n",
    "                    if layer_part.isdigit():\n",
    "                        probe_type = 'voxel'\n",
    "                        layer_num = int(layer_part)\n",
    "                        \n",
    "                        if probe_type not in probes:\n",
    "                            probes[probe_type] = {}\n",
    "                        probes[probe_type][layer_num] = probe_file\n",
    "            \n",
    "        return probes\n",
    "    \n",
    "    def get_feature_files(self, experiment: ExperimentResult) -> Dict[int, Dict[str, Path]]:\n",
    "        \"\"\"Get feature files organized by layer and split\"\"\"\n",
    "        features = {}\n",
    "        features_dir = experiment.cache_dir / \"features\"\n",
    "        \n",
    "        if not features_dir.exists():\n",
    "            return features\n",
    "            \n",
    "        for feature_file in features_dir.glob(\"*.pkl\"):\n",
    "            # Format: layer_{layer_num}_{split}.pkl\n",
    "            filename = feature_file.stem\n",
    "            if filename.startswith('layer_'):\n",
    "                parts = filename.split('_')\n",
    "                if len(parts) >= 3 and parts[1].isdigit():\n",
    "                    layer_num = int(parts[1])\n",
    "                    split = parts[2]  # train, val, test\n",
    "                    \n",
    "                    if layer_num not in features:\n",
    "                        features[layer_num] = {}\n",
    "                    features[layer_num][split] = feature_file\n",
    "                    \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all experiment data\n",
    "loader = SimpleExperimentLoader()\n",
    "experiments = loader.load_all_experiments()\n",
    "\n",
    "print(f\"Loaded {len(experiments)} experiments:\")\n",
    "for name, exp in experiments.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Model: {exp.model}\")\n",
    "    print(f\"  Task: {exp.task}\")\n",
    "    \n",
    "    # Get probe and feature information\n",
    "    probes = loader.get_probe_files(exp)\n",
    "    features = loader.get_feature_files(exp)\n",
    "    \n",
    "    print(f\"  Probe types: {list(probes.keys())}\")\n",
    "    print(f\"  Layers with features: {sorted(features.keys())}\")\n",
    "    if probes:\n",
    "        for probe_type, layers in probes.items():\n",
    "            print(f\"    {probe_type} probes: layers {sorted(layers.keys())}\")\n",
    "    if features:\n",
    "        for layer, splits in features.items():\n",
    "            print(f\"    Layer {layer}: {sorted(splits.keys())} splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Voxel Reconstruction Analysis\n",
    "\n",
    "We analyze how well different layers of DINOv2 can reconstruct 3D voxel representations from 2D images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_voxel_performance(experiment: ExperimentResult, loader: SimpleExperimentLoader):\n",
    "    \"\"\"Plot voxel reconstruction metrics across layers\"\"\"\n",
    "    # Extract metrics from results\n",
    "    metrics_dict = experiment.results.get('results', {})\n",
    "    \n",
    "    layers = []\n",
    "    iou_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    # Parse layer results\n",
    "    for layer_key, layer_data in metrics_dict.items():\n",
    "        if layer_key.startswith('layer_'):\n",
    "            layer_num = int(layer_key.split('_')[1])\n",
    "            \n",
    "            # Look for voxel probe results\n",
    "            if 'voxel' in layer_data:\n",
    "                test_metrics = layer_data['voxel'].get('test_metrics', {})\n",
    "                \n",
    "                layers.append(layer_num)\n",
    "                iou_scores.append(test_metrics.get('voxel_iou', 0))\n",
    "                precision_scores.append(test_metrics.get('voxel_precision', 0))\n",
    "                recall_scores.append(test_metrics.get('voxel_recall', 0))\n",
    "                f1_scores.append(test_metrics.get('voxel_f1', 0))\n",
    "    \n",
    "    # Sort by layer number\n",
    "    sorted_indices = np.argsort(layers)\n",
    "    layers = np.array(layers)[sorted_indices]\n",
    "    iou_scores = np.array(iou_scores)[sorted_indices]\n",
    "    precision_scores = np.array(precision_scores)[sorted_indices]\n",
    "    recall_scores = np.array(recall_scores)[sorted_indices]\n",
    "    f1_scores = np.array(f1_scores)[sorted_indices]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(f'Voxel Reconstruction Performance - {experiment.model.upper()}', fontsize=16)\n",
    "    \n",
    "    # Plot IoU\n",
    "    axes[0, 0].plot(layers, iou_scores, 'o-', linewidth=2, markersize=8, color='royalblue')\n",
    "    axes[0, 0].set_xlabel('Layer')\n",
    "    axes[0, 0].set_ylabel('IoU Score')\n",
    "    axes[0, 0].set_title('Intersection over Union')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_ylim([0, 1])\n",
    "    \n",
    "    # Plot Precision\n",
    "    axes[0, 1].plot(layers, precision_scores, 's-', linewidth=2, markersize=8, color='forestgreen')\n",
    "    axes[0, 1].set_xlabel('Layer')\n",
    "    axes[0, 1].set_ylabel('Precision')\n",
    "    axes[0, 1].set_title('Voxel Precision')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].set_ylim([0, 1])\n",
    "    \n",
    "    # Plot Recall\n",
    "    axes[1, 0].plot(layers, recall_scores, '^-', linewidth=2, markersize=8, color='darkorange')\n",
    "    axes[1, 0].set_xlabel('Layer')\n",
    "    axes[1, 0].set_ylabel('Recall')\n",
    "    axes[1, 0].set_title('Voxel Recall')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].set_ylim([0, 1])\n",
    "    \n",
    "    # Plot F1\n",
    "    axes[1, 1].plot(layers, f1_scores, 'd-', linewidth=2, markersize=8, color='crimson')\n",
    "    axes[1, 1].set_xlabel('Layer')\n",
    "    axes[1, 1].set_ylabel('F1 Score')\n",
    "    axes[1, 1].set_title('Voxel F1 Score')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].set_ylim([0, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best layer\n",
    "    if len(iou_scores) > 0:\n",
    "        best_layer_idx = np.argmax(iou_scores)\n",
    "        best_layer = layers[best_layer_idx]\n",
    "        print(f\"\\nBest performing layer: {best_layer}\")\n",
    "        print(f\"  IoU: {iou_scores[best_layer_idx]:.4f}\")\n",
    "        print(f\"  Precision: {precision_scores[best_layer_idx]:.4f}\")\n",
    "        print(f\"  Recall: {recall_scores[best_layer_idx]:.4f}\")\n",
    "        print(f\"  F1: {f1_scores[best_layer_idx]:.4f}\")\n",
    "        return best_layer\n",
    "    else:\n",
    "        print(\"No voxel reconstruction results found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze voxel reconstruction experiment\n",
    "voxel_exp = None\n",
    "for name, exp in experiments.items():\n",
    "    if exp.task == 'voxel_reconstruction' and exp.model == 'dinov2':\n",
    "        voxel_exp = exp\n",
    "        break\n",
    "\n",
    "if voxel_exp:\n",
    "    print(f\"Analyzing: {voxel_exp.name}\")\n",
    "    best_layer = plot_voxel_performance(voxel_exp, loader)\n",
    "else:\n",
    "    print(\"No DINOv2 voxel reconstruction experiment found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Qualitative Analysis - 3D Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_voxel_reconstruction(probe_file_path: str, features_file_path: str, \n",
    "                                   ground_truth_voxel_data: np.ndarray, \n",
    "                                   sample_idx: int = 0):\n",
    "    \"\"\"Visualize predicted vs ground truth voxels in 3D\"\"\"\n",
    "    # Load trained probe\n",
    "    probe_state = torch.load(probe_file_path, map_location=device)\n",
    "    \n",
    "    # Create probe model (assuming standard config)\n",
    "    probe_config = {\n",
    "        'type': 'voxel',\n",
    "        'input_dim': 18624, \n",
    "        'voxel_resolution': 32\n",
    "    }\n",
    "    probe = create_probe(probe_config)\n",
    "    probe.load_state_dict(probe_state[\"model_state_dict\"])\n",
    "    probe.to(device)\n",
    "    probe.eval()\n",
    "    \n",
    "    # Load features\n",
    "    with open(features_file_path, 'rb') as f:\n",
    "        features_data = pickle.load(f)\n",
    "    \n",
    "    features_data = features_data[\"view_data\"]\n",
    "        \n",
    "   \n",
    "    # Extract single sample\n",
    "    if isinstance(features_data, dict) and 'features' in features_data:\n",
    "        features = features_data['features'][sample_idx:sample_idx+1]\n",
    "    else:\n",
    "        features = features_data[sample_idx:sample_idx+1]\n",
    "    \n",
    "    features = torch.tensor(features).to(device)\n",
    "    features = features.view(features.size(0), -1)\n",
    "    \n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        pred_logits = probe(features)\n",
    "        pred_voxels = torch.sigmoid(pred_logits) > 0.5\n",
    "        pred_voxels = pred_voxels.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Create 3D visualization using plotly\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Ground truth voxels (blue)\n",
    "    gt_points = np.argwhere(ground_truth_voxel_data)\n",
    "    if len(gt_points) > 0:\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=gt_points[:, 0],\n",
    "            y=gt_points[:, 1],\n",
    "            z=gt_points[:, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(size=3, color='blue', opacity=0.6),\n",
    "            name='Ground Truth'\n",
    "        ))\n",
    "    \n",
    "    # Predicted voxels (red)\n",
    "    pred_points = np.argwhere(pred_voxels)\n",
    "    if len(pred_points) > 0:\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=pred_points[:, 0] + 35,  # Offset for side-by-side view\n",
    "            y=pred_points[:, 1],\n",
    "            z=pred_points[:, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(size=3, color='red', opacity=0.6),\n",
    "            name='Predicted'\n",
    "        ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Voxel Reconstruction - Sample {sample_idx}',\n",
    "        scene=dict(\n",
    "            xaxis=dict(range=[0, 70]),\n",
    "            yaxis=dict(range=[0, 32]),\n",
    "            zaxis=dict(range=[0, 32]),\n",
    "            aspectmode='data'\n",
    "        ),\n",
    "        width=900,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Calculate metrics for this sample\n",
    "    intersection = np.logical_and(ground_truth_voxel_data, pred_voxels).sum()\n",
    "    union = np.logical_or(ground_truth_voxel_data, pred_voxels).sum()\n",
    "    iou = intersection / (union + 1e-6)\n",
    "    \n",
    "    print(f\"\\nSample {sample_idx} metrics:\")\n",
    "    print(f\"  IoU: {iou:.4f}\")\n",
    "    print(f\"  GT voxels: {ground_truth_voxel_data.sum()}\")\n",
    "    print(f\"  Predicted voxels: {pred_voxels.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth data for visualization\n",
    "# Note: This requires the dataset configuration\n",
    "from omegaconf import OmegaConf\n",
    "from hydra import compose, initialize_config_dir\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "import traceback\n",
    "# Clear any existing hydra instance\n",
    "GlobalHydra.instance().clear()\n",
    "\n",
    "# Load existing config instead of creating from scratch\n",
    "config_dir = str(Path(\"../configs\").resolve())\n",
    "try:\n",
    "    with initialize_config_dir(config_dir=config_dir, version_base=None):\n",
    "        # Load the existing voxel dataset config\n",
    "        cfg = compose(config_name=\"datasets/shapenet_voxel_meshes\").datasets\n",
    "\n",
    "        # Override only what's needed for this test\n",
    "        cfg[\"categories\"] = [\"chair\"]\n",
    "        cfg[\"dataloader\"]= {'batch_size': 1, 'num_workers': 0}\n",
    "        \n",
    "        # Load test dataloader\n",
    "        _, _, test_loader = create_3dr2n2_reconstruction_dataloaders(\n",
    "            cfg, batch_size=1, num_workers=0\n",
    "        )\n",
    "        \n",
    "        # Get a sample\n",
    "        sample = next(iter(test_loader))\n",
    "        gt_voxels = sample['voxel_gt'][0].squeeze().numpy()\n",
    "        \n",
    "        best_layer = 2\n",
    "        # Visualize using best layer\n",
    "        if voxel_exp and best_layer is not None:\n",
    "            # Get probe and feature files using the loader methods\n",
    "            probe_files = loader.get_probe_files(voxel_exp)\n",
    "            feature_files = loader.get_feature_files(voxel_exp)\n",
    "            \n",
    "            if 'voxel' in probe_files and best_layer in probe_files['voxel']:\n",
    "                probe_path = probe_files['voxel'][2]\n",
    "                features_path = feature_files[best_layer]['test']\n",
    "                \n",
    "                print(f\"\\nVisualizing reconstruction from layer {best_layer}\")\n",
    "                visualize_voxel_reconstruction(str(probe_path), str(features_path), gt_voxels, sample_idx=0)\n",
    "            else:\n",
    "                print(f\"No voxel probe found for layer {best_layer}\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"Could not load dataset for visualization: {e}\")\n",
    "    print(traceback.format_exc())\n",
    "    print(\"Please ensure ShapeNet datasets are available\")\n",
    "finally:\n",
    "    # Clear hydra again\n",
    "    GlobalHydra.instance().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Viewpoint Estimation Analysis\n",
    "\n",
    "We compare how DINOv2 and I-JEPA encode viewpoint information across their layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_viewpoint_performance(experiment_data_list: List[ExperimentResult]):\n",
    "    \"\"\"Compare viewpoint estimation performance across models and probe types\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Viewpoint Estimation Performance Comparison', fontsize=16)\n",
    "    \n",
    "    colors = {'dinov2': 'royalblue', 'ijepa': 'forestgreen'}\n",
    "    markers = {'linear': 'o', 'mlp': 's'}\n",
    "    \n",
    "    for exp_data in experiment_data_list:\n",
    "        model_name = exp_data.model\n",
    "        metrics_dict = exp_data.results.get('results', {})\n",
    "        \n",
    "        # Organize data by probe type\n",
    "        probe_data = {'linear': {'layers': [], 'mae': [], 'angular_dist': []},\n",
    "                      'mlp': {'layers': [], 'mae': [], 'angular_dist': []}}\n",
    "        \n",
    "        for layer_key, layer_data in metrics_dict.items():\n",
    "            if layer_key.startswith('layer_'):\n",
    "                layer_num = int(layer_key.split('_')[1])\n",
    "                \n",
    "                for probe_type in ['linear', 'mlp']:\n",
    "                    if probe_type in layer_data:\n",
    "                        test_metrics = layer_data[probe_type].get('test_metrics', {})\n",
    "                        probe_data[probe_type]['layers'].append(layer_num)\n",
    "                        probe_data[probe_type]['mae'].append(test_metrics.get('mae', 0))\n",
    "                        probe_data[probe_type]['angular_dist'].append(\n",
    "                            test_metrics.get('angular_distance_mean', 0)\n",
    "                        )\n",
    "        \n",
    "        # Plot MAE comparison\n",
    "        ax_mae = axes[0, 0] if model_name == 'dinov2' else axes[0, 1]\n",
    "        for probe_type in ['linear', 'mlp']:\n",
    "            if probe_data[probe_type]['layers']:\n",
    "                sorted_idx = np.argsort(probe_data[probe_type]['layers'])\n",
    "                layers = np.array(probe_data[probe_type]['layers'])[sorted_idx]\n",
    "                mae = np.array(probe_data[probe_type]['mae'])[sorted_idx]\n",
    "                \n",
    "                ax_mae.plot(layers, mae, \n",
    "                           marker=markers[probe_type],\n",
    "                           linewidth=2, markersize=8,\n",
    "                           label=f'{probe_type.capitalize()} Probe',\n",
    "                           color=colors[model_name],\n",
    "                           alpha=0.8 if probe_type == 'mlp' else 1.0)\n",
    "        \n",
    "        ax_mae.set_xlabel('Layer')\n",
    "        ax_mae.set_ylabel('MAE')\n",
    "        ax_mae.set_title(f'{model_name.upper()} - Mean Absolute Error')\n",
    "        ax_mae.legend()\n",
    "        ax_mae.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot Angular Distance\n",
    "        ax_ang = axes[1, 0] if model_name == 'dinov2' else axes[1, 1]\n",
    "        for probe_type in ['linear', 'mlp']:\n",
    "            if probe_data[probe_type]['layers']:\n",
    "                sorted_idx = np.argsort(probe_data[probe_type]['layers'])\n",
    "                layers = np.array(probe_data[probe_type]['layers'])[sorted_idx]\n",
    "                angular = np.array(probe_data[probe_type]['angular_dist'])[sorted_idx]\n",
    "                \n",
    "                ax_ang.plot(layers, angular,\n",
    "                           marker=markers[probe_type],\n",
    "                           linewidth=2, markersize=8,\n",
    "                           label=f'{probe_type.capitalize()} Probe',\n",
    "                           color=colors[model_name],\n",
    "                           alpha=0.8 if probe_type == 'mlp' else 1.0)\n",
    "        \n",
    "        ax_ang.set_xlabel('Layer')\n",
    "        ax_ang.set_ylabel('Angular Distance (degrees)')\n",
    "        ax_ang.set_title(f'{model_name.upper()} - Mean Angular Distance')\n",
    "        ax_ang.legend()\n",
    "        ax_ang.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_linearity_gap(experiment_data_list: List[ExperimentResult]):\n",
    "    \"\"\"Analyze the gap between MLP and linear probe performance\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    colors = {'dinov2': 'royalblue', 'ijepa': 'forestgreen'}\n",
    "    \n",
    "    for exp_data in experiment_data_list:\n",
    "        model_name = exp_data.model\n",
    "        metrics_dict = exp_data.results.get('results', {})\n",
    "        \n",
    "        layers = []\n",
    "        linearity_gaps = []\n",
    "        \n",
    "        for layer_key, layer_data in metrics_dict.items():\n",
    "            if layer_key.startswith('layer_'):\n",
    "                layer_num = int(layer_key.split('_')[1])\n",
    "                \n",
    "                # Get MAE for both probe types\n",
    "                linear_mae = layer_data.get('linear', {}).get('test_metrics', {}).get('mae', None)\n",
    "                mlp_mae = layer_data.get('mlp', {}).get('test_metrics', {}).get('mae', None)\n",
    "                \n",
    "                if linear_mae is not None and mlp_mae is not None:\n",
    "                    layers.append(layer_num)\n",
    "                    # Linearity gap: how much better is MLP than linear\n",
    "                    gap = linear_mae - mlp_mae  # Positive means MLP is better\n",
    "                    linearity_gaps.append(gap)\n",
    "        \n",
    "        if layers:\n",
    "            sorted_idx = np.argsort(layers)\n",
    "            layers = np.array(layers)[sorted_idx]\n",
    "            linearity_gaps = np.array(linearity_gaps)[sorted_idx]\n",
    "            \n",
    "            plt.plot(layers, linearity_gaps,\n",
    "                    'o-', linewidth=2.5, markersize=8,\n",
    "                    label=model_name.upper(),\n",
    "                    color=colors[model_name])\n",
    "    \n",
    "    plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Linearity Gap (Linear MAE - MLP MAE)')\n",
    "    plt.title('Linearity of Viewpoint Representation')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotation\n",
    "    plt.text(0.02, 0.98, 'Higher = Less Linear\\n(MLP much better than Linear)',\n",
    "             transform=plt.gca().transAxes,\n",
    "             verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get viewpoint experiments\n",
    "viewpoint_exps = [exp for exp in experiments.values() \n",
    "                  if exp.task == 'viewpoint_estimation']\n",
    "\n",
    "if viewpoint_exps:\n",
    "    print(f\"Found {len(viewpoint_exps)} viewpoint experiments\")\n",
    "    plot_viewpoint_performance(viewpoint_exps)\n",
    "    plot_linearity_gap(viewpoint_exps)\n",
    "else:\n",
    "    print(\"No viewpoint estimation experiments found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Qualitative Analysis - Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_vs_true_viewpoint(probe: nn.Module, features: torch.Tensor, \n",
    "                                     targets: torch.Tensor, device: str):\n",
    "    \"\"\"Create scatter plots of predicted vs true viewpoint parameters\"\"\"\n",
    "    probe.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = probe(features.to(device)).cpu().numpy()\n",
    "    \n",
    "    targets = targets.numpy()\n",
    "    \n",
    "    # Denormalize (assuming normalized to [-1, 1])\n",
    "    pred_azimuth = (predictions[:, 0] + 1) * 180  # [-1, 1] -> [0, 360]\n",
    "    true_azimuth = (targets[:, 0] + 1) * 180\n",
    "    pred_elevation = predictions[:, 1] * 90  # [-1, 1] -> [-90, 90]\n",
    "    true_elevation = targets[:, 1] * 90\n",
    "    \n",
    "    # Calculate errors\n",
    "    az_error = np.abs(pred_azimuth - true_azimuth)\n",
    "    az_error = np.minimum(az_error, 360 - az_error)  # Handle wraparound\n",
    "    el_error = np.abs(pred_elevation - true_elevation)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Azimuth scatter\n",
    "    scatter1 = ax1.scatter(true_azimuth, pred_azimuth, c=az_error, \n",
    "                          cmap='viridis', alpha=0.6, s=20)\n",
    "    ax1.plot([0, 360], [0, 360], 'r--', alpha=0.5)\n",
    "    ax1.set_xlabel('True Azimuth (degrees)')\n",
    "    ax1.set_ylabel('Predicted Azimuth (degrees)')\n",
    "    ax1.set_title('Azimuth Predictions')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    cbar1 = plt.colorbar(scatter1, ax=ax1)\n",
    "    cbar1.set_label('Absolute Error (degrees)')\n",
    "    \n",
    "    # Elevation scatter\n",
    "    scatter2 = ax2.scatter(true_elevation, pred_elevation, c=el_error,\n",
    "                          cmap='viridis', alpha=0.6, s=20)\n",
    "    ax2.plot([-90, 90], [-90, 90], 'r--', alpha=0.5)\n",
    "    ax2.set_xlabel('True Elevation (degrees)')\n",
    "    ax2.set_ylabel('Predicted Elevation (degrees)')\n",
    "    ax2.set_title('Elevation Predictions')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    cbar2 = plt.colorbar(scatter2, ax=ax2)\n",
    "    cbar2.set_label('Absolute Error (degrees)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nPrediction Statistics:\")\n",
    "    print(f\"  Azimuth MAE: {np.mean(az_error):.2f}°\")\n",
    "    print(f\"  Elevation MAE: {np.mean(el_error):.2f}°\")\n",
    "    print(f\"  Combined Angular Distance: {np.mean(np.sqrt(az_error**2 + el_error**2)):.2f}°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_viewpoint_error_sphere(true_viewpoints: np.ndarray, pred_viewpoints: np.ndarray):\n",
    "    \"\"\"3D visualization of viewpoint errors on a sphere\"\"\"\n",
    "    # Convert to spherical coordinates\n",
    "    def viewpoint_to_xyz(azimuth, elevation, radius=1):\n",
    "        az_rad = np.deg2rad(azimuth)\n",
    "        el_rad = np.deg2rad(elevation)\n",
    "        x = radius * np.cos(el_rad) * np.cos(az_rad)\n",
    "        y = radius * np.cos(el_rad) * np.sin(az_rad)\n",
    "        z = radius * np.sin(el_rad)\n",
    "        return x, y, z\n",
    "    \n",
    "    # Denormalize viewpoints\n",
    "    true_az = (true_viewpoints[:, 0] + 1) * 180\n",
    "    true_el = true_viewpoints[:, 1] * 90\n",
    "    pred_az = (pred_viewpoints[:, 0] + 1) * 180\n",
    "    pred_el = pred_viewpoints[:, 1] * 90\n",
    "    \n",
    "    # Convert to 3D coordinates\n",
    "    true_x, true_y, true_z = viewpoint_to_xyz(true_az, true_el)\n",
    "    pred_x, pred_y, pred_z = viewpoint_to_xyz(pred_az, pred_el)\n",
    "    \n",
    "    # Calculate errors\n",
    "    errors = np.sqrt((pred_x - true_x)**2 + (pred_y - true_y)**2 + (pred_z - true_z)**2)\n",
    "    \n",
    "    # Create plotly figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add reference sphere\n",
    "    u = np.linspace(0, 2 * np.pi, 50)\n",
    "    v = np.linspace(0, np.pi, 50)\n",
    "    sphere_x = np.outer(np.cos(u), np.sin(v))\n",
    "    sphere_y = np.outer(np.sin(u), np.sin(v))\n",
    "    sphere_z = np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "    \n",
    "    fig.add_trace(go.Surface(\n",
    "        x=sphere_x, y=sphere_y, z=sphere_z,\n",
    "        opacity=0.2, colorscale='gray',\n",
    "        showscale=False, name='Unit Sphere'\n",
    "    ))\n",
    "    \n",
    "    # Add error vectors\n",
    "    # Sample subset for clarity\n",
    "    n_samples = min(200, len(true_x))\n",
    "    indices = np.random.choice(len(true_x), n_samples, replace=False)\n",
    "    \n",
    "    for i in indices:\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[true_x[i], pred_x[i]],\n",
    "            y=[true_y[i], pred_y[i]],\n",
    "            z=[true_z[i], pred_z[i]],\n",
    "            mode='lines+markers',\n",
    "            line=dict(color=errors[i], colorscale='Viridis', width=3),\n",
    "            marker=dict(size=[4, 6], color=['blue', 'red']),\n",
    "            showlegend=False\n",
    "        ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Viewpoint Prediction Errors on Unit Sphere',\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z',\n",
    "            aspectmode='cube'\n",
    "        ),\n",
    "        width=800,\n",
    "        height=800\n",
    "    )\n",
    "    \n",
    "    # Add legend manually\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[None], y=[None], z=[None],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color='blue'),\n",
    "        name='True Viewpoint'\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[None], y=[None], z=[None],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color='red'),\n",
    "        name='Predicted Viewpoint'\n",
    "    ))\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dinov2_viewpoint = None\n",
    "for exp in viewpoint_exps:\n",
    "    if exp.model == 'dinov2':\n",
    "        dinov2_viewpoint = exp\n",
    "        break\n",
    "\n",
    "if dinov2_viewpoint:\n",
    "    # Find best performing layer (lowest MAE)\n",
    "    best_layer = None\n",
    "    best_mae = float('inf')\n",
    "    best_probe_type = None\n",
    "    \n",
    "    results = dinov2_viewpoint.results.get('results', {})\n",
    "    for layer_key, layer_data in results.items():\n",
    "        if layer_key.startswith('layer_'):\n",
    "            layer_num = int(layer_key.split('_')[1])\n",
    "            for probe_type in ['linear', 'mlp']:\n",
    "                if probe_type in layer_data:\n",
    "                    mae = layer_data[probe_type].get('test_metrics', {}).get('mae', float('inf'))\n",
    "                    if mae < best_mae:\n",
    "                        best_mae = mae\n",
    "                        best_layer = layer_num\n",
    "                        best_probe_type = probe_type\n",
    "    \n",
    "    print(f\"Best layer for viewpoint: Layer {best_layer} ({best_probe_type} probe, MAE: {best_mae:.4f})\")\n",
    "    \n",
    "    # Load probe and features\n",
    "    if best_layer is not None:\n",
    "        try:\n",
    "            # Get probe and feature files using the loader methods\n",
    "            probe_files = loader.get_probe_files(dinov2_viewpoint)\n",
    "            feature_files = loader.get_feature_files(dinov2_viewpoint)\n",
    "            \n",
    "            if best_probe_type in probe_files and best_layer in probe_files[best_probe_type]:\n",
    "                probe_path = probe_files[best_probe_type][best_layer]\n",
    "                features_path = feature_files[best_layer]['test']\n",
    "                \n",
    "                # Load probe\n",
    "                probe_state = torch.load(probe_path, map_location=device)\n",
    "                probe_config = {\n",
    "                    'type': best_probe_type,\n",
    "                    'input_dim': 768,  \n",
    "                    'output_dim': 2,\n",
    "                    'hidden_dims': [256] if best_probe_type == 'mlp' else None,\n",
    "                    'task_type': 'regression'\n",
    "                }\n",
    "                original_probe_config = probe_state['probe_config']\n",
    "                probe = create_probe(original_probe_config)\n",
    "                probe.load_state_dict(probe_state[\"model_state_dict\"])\n",
    "                probe.to(device)\n",
    "                \n",
    "                # Load features\n",
    "                with open(features_path, 'rb') as f:\n",
    "                    test_data = pickle.load(f)\n",
    "                \n",
    "                features = torch.tensor(test_data['features'][:500])  # Use subset\n",
    "                targets = torch.tensor(test_data['targets'][:500])\n",
    "                \n",
    "                # Visualize predictions\n",
    "                plot_predicted_vs_true_viewpoint(probe, features, targets, device)\n",
    "                \n",
    "                # Get predictions for sphere visualization\n",
    "                with torch.no_grad():\n",
    "                    predictions = probe(features.to(device)).cpu().numpy()\n",
    "                \n",
    "                plot_viewpoint_error_sphere(targets.numpy(), predictions)\n",
    "            else:\n",
    "                print(f\"No {best_probe_type} probe found for layer {best_layer}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Could not visualize predictions: {e}\")\n",
    "            print(traceback.print_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Feature Analysis\n",
    "\n",
    "We perform deeper analysis of learned representations, focusing on DINOv2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Latent Space Cartography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_latent_space(features_file_path: str, metadata_file_path: Optional[str] = None,\n",
    "                         dataset_split: str = 'test'):\n",
    "    \"\"\"Analyze feature space organization using dimensionality reduction\"\"\"\n",
    "    with open(features_file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        features = data['features']\n",
    "        categories = data.get('metadata', {}).get('categories', [])\n",
    "        viewpoints = data.get('targets', None)\n",
    "    else:\n",
    "        features = data\n",
    "        categories = []\n",
    "        viewpoints = None\n",
    "    \n",
    "    n_samples = min(5000, len(features))\n",
    "    indices = np.random.choice(len(features), n_samples, replace=False)\n",
    "    features = features[indices]\n",
    "    \n",
    "    if len(categories) > 0:\n",
    "        categories = [categories[i] for i in indices]\n",
    "    if viewpoints is not None:\n",
    "        viewpoints = viewpoints[indices]\n",
    "    \n",
    "    print(f\"Analyzing {n_samples} samples...\")\n",
    "    \n",
    "    # Perform PCA\n",
    "    print(\"Running PCA...\")\n",
    "    pca = PCA(n_components=2)\n",
    "    features_pca = pca.fit_transform(features)\n",
    "    print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "    \n",
    "    # Perform t-SNE\n",
    "    print(\"Running t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "    features_tsne = tsne.fit_transform(features)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    \n",
    "    # PCA colored by category\n",
    "    if categories:\n",
    "        unique_cats = list(set(categories))\n",
    "        cat_to_idx = {cat: i for i, cat in enumerate(unique_cats)}\n",
    "        cat_indices = [cat_to_idx[cat] for cat in categories]\n",
    "        \n",
    "        scatter1 = axes[0, 0].scatter(features_pca[:, 0], features_pca[:, 1],\n",
    "                                     c=cat_indices, cmap='tab20', alpha=0.6, s=10)\n",
    "        axes[0, 0].set_title('PCA - Colored by Object Category')\n",
    "        axes[0, 0].set_xlabel('PC 1')\n",
    "        axes[0, 0].set_ylabel('PC 2')\n",
    "        \n",
    "        # t-SNE colored by category\n",
    "        axes[1, 0].scatter(features_tsne[:, 0], features_tsne[:, 1],\n",
    "                          c=cat_indices, cmap='tab20', alpha=0.6, s=10)\n",
    "        axes[1, 0].set_title('t-SNE - Colored by Object Category')\n",
    "        axes[1, 0].set_xlabel('t-SNE 1')\n",
    "        axes[1, 0].set_ylabel('t-SNE 2')\n",
    "    \n",
    "    # PCA colored by viewpoint\n",
    "    if viewpoints is not None:\n",
    "        azimuth = (viewpoints[:, 0] + 1) * 180  # Denormalize\n",
    "        \n",
    "        scatter2 = axes[0, 1].scatter(features_pca[:, 0], features_pca[:, 1],\n",
    "                                     c=azimuth, cmap='hsv', alpha=0.6, s=10)\n",
    "        axes[0, 1].set_title('PCA - Colored by Azimuth')\n",
    "        axes[0, 1].set_xlabel('PC 1')\n",
    "        axes[0, 1].set_ylabel('PC 2')\n",
    "        cbar1 = plt.colorbar(scatter2, ax=axes[0, 1])\n",
    "        cbar1.set_label('Azimuth (degrees)')\n",
    "        \n",
    "        # t-SNE colored by viewpoint\n",
    "        scatter3 = axes[1, 1].scatter(features_tsne[:, 0], features_tsne[:, 1],\n",
    "                                     c=azimuth, cmap='hsv', alpha=0.6, s=10)\n",
    "        axes[1, 1].set_title('t-SNE - Colored by Azimuth')\n",
    "        axes[1, 1].set_xlabel('t-SNE 1')\n",
    "        axes[1, 1].set_ylabel('t-SNE 2')\n",
    "        cbar2 = plt.colorbar(scatter3, ax=axes[1, 1])\n",
    "        cbar2.set_label('Azimuth (degrees)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dinov2_viewpoint and best_layer is not None:\n",
    "    feature_files = loader.get_feature_files(dinov2_viewpoint)\n",
    "    if best_layer in feature_files and 'test' in feature_files[best_layer]:\n",
    "        features_path = feature_files[best_layer]['test']\n",
    "        print(f\"\\nAnalyzing latent space for DINOv2 layer {best_layer}\")\n",
    "        analyze_latent_space(str(features_path))\n",
    "    else:\n",
    "        print(f\"No features found for layer {best_layer}\")\n",
    "else:\n",
    "    print(\"No DINOv2 viewpoint experiment or best layer found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Transformer Attention Analysis\n",
    "\n",
    "Note: DINOv2 models don't always expose attention weights directly. We use a feature similarity approach as an alternative visualization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def visualize_attention_maps(\n",
    "    image_tensor: torch.Tensor,\n",
    "    model_name: str,\n",
    "    device: torch.device,\n",
    "    method: str = 'aggregate', \n",
    "    layer: int = -1,\n",
    "    head: int = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize attention maps for ViT models.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): The input image tensor.\n",
    "        model_name (str): The name of the model.\n",
    "        device (torch.device): The device to run the model on.\n",
    "        method (str): Visualization method.\n",
    "                      'cls': Standard CLS token attention (for classifiers).\n",
    "                      'aggregate': Aggregated attention received by patches\n",
    "                                   (for GAP/regression models like I-JEPA).\n",
    "        layer (int): The transformer layer to visualize. Default is -1 (last layer).\n",
    "        head (int): The attention head to visualize. Default is None (average heads).\n",
    "    \"\"\"\n",
    "    model, predecessor = load_model_and_preprocessor( model_name, ckpt_path=None, device=device, cache_dir=None)\n",
    "    model.to(device).eval()\n",
    "    if image_tensor.dim() == 3:\n",
    "        image_tensor = image_tensor.unsqueeze(0)\n",
    "    inputs_on_device = image_tensor.to(device)\n",
    "\n",
    "    all_attentions = []\n",
    "    with torch.no_grad():\n",
    "        if 'timm' not in model_name and model_name != 'ijepa':\n",
    "            # HuggingFace logic\n",
    "            outputs = model(inputs_on_device, output_attentions=True)\n",
    "            all_attentions = outputs.attentions\n",
    "        else: # TIMM logic\n",
    "            attention_maps_by_layer = {}\n",
    "            hook_handles = []\n",
    "            original_fused_attn_states = {}\n",
    "            def get_hook(layer_idx):\n",
    "                def hook_fn(module, input_args, output_tensor):\n",
    "                    attention_maps_by_layer[layer_idx] = input_args[0].detach()\n",
    "                return hook_fn\n",
    "            if not hasattr(model, 'blocks') or not model.blocks: return\n",
    "            for i, block in enumerate(model.blocks):\n",
    "                attn_module = block.attn\n",
    "                if hasattr(attn_module, 'fused_attn'):\n",
    "                    original_fused_attn_states[i] = attn_module.fused_attn\n",
    "                    attn_module.fused_attn = False\n",
    "                handle = attn_module.attn_drop.register_forward_hook(get_hook(i))\n",
    "                hook_handles.append(handle)\n",
    "            _ = model(inputs_on_device)\n",
    "            for handle in hook_handles: handle.remove()\n",
    "            for i, state in original_fused_attn_states.items(): model.blocks[i].attn.fused_attn = state\n",
    "            num_heads = model.blocks[0].attn.num_heads\n",
    "            batch_size = inputs_on_device.shape[0]\n",
    "            for i in sorted(attention_maps_by_layer.keys()):\n",
    "                attn_probs = attention_maps_by_layer[i]\n",
    "                if attn_probs.ndim == 3:\n",
    "                    num_tokens = attn_probs.shape[-1]\n",
    "                    attn_probs = attn_probs.view(batch_size, num_heads, num_tokens, num_tokens)\n",
    "                all_attentions.append(attn_probs)\n",
    "\n",
    "    if not all_attentions:\n",
    "        print(\"Failed to extract any attention maps.\")\n",
    "        return\n",
    "\n",
    "    attn_from_layer = all_attentions[layer]\n",
    "    if head is not None:\n",
    "        attn_to_vis = attn_from_layer[:, head] # Select specific head\n",
    "        title_head_info = f\"Head {head}\"\n",
    "    else:\n",
    "        attn_to_vis = attn_from_layer.mean(dim=1) # Average over heads\n",
    "        title_head_info = \"Averaged Heads\"\n",
    "\n",
    "    attention_vector = None\n",
    "    plot_main_title = \"\"\n",
    "\n",
    "    if method == 'cls':\n",
    "        attention_vector = attn_to_vis[0, 0, 1:]\n",
    "        plot_main_title = \"CLS Token Attention\"\n",
    "    \n",
    "    elif method == 'aggregate':\n",
    "        patch_attn = attn_to_vis[0, 1:, 1:] \n",
    "        attention_vector = patch_attn.sum(dim=0) \n",
    "        plot_main_title = \"Aggregated Patch Attention\"\n",
    "\n",
    "    num_patch_tokens = attention_vector.shape[-1]\n",
    "    patch_grid_size = int(np.sqrt(num_patch_tokens))\n",
    "    \n",
    "    if patch_grid_size * patch_grid_size == num_patch_tokens:\n",
    "        attention_map_reshaped = attention_vector.reshape(patch_grid_size, patch_grid_size)\n",
    "    elif hasattr(model, 'patch_embed') and hasattr(model.patch_embed, 'grid_size'):\n",
    "        grid_h, grid_w = model.patch_embed.grid_size\n",
    "        expected_tokens = grid_h * grid_w\n",
    "        if num_patch_tokens != expected_tokens:\n",
    "            padding_needed = expected_tokens - num_patch_tokens\n",
    "            if padding_needed > 0:\n",
    "                padded_attention = F.pad(attention_vector, (0, padding_needed), value=0)\n",
    "                attention_map_reshaped = padded_attention.reshape(grid_h, grid_w)\n",
    "            else:\n",
    "                attention_map_reshaped = attention_vector[:expected_tokens].reshape(grid_h, grid_w)\n",
    "        else:\n",
    "            attention_map_reshaped = attention_vector.reshape(grid_h, grid_w)\n",
    "    else:\n",
    "        raise RuntimeError(\"Cannot determine grid size for non-square patch number.\")\n",
    "                \n",
    "\n",
    "\n",
    "    attention_map_reshaped = attention_map_reshaped.cpu().numpy()\n",
    "    attention_map_reshaped = (attention_map_reshaped - attention_map_reshaped.min()) / (attention_map_reshaped.max() - attention_map_reshaped.min() + 1e-6)\n",
    "    \n",
    "    attn_resized = F.interpolate(\n",
    "        torch.tensor(attention_map_reshaped).unsqueeze(0).unsqueeze(0),\n",
    "        size=(image_tensor.shape[-2], image_tensor.shape[-1]),\n",
    "        mode='bilinear',\n",
    "        align_corners=False\n",
    "    ).squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    layer_info = f\"Layer {layer if layer != -1 else len(all_attentions) - 1}\"\n",
    "    fig.suptitle(f'{plot_main_title} for {model_name}\\n({layer_info}, {title_head_info})', fontsize=16)\n",
    "\n",
    "    img_for_show = image_tensor[0].permute(1, 2, 0).cpu().numpy()\n",
    "    min_val, max_val = img_for_show.min(), img_for_show.max()\n",
    "    img_for_show = (img_for_show - min_val) / (max_val - min_val + 1e-6)\n",
    "    ax1.imshow(img_for_show); ax1.set_title('Original Image'); ax1.axis('off')\n",
    "    im = ax2.imshow(attn_resized, cmap='hot'); ax2.set_title('Attention Map'); ax2.axis('off')\n",
    "    fig.colorbar(im, ax=ax2, fraction=0.046, pad=0.04)\n",
    "    ax3.imshow(img_for_show); ax3.imshow(attn_resized, cmap='hot', alpha=0.5); ax3.set_title('Overlay'); ax3.axis('off')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "dataset_config = OmegaConf.load('../configs/datasets/shapenet_3dr2n2.yaml')\n",
    "\n",
    "# Override categories to just use chair for quick testing\n",
    "dataset_config.categories = ['chair']\n",
    "\n",
    "_, _, test_loader = create_3dr2n2_dataloaders(\n",
    "    dataset_config, batch_size=1, num_workers=0\n",
    ")\n",
    "\n",
    "# Get a sample image\n",
    "sample = next(iter(test_loader))\n",
    "image = sample['image'][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test attention visualization with a sample image\n",
    "try:\n",
    "    # Load existing dataset configuration\n",
    "\n",
    "    print(\"Visualizing DINOv2 attention...\")\n",
    "    visualize_attention_maps(image, model_name='dinov2', layer=11, head=1, device=device, method=\"cls\")\n",
    "    \n",
    "    print(\"\\nVisualizing I-JEPA (TIMM) attention...\")\n",
    "    visualize_attention_maps(image, model_name='ijepa', layer=11, device=device,method=\"aggregate\")\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize attention: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
