{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "342a0074",
   "metadata": {},
   "source": [
    "# Final Analysis: Deciphering the intrinsic latents of SSL ViTs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d240c7",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49efe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Machine learning stuff\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import rbf_kernel, cosine_similarity\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Project imports\n",
    "from src.probing.probes import create_probe, VoxelProbe, LinearProbe, MLPProbe\n",
    "from src.datasets.shapenet_voxel_meshes import create_3dr2n2_reconstruction_dataloaders\n",
    "from src.datasets.shapenet_3dr2n2 import create_3dr2n2_dataloaders\n",
    "from src.models.model_loader import load_model_and_preprocessor\n",
    "from src.analysis.layer_analysis import LayerWiseAnalyzer\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbad554",
   "metadata": {},
   "source": [
    "### 1.1 Visual Design\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a440f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualConfig:\n",
    "    \"\"\"Unified visual design config\"\"\"\n",
    "    \n",
    "    # Color palettes\n",
    "    MODEL_COLORS = {\n",
    "        'dinov2': '#2E86AB',      # Deep blue\n",
    "        'ijepa': '#A23B72',       # Wine\n",
    "        'supervised_vit': '#F18F01',  # Orange\n",
    "        'mocov3': '#C73E1D'       # Red-orange\n",
    "    }\n",
    "    \n",
    "    PROBE_COLORS = {\n",
    "        'linear': '#4A90E2',      # Light blue\n",
    "        'mlp': '#F5A623',         # Orange\n",
    "        'voxel': '#7ED321'        # Green\n",
    "    }\n",
    "    \n",
    "    METRIC_COLORS = {\n",
    "        'iou': '#50C878',         # Emerald\n",
    "        'precision': '#FFD700',   # Gold\n",
    "        'recall': '#DC143C',      # Crimson\n",
    "        'f1': '#9370DB',          # Medium purple\n",
    "        'mae': '#FF6B6B',         # Light red\n",
    "        'rmse': '#4ECDC4',        # Turquoise\n",
    "        'r2': '#95E1D3'           # Mint\n",
    "    }\n",
    "    \n",
    "    # Typography\n",
    "    FONT_FAMILY = 'Raleway'\n",
    "    FONT_FAMILY_ALT = 'Rubik'\n",
    "    TITLE_SIZE = 18\n",
    "    LABEL_SIZE = 14\n",
    "    TICK_SIZE = 12\n",
    "    LEGEND_SIZE = 12\n",
    "    \n",
    "    # Plot styling\n",
    "    FIGURE_DPI = 300\n",
    "    LINE_WIDTH = 2.5\n",
    "    MARKER_SIZE = 8\n",
    "    ALPHA = 0.8\n",
    "    GRID_ALPHA = 0.3\n",
    "    \n",
    "    # Figure sizes\n",
    "    SINGLE_FIG_SIZE = (8, 6)\n",
    "    DOUBLE_FIG_SIZE = (15, 6)\n",
    "    QUAD_FIG_SIZE = (16, 12)\n",
    "    \n",
    "    @classmethod\n",
    "    def setup_matplotlib(cls):\n",
    "        \"\"\"Configure matplotlib with our design settings\"\"\"\n",
    "        try:\n",
    "            font_dirs = ['/usr/share/fonts/', '~/.fonts']\n",
    "            font_files = fm.findSystemFonts(fontpaths=font_dirs)\n",
    "            for font_file in font_files:\n",
    "                fm.fontManager.addfont(font_file)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        plt.style.use('seaborn-v0_8-darkgrid')\n",
    "        \n",
    "        plt.rcParams.update({\n",
    "            'font.family': 'sans-serif',\n",
    "            'font.sans-serif': [cls.FONT_FAMILY, cls.FONT_FAMILY_ALT, 'DejaVu Sans'],\n",
    "            'font.size': cls.LABEL_SIZE,\n",
    "            'axes.titlesize': cls.TITLE_SIZE,\n",
    "            'axes.labelsize': cls.LABEL_SIZE,\n",
    "            'xtick.labelsize': cls.TICK_SIZE,\n",
    "            'ytick.labelsize': cls.TICK_SIZE,\n",
    "            'legend.fontsize': cls.LEGEND_SIZE,\n",
    "            'figure.dpi': cls.FIGURE_DPI,\n",
    "            'savefig.dpi': cls.FIGURE_DPI,\n",
    "            'axes.linewidth': 1.5,\n",
    "            'axes.grid': True,\n",
    "            'grid.alpha': cls.GRID_ALPHA,\n",
    "            'axes.spines.top': False,\n",
    "            'axes.spines.right': False,\n",
    "        })\n",
    "        \n",
    "        sns.set_palette([cls.MODEL_COLORS[m] for m in cls.MODEL_COLORS])\n",
    "    \n",
    "    @classmethod\n",
    "    def get_plotly_layout(cls, title: str = \"\") -> dict:\n",
    "        return dict(\n",
    "            title=dict(text=title, font=dict(size=cls.TITLE_SIZE, family=cls.FONT_FAMILY)),\n",
    "            font=dict(family=cls.FONT_FAMILY, size=cls.LABEL_SIZE),\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white',\n",
    "            showlegend=True,\n",
    "            hovermode='closest',\n",
    "            margin=dict(l=80, r=80, t=100, b=80)\n",
    "        )\n",
    "\n",
    "VisualConfig.setup_matplotlib()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4a967",
   "metadata": {},
   "source": [
    "### 1.2 Output Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd89c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputManager:\n",
    "    \"\"\"Manages saving of figures and data\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir: str = \"final_analysis\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        self.dirs = {\n",
    "            'root': self.base_dir,\n",
    "            'figures': self.base_dir / 'figures',\n",
    "            'subplots': self.base_dir / 'figures' / 'subplots',\n",
    "            'data': self.base_dir / 'data',\n",
    "            'comparisons': self.base_dir / 'comparisons'\n",
    "        }\n",
    "        \n",
    "        for dir_path in self.dirs.values():\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def save_figure(self, fig: plt.Figure, name: str, save_subplots: bool = False):\n",
    "        filename = f\"{name}_{self.timestamp}.png\"\n",
    "        filepath = self.dirs['figures'] / filename\n",
    "        fig.savefig(filepath, bbox_inches='tight', dpi=VisualConfig.FIGURE_DPI)\n",
    "        print(f\"Saved figure: {filepath}\")\n",
    "        \n",
    "        # we can also save each subplot separately (nice for the poster) \n",
    "        if save_subplots and hasattr(fig, 'axes'):\n",
    "            for i, ax in enumerate(fig.axes):\n",
    "                if hasattr(ax, 'get_title') and ax.get_title():\n",
    "                    subplot_fig, subplot_ax = plt.subplots(figsize=VisualConfig.SINGLE_FIG_SIZE)\n",
    "                    \n",
    "                    for line in ax.get_lines():\n",
    "                        subplot_ax.plot(line.get_xdata(), line.get_ydata(), \n",
    "                                      color=line.get_color(), \n",
    "                                      linewidth=line.get_linewidth(),\n",
    "                                      marker=line.get_marker(),\n",
    "                                      markersize=line.get_markersize(),\n",
    "                                      label=line.get_label())\n",
    "                    \n",
    "                    subplot_ax.set_xlabel(ax.get_xlabel())\n",
    "                    subplot_ax.set_ylabel(ax.get_ylabel())\n",
    "                    subplot_ax.set_title(ax.get_title())\n",
    "                    subplot_ax.grid(True, alpha=VisualConfig.GRID_ALPHA)\n",
    "                    if ax.get_legend():\n",
    "                        subplot_ax.legend()\n",
    "                    \n",
    "                    subplot_filename = f\"{name}_subplot_{i}_{ax.get_title().replace(' ', '_').lower()}_{self.timestamp}.png\"\n",
    "                    subplot_filepath = self.dirs['subplots'] / subplot_filename\n",
    "                    subplot_fig.savefig(subplot_filepath, bbox_inches='tight', dpi=VisualConfig.FIGURE_DPI)\n",
    "                    plt.close(subplot_fig)\n",
    "    \n",
    "    def save_plotly_figure(self, fig: go.Figure, name: str):\n",
    "        \"\"\"Save the figure to the provided writepath\"\"\"\n",
    "        filename = f\"{name}_{self.timestamp}\"\n",
    "        filepath_html = self.dirs['figures'] / f\"{filename}.html\"\n",
    "        filepath_png = self.dirs['figures'] / f\"{filename}.png\"\n",
    "        \n",
    "        fig.write_html(filepath_html)\n",
    "        fig.write_image(filepath_png, width=1200, height=800)\n",
    "        print(f\"Saved plotly figure: {filepath_png}\")\n",
    "    \n",
    "    def save_data(self, data: Any, name: str):\n",
    "        \"\"\"Save data thattwas used to generate the stuff\"\"\"\n",
    "        filename = f\"{name}_{self.timestamp}.json\"\n",
    "        filepath = self.dirs['data'] / filename\n",
    "        \n",
    "        def convert_to_serializable(obj):\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, (np.integer, np.floating)):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, Path):\n",
    "                return str(obj)\n",
    "            elif hasattr(obj, '__dict__'):\n",
    "                return obj.__dict__\n",
    "            return obj\n",
    "        \n",
    "        serializable_data = json.loads(\n",
    "            json.dumps(data, default=convert_to_serializable)\n",
    "        )\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(serializable_data, f, indent=2)\n",
    "        print(f\"Saved data: {filepath}\")\n",
    "\n",
    "output_manager = OutputManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed09fb5",
   "metadata": {},
   "source": [
    "## 2. Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d817bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"Contains the results for all the experiments in a standardized format\"\"\"\n",
    "    name: str\n",
    "    model: str\n",
    "    task: str\n",
    "    results: Dict[str, Any]\n",
    "    cache_dir: Path\n",
    "    results_dir: Path\n",
    "    \n",
    "    def get_best_layer(self, probe_type: str = None) -> Tuple[int, float]:\n",
    "        \"\"\"Get best performing layer and its metric value\"\"\"\n",
    "        best_layer = None\n",
    "        best_metric = float('inf') if self.task == 'viewpoint_estimation' else 0.0\n",
    "        \n",
    "        for layer_key, layer_data in self.results.get('results', {}).items():\n",
    "            if not layer_key.startswith('layer_'):\n",
    "                continue\n",
    "                \n",
    "            layer_num = int(layer_key.split('_')[1])\n",
    "            \n",
    "            if probe_type:\n",
    "                if probe_type in layer_data:\n",
    "                    metric = self._get_primary_metric(layer_data[probe_type])\n",
    "                    if self._is_better_metric(metric, best_metric):\n",
    "                        best_metric = metric\n",
    "                        best_layer = layer_num\n",
    "            else:\n",
    "                for p_type, p_data in layer_data.items():\n",
    "                    metric = self._get_primary_metric(p_data)\n",
    "                    if self._is_better_metric(metric, best_metric):\n",
    "                        best_metric = metric\n",
    "                        best_layer = layer_num\n",
    "        \n",
    "        return best_layer, best_metric\n",
    "    \n",
    "    def _get_primary_metric(self, probe_data: Dict) -> float:\n",
    "        \"\"\"Get primary metric for the task\"\"\"\n",
    "        test_metrics = probe_data.get('test_metrics', {})\n",
    "        if self.task == 'viewpoint_estimation':\n",
    "            return test_metrics.get('mae', float('inf'))\n",
    "        else:  # voxel_reconstruction\n",
    "            return test_metrics.get('voxel_iou', 0.0)\n",
    "    \n",
    "    def _is_better_metric(self, new_metric: float, best_metric: float) -> bool:\n",
    "        \"\"\"Check if new metric is better than current best\"\"\"\n",
    "        if self.task == 'viewpoint_estimation':\n",
    "            return new_metric < best_metric\n",
    "        else:\n",
    "            return new_metric > best_metric\n",
    "\n",
    "\n",
    "class ExperimentLoader:\n",
    "    \"\"\"Loads all the data from the cache/results dir\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = \".\"):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.cache_path = self.base_path / \"cache\"\n",
    "        self.results_path = self.base_path / \"results\"\n",
    "        \n",
    "    def load_all_experiments(self) -> Dict[str, ExperimentResult]:\n",
    "        \"\"\"Load all experiments from results directory\"\"\"\n",
    "        experiments = {}\n",
    "        \n",
    "        for results_dir in self.results_path.iterdir():\n",
    "            if not results_dir.is_dir() or results_dir.name.startswith('.'):\n",
    "                continue\n",
    "                \n",
    "            exp_name = results_dir.name\n",
    "            model, task = self._parse_experiment_name(exp_name)\n",
    "            \n",
    "            results_file = results_dir / \"results.json\"\n",
    "            if not results_file.exists():\n",
    "                print(f\"Warning: No results.json found for {exp_name}\")\n",
    "                continue\n",
    "                \n",
    "            with open(results_file, 'r') as f:\n",
    "                results_data = json.load(f)\n",
    "            \n",
    "            cache_dir = self._find_cache_dir(exp_name)\n",
    "            \n",
    "            experiments[exp_name] = ExperimentResult(\n",
    "                name=exp_name,\n",
    "                model=model,\n",
    "                task=task,\n",
    "                results=results_data,\n",
    "                cache_dir=cache_dir,\n",
    "                results_dir=results_dir\n",
    "            )\n",
    "            \n",
    "        return experiments\n",
    "    \n",
    "    def categorize_experiments(self, experiments: Dict[str, ExperimentResult]) -> Dict[str, List[ExperimentResult]]:\n",
    "        \"\"\"Categorize experiments by task and model\"\"\"\n",
    "        categorized = {\n",
    "            'voxel_reconstruction': [],\n",
    "            'viewpoint_estimation': [],\n",
    "            'by_model': {}\n",
    "        }\n",
    "        \n",
    "        for exp in experiments.values():\n",
    "            categorized[exp.task].append(exp)\n",
    "            \n",
    "            if exp.model not in categorized['by_model']:\n",
    "                categorized['by_model'][exp.model] = []\n",
    "            categorized['by_model'][exp.model].append(exp)\n",
    "        \n",
    "        return categorized\n",
    "    \n",
    "    def _parse_experiment_name(self, exp_name: str) -> Tuple[str, str]:\n",
    "        \"\"\"Extract model and task from experiment name\"\"\"\n",
    "        exp_lower = exp_name.lower()\n",
    "        \n",
    "        if 'dinov2' in exp_lower:\n",
    "            model = 'dinov2'\n",
    "        elif 'ijepa' in exp_lower:\n",
    "            model = 'ijepa'\n",
    "        elif 'supervised' in exp_lower:\n",
    "            model = 'supervised_vit'\n",
    "        else:\n",
    "            model = 'unknown'\n",
    "        \n",
    "        # which task was this?    \n",
    "        if 'viewpoint' in exp_lower:\n",
    "            task = 'viewpoint_estimation'\n",
    "        elif 'voxel' in exp_lower or 'reconstruction' in exp_lower:\n",
    "            task = 'voxel_reconstruction'\n",
    "        else:\n",
    "            task = 'unknown'\n",
    "            \n",
    "        return model, task\n",
    "    \n",
    "    def _find_cache_dir(self, exp_name: str) -> Path:\n",
    "        \"\"\"Find corresponding cache dir\"\"\"\n",
    "        cache_dir = self.cache_path / exp_name\n",
    "        if cache_dir.exists():\n",
    "            return cache_dir\n",
    "            \n",
    "        alternatives = [\n",
    "            exp_name.replace('phase1_', '').replace('phase2_', ''),\n",
    "            exp_name.replace('_probing', ''),\n",
    "            exp_name.replace('_reconstruction', '')\n",
    "        ]\n",
    "        \n",
    "        for alt_name in alternatives:\n",
    "            if alt_name and (self.cache_path / alt_name).exists():\n",
    "                return self.cache_path / alt_name\n",
    "                \n",
    "        return self.cache_path / exp_name\n",
    "    \n",
    "    def get_probe_files(self, experiment: ExperimentResult) -> Dict[str, Dict[int, Path]]:\n",
    "        \"\"\"Organize the probes by experiment and wihch layer they were on\"\"\"\n",
    "        probes = {}\n",
    "        probes_dir = experiment.cache_dir / \"probes\"\n",
    "        \n",
    "        if not probes_dir.exists():\n",
    "            return probes\n",
    "            \n",
    "        for probe_file in probes_dir.glob(\"*.pth\"):\n",
    "            filename = probe_file.stem\n",
    "            \n",
    "            if experiment.task == 'viewpoint_estimation':\n",
    "                if '_layer_' in filename:\n",
    "                    parts = filename.split('_layer_')\n",
    "                    if len(parts) == 2 and parts[1].isdigit():\n",
    "                        probe_type = parts[0]\n",
    "                        layer_num = int(parts[1])\n",
    "                        \n",
    "                        if probe_type not in probes:\n",
    "                            probes[probe_type] = {}\n",
    "                        probes[probe_type][layer_num] = probe_file\n",
    "                        \n",
    "            elif experiment.task == 'voxel_reconstruction':\n",
    "                if '_voxel_layer_' in filename:\n",
    "                    layer_part = filename.split('_voxel_layer_')[-1]\n",
    "                    if layer_part.isdigit():\n",
    "                        probe_type = 'voxel'\n",
    "                        layer_num = int(layer_part)\n",
    "                        \n",
    "                        if probe_type not in probes:\n",
    "                            probes[probe_type] = {}\n",
    "                        probes[probe_type][layer_num] = probe_file\n",
    "        \n",
    "        return probes\n",
    "    \n",
    "    def get_feature_files(self, experiment: ExperimentResult) -> Dict[int, Dict[str, Path]]:\n",
    "        \"\"\"Get feature files organized in the same manner as above\"\"\"\n",
    "        features = {}\n",
    "        features_dir = experiment.cache_dir / \"features\"\n",
    "        \n",
    "        if not features_dir.exists():\n",
    "            return features\n",
    "            \n",
    "        for feature_file in features_dir.glob(\"*.pkl\"):\n",
    "            filename = feature_file.stem\n",
    "            if filename.startswith('layer_'):\n",
    "                parts = filename.split('_')\n",
    "                if len(parts) >= 3 and parts[1].isdigit():\n",
    "                    layer_num = int(parts[1])\n",
    "                    split = parts[2]\n",
    "                    \n",
    "                    if layer_num not in features:\n",
    "                        features[layer_num] = {}\n",
    "                    features[layer_num][split] = feature_file\n",
    "                    \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a7ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we load and categorize all the experiments...\n",
    "loader = ExperimentLoader()\n",
    "all_experiments = loader.load_all_experiments()\n",
    "categorized_experiments = loader.categorize_experiments(all_experiments)\n",
    "\n",
    "print(f\"Loaded {len(all_experiments)} experiments:\")\n",
    "print(f\"  Voxel Reconstruction: {len(categorized_experiments['voxel_reconstruction'])}\")\n",
    "print(f\"  Viewpoint Estimation: {len(categorized_experiments['viewpoint_estimation'])}\")\n",
    "print(\"\\nBy model:\")\n",
    "for model, exps in categorized_experiments['by_model'].items():\n",
    "    print(f\"  {model}: {len(exps)} experiments\")\n",
    "    for exp in exps:\n",
    "        print(f\"    - {exp.name} ({exp.task})\")\n",
    "\n",
    "# save the summary we found above just in case\n",
    "experiment_summary = {\n",
    "    'total_experiments': len(all_experiments),\n",
    "    'experiments_by_task': {\n",
    "        task: [exp.name for exp in exps] \n",
    "        for task, exps in categorized_experiments.items() \n",
    "        if task != 'by_model'\n",
    "    },\n",
    "    'experiments_by_model': {\n",
    "        model: [exp.name for exp in exps]\n",
    "        for model, exps in categorized_experiments['by_model'].items()\n",
    "    }\n",
    "}\n",
    "output_manager.save_data(experiment_summary, 'experiment_summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3475e799",
   "metadata": {},
   "source": [
    "## 3. Voxel Reconstruction Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d7621",
   "metadata": {},
   "source": [
    "### 3.1 Quantitive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_voxel_reconstruction_all_models(experiments: List[ExperimentResult], \n",
    "                                          output_manager: OutputManager):\n",
    "    \"\"\"Analyze the voxel reconstruction performance for all associated models/experiments\"\"\"\n",
    "    \n",
    "    model_data = {}\n",
    "    \n",
    "    for exp in experiments:\n",
    "        metrics_dict = exp.results.get('results', {})\n",
    "        \n",
    "        layers = []\n",
    "        metrics = {\n",
    "            'iou': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': []\n",
    "        }\n",
    "        \n",
    "        for layer_key, layer_data in metrics_dict.items():\n",
    "            if layer_key.startswith('layer_') and 'voxel' in layer_data:\n",
    "                layer_num = int(layer_key.split('_')[1])\n",
    "                test_metrics = layer_data['voxel'].get('test_metrics', {})\n",
    "                \n",
    "                layers.append(layer_num)\n",
    "                metrics['iou'].append(test_metrics.get('voxel_iou', 0))\n",
    "                metrics['precision'].append(test_metrics.get('voxel_precision', 0))\n",
    "                metrics['recall'].append(test_metrics.get('voxel_recall', 0))\n",
    "                metrics['f1'].append(test_metrics.get('voxel_f1', 0))\n",
    "        \n",
    "        if layers:\n",
    "            sorted_idx = np.argsort(layers)\n",
    "            model_data[exp.model] = {\n",
    "                'layers': np.array(layers)[sorted_idx],\n",
    "                'metrics': {k: np.array(v)[sorted_idx] for k, v in metrics.items()},\n",
    "                'experiment': exp\n",
    "            }\n",
    "    \n",
    "    #===================================================================================================\n",
    "    # VISUALIZATION CODE BEGINS\n",
    "    #=================================================================================================== \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.25)\n",
    "    \n",
    "    ax_main = fig.add_subplot(gs[0, :])\n",
    "    \n",
    "    ax_iou = fig.add_subplot(gs[1, 0])\n",
    "    ax_precision = fig.add_subplot(gs[1, 1])\n",
    "    ax_recall = fig.add_subplot(gs[2, 0])\n",
    "    ax_f1 = fig.add_subplot(gs[2, 1])\n",
    "    \n",
    "    metric_axes = {\n",
    "        'iou': (ax_iou, 'IoU Score', 'Intersection over Union'),\n",
    "        'precision': (ax_precision, 'Precision', 'Voxel Precision'),\n",
    "        'recall': (ax_recall, 'Recall', 'Voxel Recall'),\n",
    "        'f1': (ax_f1, 'F1 Score', 'Voxel F1 Score')\n",
    "    }\n",
    "    \n",
    "    # plot IOU \n",
    "    for model_name, data in model_data.items():\n",
    "        ax_main.plot(data['layers'], data['metrics']['iou'],\n",
    "                    'o-', linewidth=VisualConfig.LINE_WIDTH,\n",
    "                    markersize=VisualConfig.MARKER_SIZE,\n",
    "                    color=VisualConfig.MODEL_COLORS.get(model_name, '#333'),\n",
    "                    label=model_name.upper(),\n",
    "                    alpha=VisualConfig.ALPHA)\n",
    "    \n",
    "    ax_main.set_xlabel('Layer', fontsize=VisualConfig.LABEL_SIZE)\n",
    "    ax_main.set_ylabel('IoU Score', fontsize=VisualConfig.LABEL_SIZE)\n",
    "    ax_main.set_title('Voxel Reconstruction Performance Comparison',\n",
    "                     fontsize=VisualConfig.TITLE_SIZE, fontweight='bold')\n",
    "    ax_main.legend(fontsize=VisualConfig.LEGEND_SIZE)\n",
    "    ax_main.grid(True, alpha=VisualConfig.GRID_ALPHA)\n",
    "    ax_main.set_ylim([0, 1])\n",
    "    \n",
    "    # plot all the other metrics\n",
    "    for metric_name, (ax, ylabel, title) in metric_axes.items():\n",
    "        for model_name, data in model_data.items():\n",
    "            ax.plot(data['layers'], data['metrics'][metric_name],\n",
    "                   'o-', linewidth=VisualConfig.LINE_WIDTH,\n",
    "                   markersize=VisualConfig.MARKER_SIZE,\n",
    "                   color=VisualConfig.MODEL_COLORS.get(model_name, '#333'),\n",
    "                   label=model_name.upper(),\n",
    "                   alpha=VisualConfig.ALPHA)\n",
    "        \n",
    "        ax.set_xlabel('Layer', fontsize=VisualConfig.LABEL_SIZE)\n",
    "        ax.set_ylabel(ylabel, fontsize=VisualConfig.LABEL_SIZE)\n",
    "        ax.set_title(title, fontsize=VisualConfig.LABEL_SIZE + 2)\n",
    "        ax.legend(fontsize=VisualConfig.LEGEND_SIZE - 2)\n",
    "        ax.grid(True, alpha=VisualConfig.GRID_ALPHA)\n",
    "        ax.set_ylim([0, 1])\n",
    "    \n",
    "    plt.suptitle('Comprehensive Voxel Reconstruction Analysis',\n",
    "                fontsize=VisualConfig.TITLE_SIZE + 2, fontweight='bold')\n",
    "    \n",
    "    output_manager.save_figure(fig, 'voxel_reconstruction_all_models', save_subplots=True)\n",
    "    plt.show()\n",
    "    \n",
    "    raw_data = {\n",
    "        model: {\n",
    "            'layers': data['layers'].tolist(),\n",
    "            'metrics': {k: v.tolist() for k, v in data['metrics'].items()},\n",
    "            'best_layer': int(data['layers'][np.argmax(data['metrics']['iou'])]),\n",
    "            'best_iou': float(np.max(data['metrics']['iou']))\n",
    "        }\n",
    "        for model, data in model_data.items()\n",
    "    }\n",
    "    output_manager.save_data(raw_data, 'voxel_reconstruction_metrics')\n",
    "   \n",
    "    # summary of the test \n",
    "    print(\"\\nVoxel Reconstruction Summary:\")\n",
    "    for model, data in model_data.items():\n",
    "        best_idx = np.argmax(data['metrics']['iou'])\n",
    "        print(f\"\\n{model.upper()}:\")\n",
    "        print(f\"  Best Layer: {data['layers'][best_idx]}\")\n",
    "        print(f\"  Best IoU: {data['metrics']['iou'][best_idx]:.4f}\")\n",
    "        print(f\"  Precision: {data['metrics']['precision'][best_idx]:.4f}\")\n",
    "        print(f\"  Recall: {data['metrics']['recall'][best_idx]:.4f}\")\n",
    "        print(f\"  F1 Score: {data['metrics']['f1'][best_idx]:.4f}\")\n",
    "    \n",
    "    return model_data\n",
    "\n",
    "\n",
    "#===================================================================================================\n",
    "# RUNNING THE ANALYSIS\n",
    "#===================================================================================================\n",
    "    \n",
    "voxel_model_data = analyze_voxel_reconstruction_all_models(\n",
    "    categorized_experiments['voxel_reconstruction'],\n",
    "    output_manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581addb7",
   "metadata": {},
   "source": [
    "### 3.2 Comparative Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db48e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparative_voxel_visualizations(model_data: Dict, loader: ExperimentLoader,\n",
    "                                           output_manager: OutputManager):\n",
    "    \"\"\"Create side-by-side 3D voxel reconstructions for all the best layers\"\"\"\n",
    "    \n",
    "    from omegaconf import OmegaConf\n",
    "    from hydra import compose, initialize_config_dir\n",
    "    from hydra.core.global_hydra import GlobalHydra\n",
    "    \n",
    "    GlobalHydra.instance().clear()\n",
    "    \n",
    "    try:\n",
    "        config_dir = str(Path(\"../configs\").resolve())\n",
    "        with initialize_config_dir(config_dir=config_dir, version_base=None):\n",
    "            cfg = compose(config_name=\"datasets/shapenet_voxel_meshes\").datasets\n",
    "            cfg.categories = [\"chair\"]\n",
    "            cfg.dataloader = {'batch_size': 1, 'num_workers': 0}\n",
    "            \n",
    "            _, _, test_loader = create_3dr2n2_reconstruction_dataloaders(\n",
    "                cfg, batch_size=1, num_workers=0\n",
    "            )\n",
    "            \n",
    "            sample = next(iter(test_loader))\n",
    "            gt_voxels = sample['voxel_gt'][0].squeeze().numpy()\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=len(model_data) + 1,\n",
    "            subplot_titles=['Ground Truth'] + [m.upper() for m in model_data.keys()],\n",
    "            specs=[[{'type': 'scatter3d'} for _ in range(len(model_data) + 1)]],\n",
    "            horizontal_spacing=0.02\n",
    "        )\n",
    "        \n",
    "        # add GT data\n",
    "        gt_points = np.argwhere(gt_voxels)\n",
    "        if len(gt_points) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=gt_points[:, 0],\n",
    "                    y=gt_points[:, 1],\n",
    "                    z=gt_points[:, 2],\n",
    "                    mode='markers',\n",
    "                    marker=dict(size=3, color='gray', opacity=0.8),\n",
    "                    name='Ground Truth',\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # add the preds for each model \n",
    "        for idx, (model_name, data) in enumerate(model_data.items()):\n",
    "            exp = data['experiment']\n",
    "            best_layer = data['layers'][np.argmax(data['metrics']['iou'])]\n",
    "            \n",
    "            # load all the probes & features\n",
    "            probe_files = loader.get_probe_files(exp)\n",
    "            feature_files = loader.get_feature_files(exp)\n",
    "            \n",
    "            if 'voxel' in probe_files and best_layer in probe_files['voxel']:\n",
    "                probe_path = probe_files['voxel'][best_layer]\n",
    "                features_path = feature_files[best_layer]['test']\n",
    "                \n",
    "                probe_state = torch.load(probe_path, map_location=device)\n",
    "                probe_config = probe_state.get('probe_config', {\n",
    "                    'type': 'voxel',\n",
    "                    'input_dim': 18624,\n",
    "                    'voxel_resolution': 32\n",
    "                })\n",
    "                \n",
    "                probe = create_probe(probe_config)\n",
    "                probe.load_state_dict(probe_state['model_state_dict'])\n",
    "                probe.to(device).eval()\n",
    "                \n",
    "                with open(features_path, 'rb') as f:\n",
    "                    features_data = pickle.load(f)\n",
    "                \n",
    "                if 'view_data' in features_data:\n",
    "                    features = features_data['view_data'][0:1]\n",
    "                else:\n",
    "                    features = features_data['features'][0:1]\n",
    "                \n",
    "                features = torch.tensor(features).to(device).view(1, -1)\n",
    "                \n",
    "                # get prediction \n",
    "                with torch.no_grad():\n",
    "                    pred_logits = probe(features)\n",
    "                    pred_voxels = (torch.sigmoid(pred_logits) > 0.5).squeeze().cpu().numpy()\n",
    "                \n",
    "                # add 2 plot\n",
    "                pred_points = np.argwhere(pred_voxels)\n",
    "                if len(pred_points) > 0:\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter3d(\n",
    "                            x=pred_points[:, 0],\n",
    "                            y=pred_points[:, 1],\n",
    "                            z=pred_points[:, 2],\n",
    "                            mode='markers',\n",
    "                            marker=dict(\n",
    "                                size=3,\n",
    "                                color=VisualConfig.MODEL_COLORS.get(model_name, '#333'),\n",
    "                                opacity=0.8\n",
    "                            ),\n",
    "                            name=model_name.upper(),\n",
    "                            showlegend=False\n",
    "                        ),\n",
    "                        row=1, col=idx + 2\n",
    "                    )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=dict(\n",
    "                text='Comparative Voxel Reconstructions (Best Layers)',\n",
    "                font=dict(size=VisualConfig.TITLE_SIZE, family=VisualConfig.FONT_FAMILY)\n",
    "            ),\n",
    "            scene=dict(\n",
    "                aspectmode='cube',\n",
    "                xaxis=dict(range=[0, 32], showticklabels=False),\n",
    "                yaxis=dict(range=[0, 32], showticklabels=False),\n",
    "                zaxis=dict(range=[0, 32], showticklabels=False)\n",
    "            ),\n",
    "            width=300 * (len(model_data) + 1),\n",
    "            height=400\n",
    "        )\n",
    "        \n",
    "        for i in range(len(model_data) + 1):\n",
    "            fig.update_scenes(\n",
    "                dict(\n",
    "                    aspectmode='cube',\n",
    "                    xaxis=dict(range=[0, 32], showticklabels=False),\n",
    "                    yaxis=dict(range=[0, 32], showticklabels=False),\n",
    "                    zaxis=dict(range=[0, 32], showticklabels=False)\n",
    "                ),\n",
    "                row=1, col=i + 1\n",
    "            )\n",
    "        \n",
    "        output_manager.save_plotly_figure(fig, 'voxel_reconstruction_comparison_3d')\n",
    "        fig.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not create 3D visualizations: {e}\")\n",
    "    finally:\n",
    "        GlobalHydra.instance().clear()\n",
    "\n",
    "#===================================================================================================\n",
    "# RUNNING THE ANALYSIS\n",
    "#===================================================================================================\n",
    "if voxel_model_data:\n",
    "    create_comparative_voxel_visualizations(voxel_model_data, loader, output_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab5a3ac",
   "metadata": {},
   "source": [
    "### 3.3 Analysis of Layer Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30eb1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_voxel_layer_evolution(model_data: Dict, output_manager: OutputManager):\n",
    "    \"\"\"Analyze how voxel reconstruction quality evolves through layers\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=VisualConfig.QUAD_FIG_SIZE)\n",
    "    \n",
    "    # evolution rate of the particular metric \n",
    "    ax_evolution = axes[0, 0]\n",
    "    for model_name, data in model_data.items():\n",
    "        iou_values = data['metrics']['iou']\n",
    "        if len(iou_values) > 1:\n",
    "            # rate of change?\n",
    "            evolution_rate = np.diff(iou_values) / np.diff(data['layers'])\n",
    "            ax_evolution.plot(data['layers'][1:], evolution_rate,\n",
    "                            'o-', linewidth=VisualConfig.LINE_WIDTH,\n",
    "                            markersize=VisualConfig.MARKER_SIZE,\n",
    "                            color=VisualConfig.MODEL_COLORS.get(model_name, '#333'),\n",
    "                            label=model_name.upper(),\n",
    "                            alpha=VisualConfig.ALPHA)\n",
    "    \n",
    "    ax_evolution.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax_evolution.set_xlabel('Layer')\n",
    "    ax_evolution.set_ylabel('IoU Change Rate')\n",
    "    ax_evolution.set_title('Rate of Performance Change')\n",
    "    ax_evolution.legend()\n",
    "    ax_evolution.grid(True, alpha=VisualConfig.GRID_ALPHA)\n",
    "    \n",
    "    ax_cumulative = axes[0, 1]\n",
    "    for model_name, data in model_data.items():\n",
    "        cumulative_iou = np.cumsum(data['metrics']['iou']) / np.arange(1, len(data['metrics']['iou']) + 1)\n",
    "        ax_cumulative.plot(data['layers'], cumulative_iou,\n",
    "                          'o-', linewidth=VisualConfig.LINE_WIDTH,\n",
    "                          markersize=VisualConfig.MARKER_SIZE,\n",
    "                          color=VisualConfig.MODEL_COLORS.get(model_name, '#333'),\n",
    "                          label=model_name.upper(),\n",
    "                          alpha=VisualConfig.ALPHA)\n",
    "    \n",
    "    ax_cumulative.set_xlabel('Layer')\n",
    "    ax_cumulative.set_ylabel('Cumulative Average IoU')\n",
    "    ax_cumulative.set_title('Cumulative Performance')\n",
    "    ax_cumulative.legend()\n",
    "    ax_cumulative.grid(True, alpha=VisualConfig.GRID_ALPHA)\n",
    "    ax_cumulative.set_ylim([0, 1])\n",
    "    \n",
    "    # precision / recall tradeoff \n",
    "    ax_pr = axes[1, 0]\n",
    "    for model_name, data in model_data.items():\n",
    "        precision = data['metrics']['precision']\n",
    "        recall = data['metrics']['recall']\n",
    "        \n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(data['layers'])))\n",
    "        \n",
    "        for i in range(len(data['layers']) - 1):\n",
    "            ax_pr.plot([recall[i], recall[i+1]], [precision[i], precision[i+1]],\n",
    "                      color=VisualConfig.MODEL_COLORS.get(model_name, '#333'),\n",
    "                      alpha=0.6, linewidth=2)\n",
    "        \n",
    "        ax_pr.scatter(recall[0], precision[0], s=100, \n",
    "                     color=VisualConfig.MODEL_COLORS.get(model_name, '#333'),\n",
    "                     marker='o', label=f'{model_name.upper()} (start)')\n",
    "        ax_pr.scatter(recall[-1], precision[-1], s=100,\n",
    "                     color=VisualConfig.MODEL_COLORS.get(model_name, '#333'),\n",
    "                     marker='s', label=f'{model_name.upper()} (end)')\n",
    "    \n",
    "    ax_pr.set_xlabel('Recall')\n",
    "    ax_pr.set_ylabel('Precision')\n",
    "    ax_pr.set_title('Precision-Recall Evolution')\n",
    "    ax_pr.legend()\n",
    "    ax_pr.grid(True, alpha=VisualConfig.GRID_ALPHA)\n",
    "    ax_pr.set_xlim([0, 1])\n",
    "    ax_pr.set_ylim([0, 1])\n",
    "    \n",
    "    ax_corr = axes[1, 1]\n",
    "    for model_name, data in model_data.items():\n",
    "        metrics_array = np.array([\n",
    "            data['metrics']['iou'],\n",
    "            data['metrics']['precision'],\n",
    "            data['metrics']['recall'],\n",
    "            data['metrics']['f1']\n",
    "        ])\n",
    "        \n",
    "        # iou vs f1 correlation \n",
    "        ax_corr.scatter(data['metrics']['iou'], data['metrics']['f1'],\n",
    "                       s=80, alpha=0.7,\n",
    "                       color=VisualConfig.MODEL_COLORS.get(model_name, '#333'),\n",
    "                       label=model_name.upper())\n",
    "    \n",
    "    ax_corr.set_xlabel('IoU Score')\n",
    "    ax_corr.set_ylabel('F1 Score')\n",
    "    ax_corr.set_title('IoU vs F1 Score Correlation')\n",
    "    ax_corr.legend()\n",
    "    ax_corr.grid(True, alpha=VisualConfig.GRID_ALPHA)\n",
    "    ax_corr.set_xlim([0, 1])\n",
    "    ax_corr.set_ylim([0, 1])\n",
    "    \n",
    "    plt.suptitle('Voxel Reconstruction Layer Evolution Analysis',\n",
    "                fontsize=VisualConfig.TITLE_SIZE + 2, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_manager.save_figure(fig, 'voxel_layer_evolution', save_subplots=True)\n",
    "    plt.show()\n",
    "\n",
    "# Analyze layer evolution\n",
    "if voxel_model_data:\n",
    "    analyze_voxel_layer_evolution(voxel_model_data, output_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495fedd4",
   "metadata": {},
   "source": [
    "## 4. Viewpoint Estimation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df27eff",
   "metadata": {},
   "source": [
    "### 4.1 Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_viewpoint_estimation_all_models(experiments: List[ExperimentResult],\n",
    "                                           output_manager: OutputManager):\n",
    "    \"\"\"Comprehensive viewpoint estimation analysis for all models\"\"\"\n",
    "    \n",
    "    model_probe_data = {}\n",
    "    \n",
    "    for exp in experiments:\n",
    "        if exp.model not in model_probe_data:\n",
    "            model_probe_data[exp.model] = {}\n",
    "        \n",
    "        metrics_dict = exp.results.get('results', {})\n",
    "        \n",
    "        for probe_type in ['linear', 'mlp']:\n",
    "            layers = []\n",
    "            metrics = {\n",
    "                'mae': [],\n",
    "                'rmse': [],\n",
    "                'r2': [],\n",
    "                'angular_distance': [],\n",
    "                'azimuth_mae': [],\n",
    "                'elevation_mae': []\n",
    "            }\n",
    "            \n",
    "            for layer_key, layer_data in metrics_dict.items():\n",
    "                if layer_key.startswith('layer_') and probe_type in layer_data:\n",
    "                    layer_num = int(layer_key.split('_')[1])\n",
    "                    test_metrics = layer_data[probe_type].get('test_metrics', {})\n",
    "                    \n",
    "                    layers.append(layer_num)\n",
    "                    metrics['mae'].append(test_metrics.get('mae', float('inf')))\n",
    "                    metrics['rmse'].append(test_metrics.get('rmse', float('inf')))\n",
    "                    metrics['r2'].append(test_metrics.get('r2', 0))\n",
    "                    metrics['angular_distance'].append(test_metrics.get('angular_distance_mean', float('inf')))\n",
    "                    metrics['azimuth_mae'].append(test_metrics.get('azimuth_mae', float('inf')))\n",
    "                    metrics['elevation_mae'].append(test_metrics.get('elevation_mae', float('inf')))\n",
    "            \n",
    "            if layers:\n",
    "                sorted_idx = np.argsort(layers)\n",
    "                model_probe_data[exp.model][probe_type] = {\n",
    "                    'layers': np.array(layers)[sorted_idx],\n",
    "                    'metrics': {k: np.array(v)[sorted_idx] for k, v in metrics.items()},\n",
    "                    'experiment': exp\n",
    "                }\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    gs = fig.add_gridspec(4, 2, hspace=0.3, wspace=0.25)\n",
    "    \n",
    "    # MAE comparison\n",
    "    ax_mae = fig.add_subplot(gs[0, 0])\n",
    "    for model_name, probe_data in model_probe_data.items():\n",
    "        for probe_type, data in probe_data.items():\n",
    "            linestyle = '-' if probe_type == 'linear' else '--'\n",
    "            ax_mae.plot(data['layers'], data['metrics']['mae'],\n",
    "                       linestyle=linestyle, linewidth=VisualConfig.LINE_WIDTH,\n",
    "                       marker='o' if probe_type == 'linear' else 's',\n",
    "                       markersize=VisualConfig.MARKER_SIZE,\n",
    "                       color=VisualConfig.MODEL_COLORS.get(model_name, '#333'),\n",
    "                       label=f'{model_name.upper()} ({probe_type})',\n",
    "                       alpha=VisualConfig.ALPHA)\n",
    "    \n",
    "    ax_mae.set_xlabel('Layer')\n",
    "    ax_mae.set_ylabel('MAE')\n",
    "    ax_mae.set_title('Mean Absolute Error Comparison')\n",
    "    ax_mae.legend(fontsize=VisualConfig.LEGEND_SIZE - 2)\n",
    "    ax_mae.grid(True, alpha=VisualConfig.GRID_ALPHA)\n",
    "    \n",
    "    # Angular distance comparison\n",
    "    ax_angular = fig.add_subplot(gs[0, 1])\n",
    "    for model_name, probe_data in model_probe_data.items():\n",
    "        for probe_type, data in probe_data.items():\n",
    "            if 'angular_distance' in data['metrics']:\n",
    "                linestyle = '-' if probe_type == 'linear' else '--'\n",
    "                ax_angular.plot(data['layers'], data['metrics']['angular_distance'],\n",
    "                               linestyle=linestyle, linewidth=VisualConfig.LINE_WIDTH,\n",
    "                               marker='o' if probe_type == 'linear' else 's',\n",
    "                               markersize=VisualConfig.MARKER_SIZE,\n",
    "                               color=VisualConfig.MODEL_COLORS.get(model_name, '#333'),\n",
    "                               label=f'{model_name.upper()} ({probe_type})',\n",
    "                               alpha=VisualConfig.ALPHA)\n",
    "    \n",
    "    ax_angular.set_xlabel('Layer')\n",
    "    ax_angular.set_ylabel('Angular Distance (degrees)')\n",
    "    ax_angular.set_title('Mean Angular Distance Comparison')\n",
    "    ax_angular.legend(fontsize=VisualConfig.LEGEND_SIZE - 2)\n",
    "    ax_angular.grid(True, alpha=VisualConfig.GRID_ALPHA)\n",
    "    \n",
    "    # Linearity gap analysis\n",
    "    ax_linearity = fig.add_subplot(gs[1, :])\n",
    "    for model_name, probe_data in model_probe_data.items():\n",
    "        if 'linear' in probe_data and 'mlp' in probe_data:\n",
    "            linear_data = probe_data['linear']\n",
    "            mlp_data = probe_data['mlp']\n",
    "            \n",
    "            common_layers = np.intersect1d(linear_data['layers'], mlp_data['layers'])\n",
    "            if len(common_layers) > 0:\n",
    "                linear_mae = [linear_data['metrics']['mae'][np.where(linear_data['layers'] == l)[0][0]] \n",
    "                             for l in common_layers]\n",
    "                mlp_mae = [mlp_data['metrics']['mae'][np.where(mlp_data['layers'] == l)[0][0]] \n",
    "                          for l in common_layers]\n",
    "                \n",
    "                linearity_gap = np.array(linear_mae) - np.array(mlp_mae)\n",
    "                \n",
    "                ax_linearity.plot(common_layers, linearity_gap,\n",
    "                                 'o-', linewidth=VisualConfig.LINE_WIDTH * 1.2,\n",
    "                                 markersize=VisualConfig.MARKER_SIZE * 1.2,\n",
    "                                 color=VisualConfig.MODEL_COLORS.get(model_name, '#333'),\n",
    "                                 label=model_name.upper(),\n",
    "                                 alpha=VisualConfig.ALPHA)\n",
    "    \n",
    "    ax_linearity.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax_linearity.set_xlabel('Layer', fontsize=VisualConfig.LABEL_SIZE)\n",
    "    ax_linearity.set_ylabel('Linearity Gap (Linear MAE - MLP MAE)', fontsize=VisualConfig.LABEL_SIZE)\n",
    "    ax_linearity.set_title('Linearity of Viewpoint Representation', fontsize=VisualConfig.TITLE_SIZE)\n",
    "    ax_linearity.legend(fontsize=VisualConfig.LEGEND_SIZE)\n",
    "    ax_linearity.grid(True, alpha=VisualConfig.GRID_ALPHA)\n",
    "    \n",
    "    ax_linearity.text(0.02, 0.98, 'Higher = Less Linear\\n(MLP much better than Linear)',\n",
    "                     transform=ax_linearity.transAxes,\n",
    "                     verticalalignment='top',\n",
    "                     bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # R^2 score comparison\n",
    "    ax_r2 = fig.add_subplot(gs[2, 0])\n",
    "    for model_name, probe_data in model_probe_data.items():\n",
    "        for probe_type, data in probe_data.items():\n",
    "            if 'r2' in data['metrics']:\n",
    "                linestyle = '-' if probe_type == 'linear' else '--'\n",
    "                ax_r2.plot(data['layers'], data['metrics']['r2'],\n",
    "                          linestyle=linestyle, linewidth=VisualConfig.LINE_WIDTH,\n",
    "                          marker='o' if probe_type == 'linear' else 's',\n",
    "                          markersize=VisualConfig.MARKER_SIZE,\n",
    "                          color=VisualConfig.MODEL_COLORS.get(model_name, '#333'),\n",
    "                          label=f'{model_name.upper()} ({probe_type})',\n",
    "                          alpha=VisualConfig.ALPHA)\n",
    "    \n",
    "    ax_r2.set_xlabel('Layer')\n",
    "    ax_r2.set_ylabel('R² Score')\n",
    "    ax_r2.set_title('R² Score Comparison')\n",
    "    ax_r2.legend(fontsize=VisualConfig.LEGEND_SIZE - 2)\n",
    "    ax_r2.grid(True, alpha=VisualConfig.GRID_ALPHA)\n",
    "    ax_r2.set_ylim([0, 1])\n",
    "    \n",
    "    # Azimuth vs Elevation error\n",
    "    ax_az_el = fig.add_subplot(gs[2, 1])\n",
    "    for model_name, probe_data in model_probe_data.items():\n",
    "        for probe_type, data in probe_data.items():\n",
    "            if 'azimuth_mae' in data['metrics'] and 'elevation_mae' in data['metrics']:\n",
    "                best_idx = np.argmin(data['metrics']['mae'])\n",
    "                marker = 'o' if probe_type == 'linear' else 's'\n",
    "                ax_az_el.scatter(data['metrics']['azimuth_mae'][best_idx],\n",
    "                               data['metrics']['elevation_mae'][best_idx],\n",
    "                               s=150, marker=marker,\n",
    "                               color=VisualConfig.MODEL_COLORS.get(model_name, '#333'),\n",
    "                               alpha=VisualConfig.ALPHA,\n",
    "                               label=f'{model_name.upper()} ({probe_type})')\n",
    "    \n",
    "    ax_az_el.set_xlabel('Azimuth MAE (degrees)')\n",
    "    ax_az_el.set_ylabel('Elevation MAE (degrees)')\n",
    "    ax_az_el.set_title('Azimuth vs Elevation Error (Best Layers)')\n",
    "    ax_az_el.legend(fontsize=VisualConfig.LEGEND_SIZE - 2)\n",
    "    ax_az_el.grid(True, alpha=VisualConfig.GRID_ALPHA)\n",
    "    \n",
    "    ax_summary = fig.add_subplot(gs[3, :])\n",
    "    summary_data = []\n",
    "    models = []\n",
    "    \n",
    "    for model_name, probe_data in model_probe_data.items():\n",
    "        for probe_type, data in probe_data.items():\n",
    "            best_idx = np.argmin(data['metrics']['mae'])\n",
    "            best_layer = data['layers'][best_idx]\n",
    "            best_mae = data['metrics']['mae'][best_idx]\n",
    "            \n",
    "            summary_data.append({\n",
    "                'model': model_name.upper(),\n",
    "                'probe': probe_type,\n",
    "                'layer': best_layer,\n",
    "                'mae': best_mae\n",
    "            })\n",
    "            models.append(f'{model_name.upper()}\\n({probe_type})')\n",
    "    \n",
    "    x_pos = np.arange(len(models))\n",
    "    mae_values = [d['mae'] for d in summary_data]\n",
    "    colors = [VisualConfig.MODEL_COLORS.get(d['model'].lower(), '#333') for d in summary_data]\n",
    "    \n",
    "    bars = ax_summary.bar(x_pos, mae_values, color=colors, alpha=VisualConfig.ALPHA)\n",
    "    \n",
    "    for i, (bar, data) in enumerate(zip(bars, summary_data)):\n",
    "        height = bar.get_height()\n",
    "        ax_summary.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'L{data[\"layer\"]}',\n",
    "                       ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    ax_summary.set_xlabel('Model & Probe Type')\n",
    "    ax_summary.set_ylabel('Best MAE')\n",
    "    ax_summary.set_title('Best Performance Summary')\n",
    "    ax_summary.set_xticks(x_pos)\n",
    "    ax_summary.set_xticklabels(models, fontsize=10)\n",
    "    ax_summary.grid(True, alpha=VisualConfig.GRID_ALPHA, axis='y')\n",
    "    \n",
    "    plt.suptitle('Comprehensive Viewpoint Estimation Analysis',\n",
    "                fontsize=VisualConfig.TITLE_SIZE + 2, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_manager.save_figure(fig, 'viewpoint_estimation_all_models', save_subplots=True)\n",
    "    plt.show()\n",
    "    \n",
    "    raw_data = {}\n",
    "    for model_name, probe_data in model_probe_data.items():\n",
    "        raw_data[model_name] = {}\n",
    "        for probe_type, data in probe_data.items():\n",
    "            best_idx = np.argmin(data['metrics']['mae'])\n",
    "            raw_data[model_name][probe_type] = {\n",
    "                'layers': data['layers'].tolist(),\n",
    "                'metrics': {k: v.tolist() for k, v in data['metrics'].items()},\n",
    "                'best_layer': int(data['layers'][best_idx]),\n",
    "                'best_mae': float(data['metrics']['mae'][best_idx])\n",
    "            }\n",
    "    \n",
    "    output_manager.save_data(raw_data, 'viewpoint_estimation_metrics')\n",
    "    \n",
    "    return model_probe_data\n",
    "\n",
    "viewpoint_model_data = analyze_viewpoint_estimation_all_models(\n",
    "    categorized_experiments['viewpoint_estimation'],\n",
    "    output_manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a7060",
   "metadata": {},
   "source": [
    "### 4.2 Viewpoint Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_viewpoint_error_analysis(model_probe_data: Dict, loader: ExperimentLoader,\n",
    "                                   output_manager: OutputManager):\n",
    "    \"\"\"Create visualizations of the error with advanced stuff\"\"\"\n",
    "    \n",
    "    best_configs = {}\n",
    "    for model_name, probe_data in model_probe_data.items():\n",
    "        best_mae = float('inf')\n",
    "        best_config = None\n",
    "        \n",
    "        for probe_type, data in probe_data.items():\n",
    "            min_idx = np.argmin(data['metrics']['mae'])\n",
    "            if data['metrics']['mae'][min_idx] < best_mae:\n",
    "                best_mae = data['metrics']['mae'][min_idx]\n",
    "                best_config = {\n",
    "                    'model': model_name,\n",
    "                    'probe_type': probe_type,\n",
    "                    'layer': data['layers'][min_idx],\n",
    "                    'experiment': data['experiment'],\n",
    "                    'mae': best_mae\n",
    "                }\n",
    "        \n",
    "        if best_config:\n",
    "            best_configs[model_name] = best_config\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(2, len(best_configs), hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    for idx, (model_name, config) in enumerate(best_configs.items()):\n",
    "        try:\n",
    "            exp = config['experiment']\n",
    "            probe_files = loader.get_probe_files(exp)\n",
    "            feature_files = loader.get_feature_files(exp)\n",
    "            \n",
    "            if config['probe_type'] not in probe_files:\n",
    "                continue\n",
    "            if config['layer'] not in probe_files[config['probe_type']]:\n",
    "                continue\n",
    "                \n",
    "            probe_path = probe_files[config['probe_type']][config['layer']]\n",
    "            features_path = feature_files[config['layer']]['test']\n",
    "            \n",
    "            probe_state = torch.load(probe_path, map_location=device)\n",
    "            original_config = probe_state.get('probe_config', {\n",
    "                'type': config['probe_type'],\n",
    "                'input_dim': 768,\n",
    "                'output_dim': 2,\n",
    "                'task_type': 'regression'\n",
    "            })\n",
    "            \n",
    "            if config['probe_type'] == 'mlp' and 'hidden_dims' not in original_config:\n",
    "                original_config['hidden_dims'] = [256]\n",
    "            \n",
    "            probe = create_probe(original_config)\n",
    "            probe.load_state_dict(probe_state['model_state_dict'])\n",
    "            probe.to(device).eval()\n",
    "            \n",
    "            with open(features_path, 'rb') as f:\n",
    "                test_data = pickle.load(f)\n",
    "            \n",
    "            n_samples = min(1000, len(test_data['features']))\n",
    "            features = torch.tensor(test_data['features'][:n_samples])\n",
    "            targets = torch.tensor(test_data['targets'][:n_samples])\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                predictions = probe(features.to(device)).cpu().numpy()\n",
    "            \n",
    "            targets = targets.numpy()\n",
    "            \n",
    "            pred_azimuth = (predictions[:, 0] + 1) * 180\n",
    "            true_azimuth = (targets[:, 0] + 1) * 180\n",
    "            pred_elevation = predictions[:, 1] * 90\n",
    "            true_elevation = targets[:, 1] * 90\n",
    "            \n",
    "            az_error = np.abs(pred_azimuth - true_azimuth)\n",
    "            az_error = np.minimum(az_error, 360 - az_error)\n",
    "            el_error = np.abs(pred_elevation - true_elevation)\n",
    "            \n",
    "            ax_az = fig.add_subplot(gs[0, idx])\n",
    "            scatter = ax_az.scatter(true_azimuth, pred_azimuth, c=az_error,\n",
    "                                   cmap='viridis', alpha=0.5, s=10)\n",
    "            ax_az.plot([0, 360], [0, 360], 'r--', alpha=0.5, linewidth=1)\n",
    "            ax_az.set_xlabel('True Azimuth (°)')\n",
    "            ax_az.set_ylabel('Predicted Azimuth (°)')\n",
    "            ax_az.set_title(f'{model_name.upper()}\\n{config[\"probe_type\"]} L{config[\"layer\"]}')\n",
    "            ax_az.grid(True, alpha=VisualConfig.GRID_ALPHA)\n",
    "            \n",
    "            ax_el = fig.add_subplot(gs[1, idx])\n",
    "            scatter = ax_el.scatter(true_elevation, pred_elevation, c=el_error,\n",
    "                                   cmap='viridis', alpha=0.5, s=10)\n",
    "            ax_el.plot([-90, 90], [-90, 90], 'r--', alpha=0.5, linewidth=1)\n",
    "            ax_el.set_xlabel('True Elevation (°)')\n",
    "            ax_el.set_ylabel('Predicted Elevation (°)')\n",
    "            ax_el.set_title(f'MAE: {config[\"mae\"]:.2f}°')\n",
    "            ax_el.grid(True, alpha=VisualConfig.GRID_ALPHA)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Could not create visualization for {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    plt.suptitle('Viewpoint Prediction Error Analysis (Best Configurations)',\n",
    "                fontsize=VisualConfig.TITLE_SIZE + 2, fontweight='bold')\n",
    "    \n",
    "    output_manager.save_figure(fig, 'viewpoint_error_analysis', save_subplots=True)\n",
    "    plt.show()\n",
    "    \n",
    "    create_3d_viewpoint_sphere(best_configs, loader, output_manager)\n",
    "\n",
    "def create_3d_viewpoint_sphere(best_configs: Dict, loader: ExperimentLoader,\n",
    "                               output_manager: OutputManager, n: Optional=10):\n",
    "    \"\"\"Create 3D sphere visualization of viewpoint errors\"\"\"\n",
    "    \n",
    "    best_model = min(best_configs.items(), key=lambda x: x[1]['mae'])\n",
    "    model_name, config = best_model\n",
    "    \n",
    "    try:\n",
    "        exp = config['experiment']\n",
    "        probe_files = loader.get_probe_files(exp)\n",
    "        feature_files = loader.get_feature_files(exp)\n",
    "        \n",
    "        probe_path = probe_files[config['probe_type']][config['layer']]\n",
    "        features_path = feature_files[config['layer']]['test']\n",
    "        \n",
    "        probe_state = torch.load(probe_path, map_location=device)\n",
    "        original_config = probe_state.get('probe_config', {\n",
    "            'type': config['probe_type'],\n",
    "            'input_dim': 768,\n",
    "            'output_dim': 2,\n",
    "            'task_type': 'regression'\n",
    "        })\n",
    "        \n",
    "        if config['probe_type'] == 'mlp' and 'hidden_dims' not in original_config:\n",
    "            original_config['hidden_dims'] = [256]\n",
    "        \n",
    "        probe = create_probe(original_config)\n",
    "        probe.load_state_dict(probe_state['model_state_dict'])\n",
    "        probe.to(device).eval()\n",
    "        \n",
    "        with open(features_path, 'rb') as f:\n",
    "            test_data = pickle.load(f)\n",
    "        \n",
    "        n_samples = min(500, len(test_data['features']))\n",
    "        features = torch.tensor(test_data['features'][:n_samples])\n",
    "        targets = torch.tensor(test_data['targets'][:n_samples])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = probe(features.to(device)).cpu().numpy()\n",
    "        \n",
    "        targets = targets.numpy()\n",
    "        \n",
    "        def viewpoint_to_xyz(azimuth, elevation, radius=1):\n",
    "            az_rad = np.deg2rad(azimuth)\n",
    "            el_rad = np.deg2rad(elevation)\n",
    "            x = radius * np.cos(el_rad) * np.cos(az_rad)\n",
    "            y = radius * np.cos(el_rad) * np.sin(az_rad)\n",
    "            z = radius * np.sin(el_rad)\n",
    "            return x, y, z\n",
    "        \n",
    "        true_az = (targets[:, 0] + 1) * 180\n",
    "        true_el = targets[:, 1] * 90\n",
    "        pred_az = (predictions[:, 0] + 1) * 180\n",
    "        pred_el = predictions[:, 1] * 90\n",
    "        \n",
    "        true_x, true_y, true_z = viewpoint_to_xyz(true_az, true_el)\n",
    "        pred_x, pred_y, pred_z = viewpoint_to_xyz(pred_az, pred_el)\n",
    "        \n",
    "        errors = np.sqrt((pred_x - true_x)**2 + (pred_y - true_y)**2 + (pred_z - true_z)**2)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        u = np.linspace(0, 2 * np.pi, 50)\n",
    "        v = np.linspace(0, np.pi, 50)\n",
    "        sphere_x = np.outer(np.cos(u), np.sin(v))\n",
    "        sphere_y = np.outer(np.sin(u), np.sin(v))\n",
    "        sphere_z = np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "        \n",
    "        fig.add_trace(go.Surface(\n",
    "            x=sphere_x, y=sphere_y, z=sphere_z,\n",
    "            opacity=0.1, colorscale='gray',\n",
    "            showscale=False, name='Unit Sphere'\n",
    "        ))\n",
    "        \n",
    "        for i in range(n):\n",
    "            color = plt.cm.viridis(errors[i] / errors.max())\n",
    "            color_rgb = f'rgb({int(color[0]*255)},{int(color[1]*255)},{int(color[2]*255)})'\n",
    "            \n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=[true_x[i], pred_x[i]],\n",
    "                y=[true_y[i], pred_y[i]],\n",
    "                z=[true_z[i], pred_z[i]],\n",
    "                mode='lines+markers',\n",
    "                line=dict(color=color_rgb, width=2),\n",
    "                marker=dict(size=[3, 5], color=['blue', 'red']),\n",
    "                showlegend=False,\n",
    "                hovertemplate=f'Error: {errors[i]:.3f}<br>True: ({true_az[i]:.1f}°, {true_el[i]:.1f}°)<br>Pred: ({pred_az[i]:.1f}°, {pred_el[i]:.1f}°)'\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=dict(\n",
    "                text=f'Viewpoint Prediction on Unit Sphere (n={n}) - {model_name.upper()} ({config[\"probe_type\"]} L{config[\"layer\"]})',\n",
    "                font=dict(size=VisualConfig.TITLE_SIZE, family=VisualConfig.FONT_FAMILY)\n",
    "            ),\n",
    "            scene=dict(\n",
    "                xaxis_title='X',\n",
    "                yaxis_title='Y',\n",
    "                zaxis_title='Z',\n",
    "                aspectmode='cube',\n",
    "                camera=dict(eye=dict(x=1.5, y=1.5, z=1.5))\n",
    "            ),\n",
    "            width=1000,\n",
    "            height=800,\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[None], y=[None], z=[None],\n",
    "            mode='markers',\n",
    "            marker=dict(size=8, color='blue'),\n",
    "            name='True Viewpoint'\n",
    "        ))\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[None], y=[None], z=[None],\n",
    "            mode='markers',\n",
    "            marker=dict(size=8, color='red'),\n",
    "            name='Predicted Viewpoint'\n",
    "        ))\n",
    "        \n",
    "        output_manager.save_plotly_figure(fig, 'viewpoint_sphere_best_model')\n",
    "        fig.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not create sphere visualization: {e}\")\n",
    "\n",
    "if viewpoint_model_data:\n",
    "    create_viewpoint_error_analysis(viewpoint_model_data, loader, output_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef75998",
   "metadata": {},
   "source": [
    "## 5. Comparative Analysis of Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f314f3d",
   "metadata": {},
   "source": [
    "### 5.1 Cross-Task Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745fd5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_task_comparison(all_experiments: Dict[str, ExperimentResult],\n",
    "                                output_manager: OutputManager):\n",
    "    \"\"\"Compare model performance across both tasks\"\"\"\n",
    "    \n",
    "    performance_data = {}\n",
    "    \n",
    "    for exp in all_experiments.values():\n",
    "        if exp.model not in performance_data:\n",
    "            performance_data[exp.model] = {}\n",
    "        \n",
    "        best_layer, best_metric = exp.get_best_layer()\n",
    "        \n",
    "        if exp.task == 'viewpoint_estimation':\n",
    "            performance_data[exp.model]['viewpoint'] = {\n",
    "                'best_mae': best_metric,\n",
    "                'best_layer': best_layer\n",
    "            }\n",
    "        else:  # voxel_reconstruction\n",
    "            performance_data[exp.model]['voxel'] = {\n",
    "                'best_iou': best_metric,\n",
    "                'best_layer': best_layer\n",
    "            }\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=VisualConfig.DOUBLE_FIG_SIZE)\n",
    "    \n",
    "    models = list(performance_data.keys())\n",
    "    viewpoint_scores = []\n",
    "    voxel_scores = []\n",
    "    \n",
    "    for model in models:\n",
    "        viewpoint_mae = performance_data[model].get('viewpoint', {}).get('best_mae', 1.0)\n",
    "        voxel_iou = performance_data[model].get('voxel', {}).get('best_iou', 0.0)\n",
    "        \n",
    "        viewpoint_score = 1.0 - min(viewpoint_mae, 1.0)\n",
    "        viewpoint_scores.append(viewpoint_score)\n",
    "        voxel_scores.append(voxel_iou)\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, viewpoint_scores, width, label='Viewpoint (1 - MAE)',\n",
    "                    color=[VisualConfig.MODEL_COLORS.get(m, '#333') for m in models],\n",
    "                    alpha=VisualConfig.ALPHA)\n",
    "    bars2 = ax1.bar(x + width/2, voxel_scores, width, label='Voxel (IoU)',\n",
    "                    color=[VisualConfig.MODEL_COLORS.get(m, '#333') for m in models],\n",
    "                    alpha=VisualConfig.ALPHA * 0.7)\n",
    "    \n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Normalized Score (0-1)')\n",
    "    ax1.set_title('Cross-Task Performance Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([m.upper() for m in models])\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=VisualConfig.GRID_ALPHA, axis='y')\n",
    "    ax1.set_ylim([0, 1])\n",
    "    \n",
    "    viewpoint_layers = [performance_data[m].get('viewpoint', {}).get('best_layer', 0) for m in models]\n",
    "    voxel_layers = [performance_data[m].get('voxel', {}).get('best_layer', 0) for m in models]\n",
    "    \n",
    "    ax2.scatter(viewpoint_layers, voxel_layers,\n",
    "               s=200, alpha=VisualConfig.ALPHA,\n",
    "               c=[VisualConfig.MODEL_COLORS.get(m, '#333') for m in models])\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        ax2.annotate(model.upper(), (viewpoint_layers[i], voxel_layers[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=VisualConfig.LABEL_SIZE - 2)\n",
    "    \n",
    "    max_layer = max(max(viewpoint_layers), max(voxel_layers))\n",
    "    ax2.plot([0, max_layer], [0, max_layer], 'k--', alpha=0.3)\n",
    "    \n",
    "    ax2.set_xlabel('Best Layer for Viewpoint')\n",
    "    ax2.set_ylabel('Best Layer for Voxel Reconstruction')\n",
    "    ax2.set_title('Optimal Layer Comparison Across Tasks')\n",
    "    ax2.grid(True, alpha=VisualConfig.GRID_ALPHA)\n",
    "    \n",
    "    plt.suptitle('Cross-Task Model Analysis', fontsize=VisualConfig.TITLE_SIZE + 2, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_manager.save_figure(fig, 'cross_task_comparison')\n",
    "    plt.show()\n",
    "    \n",
    "    output_manager.save_data(performance_data, 'cross_task_performance')\n",
    "\n",
    "create_cross_task_comparison(all_experiments, output_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a20429e",
   "metadata": {},
   "source": [
    "### 5.2 Model Architecture Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b6603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_architecture_impact(categorized_experiments: Dict, output_manager: OutputManager):\n",
    "    \"\"\"Analyze how different pre-training objectives affect 3D understanding\"\"\"\n",
    "    \n",
    "    model_categories = {\n",
    "        'Self-Supervised': ['dinov2', 'ijepa', 'mocov3'],\n",
    "        'Supervised': ['supervised_vit']\n",
    "    }\n",
    "    \n",
    "    architecture_features = {\n",
    "        'dinov2': {'type': 'Self-Distillation', 'objective': 'Knowledge Distillation'},\n",
    "        'ijepa': {'type': 'Predictive', 'objective': 'Masked Prediction'},\n",
    "        'supervised_vit': {'type': 'Supervised', 'objective': 'Classification'}\n",
    "    }\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    ax_objective = fig.add_subplot(gs[0, :2])\n",
    "    \n",
    "    objective_performance = {}\n",
    "    for model, exps in categorized_experiments['by_model'].items():\n",
    "        obj = architecture_features.get(model, {}).get('objective', 'Unknown')\n",
    "        if obj not in objective_performance:\n",
    "            objective_performance[obj] = {'viewpoint': [], 'voxel': []}\n",
    "        \n",
    "        for exp in exps:\n",
    "            best_layer, best_metric = exp.get_best_layer()\n",
    "            if exp.task == 'viewpoint_estimation':\n",
    "                objective_performance[obj]['viewpoint'].append(1 - min(best_metric, 1.0))\n",
    "            else:\n",
    "                objective_performance[obj]['voxel'].append(best_metric)\n",
    "    \n",
    "    objectives = list(objective_performance.keys())\n",
    "    x = np.arange(len(objectives))\n",
    "    width = 0.35\n",
    "    \n",
    "    viewpoint_means = [np.mean(objective_performance[obj]['viewpoint']) if objective_performance[obj]['viewpoint'] else 0 \n",
    "                      for obj in objectives]\n",
    "    voxel_means = [np.mean(objective_performance[obj]['voxel']) if objective_performance[obj]['voxel'] else 0 \n",
    "                  for obj in objectives]\n",
    "    \n",
    "    bars1 = ax_objective.bar(x - width/2, viewpoint_means, width, label='Viewpoint Task',\n",
    "                            color=VisualConfig.PROBE_COLORS['linear'], alpha=VisualConfig.ALPHA)\n",
    "    bars2 = ax_objective.bar(x + width/2, voxel_means, width, label='Voxel Task',\n",
    "                            color=VisualConfig.PROBE_COLORS['voxel'], alpha=VisualConfig.ALPHA)\n",
    "    \n",
    "    ax_objective.set_xlabel('Pre-training Objective')\n",
    "    ax_objective.set_ylabel('Average Performance')\n",
    "    ax_objective.set_title('Performance by Pre-training Objective')\n",
    "    ax_objective.set_xticks(x)\n",
    "    ax_objective.set_xticklabels(objectives, rotation=15, ha='right')\n",
    "    ax_objective.legend()\n",
    "    ax_objective.grid(True, alpha=VisualConfig.GRID_ALPHA, axis='y')\n",
    "    \n",
    "    ax_heatmap = fig.add_subplot(gs[0, 2])\n",
    "    \n",
    "    models = list(categorized_experiments['by_model'].keys())\n",
    "    tasks = ['viewpoint', 'voxel']\n",
    "    layer_matrix = np.zeros((len(models), len(tasks)))\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        for exp in categorized_experiments['by_model'][model]:\n",
    "            best_layer, _ = exp.get_best_layer()\n",
    "            j = 0 if exp.task == 'viewpoint_estimation' else 1\n",
    "            layer_matrix[i, j] = best_layer\n",
    "    \n",
    "    im = ax_heatmap.imshow(layer_matrix, cmap='viridis', aspect='auto')\n",
    "    ax_heatmap.set_xticks(np.arange(len(tasks)))\n",
    "    ax_heatmap.set_yticks(np.arange(len(models)))\n",
    "    ax_heatmap.set_xticklabels(['Viewpoint', 'Voxel'])\n",
    "    ax_heatmap.set_yticklabels([m.upper() for m in models])\n",
    "    ax_heatmap.set_title('Optimal Layer Heatmap')\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        for j in range(len(tasks)):\n",
    "            text = ax_heatmap.text(j, i, f'{int(layer_matrix[i, j])}',\n",
    "                                  ha='center', va='center', color='white', fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im, ax=ax_heatmap, label='Layer Number')\n",
    "    \n",
    "    ax_comparison = fig.add_subplot(gs[1, :])\n",
    "    \n",
    "    category_data = {'Self-Supervised': {'layers': [], 'viewpoint': [], 'voxel': []},\n",
    "                    'Supervised': {'layers': [], 'viewpoint': [], 'voxel': []}}\n",
    "    \n",
    "    for model, exps in categorized_experiments['by_model'].items():\n",
    "        category = 'Supervised' if model == 'supervised_vit' else 'Self-Supervised'\n",
    "        \n",
    "        for exp in exps:\n",
    "            results = exp.results.get('results', {})\n",
    "            for layer_key, layer_data in results.items():\n",
    "                if layer_key.startswith('layer_'):\n",
    "                    layer_num = int(layer_key.split('_')[1])\n",
    "                    \n",
    "                    if exp.task == 'viewpoint_estimation':\n",
    "                        mae_values = []\n",
    "                        for probe_type in ['linear', 'mlp']:\n",
    "                            if probe_type in layer_data:\n",
    "                                mae = layer_data[probe_type].get('test_metrics', {}).get('mae', 1.0)\n",
    "                                mae_values.append(mae)\n",
    "                        if mae_values:\n",
    "                            category_data[category]['layers'].append(layer_num)\n",
    "                            category_data[category]['viewpoint'].append(1 - min(mae_values))\n",
    "                    else:\n",
    "                        if 'voxel' in layer_data:\n",
    "                            iou = layer_data['voxel'].get('test_metrics', {}).get('voxel_iou', 0)\n",
    "                            category_data[category]['layers'].append(layer_num)\n",
    "                            category_data[category]['voxel'].append(iou)\n",
    "    \n",
    "    for category, data in category_data.items():\n",
    "        if data['viewpoint']:\n",
    "            unique_layers = sorted(set(data['layers']))\n",
    "            avg_viewpoint = []\n",
    "            for layer in unique_layers:\n",
    "                layer_indices = [i for i, l in enumerate(data['layers']) if l == layer]\n",
    "                if 'viewpoint' in data and any(i < len(data['viewpoint']) for i in layer_indices):\n",
    "                    avg = np.mean([data['viewpoint'][i] for i in layer_indices if i < len(data['viewpoint'])])\n",
    "                    avg_viewpoint.append(avg)\n",
    "                else:\n",
    "                    avg_viewpoint.append(0)\n",
    "            \n",
    "            linestyle = '-' if category == 'Self-Supervised' else '--'\n",
    "            ax_comparison.plot(unique_layers, avg_viewpoint,\n",
    "                             linestyle=linestyle, linewidth=VisualConfig.LINE_WIDTH * 1.2,\n",
    "                             marker='o', markersize=VisualConfig.MARKER_SIZE,\n",
    "                             label=f'{category} (Viewpoint)',\n",
    "                             alpha=VisualConfig.ALPHA)\n",
    "    \n",
    "    ax_comparison.set_xlabel('Layer')\n",
    "    ax_comparison.set_ylabel('Average Performance')\n",
    "    ax_comparison.set_title('Self-Supervised vs Supervised Learning: Layer-wise Performance')\n",
    "    ax_comparison.legend()\n",
    "    ax_comparison.grid(True, alpha=VisualConfig.GRID_ALPHA)\n",
    "    \n",
    "    plt.suptitle('Architecture and Pre-training Impact Analysis',\n",
    "                fontsize=VisualConfig.TITLE_SIZE + 2, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_manager.save_figure(fig, 'architecture_impact_analysis')\n",
    "    plt.show()\n",
    "\n",
    "analyze_architecture_impact(categorized_experiments, output_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387acdc3",
   "metadata": {},
   "source": [
    "### 5.3 Layer-Wise Evolution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef91f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_task_performance_heatmap(experiment: ExperimentResult, output_manager: OutputManager):\n",
    "    \"\"\"\n",
    "    Creates a performance heatmap that adapts to the experiment's task (viewpoint or voxel).\n",
    "    \"\"\"\n",
    "    task = experiment.task\n",
    "    model_name = experiment.model\n",
    "    results = experiment.results.get('results', {})\n",
    "    \n",
    "    if 'viewpoint' in task:\n",
    "        metric_key = 'mae'\n",
    "        probe_keys = ['linear', 'mlp']\n",
    "        cmap = 'viridis_r'  \n",
    "        title = f'Performance Heatmap (MAE) - {model_name.upper()}'\n",
    "    elif 'voxel' in task:\n",
    "        metric_key = 'voxel_iou'\n",
    "        probe_keys = ['voxel']\n",
    "        cmap = 'viridis'  \n",
    "        title = f'Performance Heatmap (IoU) - {model_name.upper()}'\n",
    "    else:\n",
    "        return \n",
    "\n",
    "    layers = sorted([int(lk.split('_')[1]) for lk in results.keys() if lk.startswith('layer_')])\n",
    "    data_matrix = np.full((len(probe_keys), len(layers)), np.nan)\n",
    "\n",
    "    for i, p_type in enumerate(probe_keys):\n",
    "        for j, layer in enumerate(layers):\n",
    "            metric_val = results.get(f'layer_{layer}', {}).get(p_type, {}).get('test_metrics', {}).get(metric_key)\n",
    "            if metric_val is not None:\n",
    "                data_matrix[i, j] = metric_val\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 2 * len(probe_keys)))\n",
    "    sns.heatmap(data_matrix, annot=True, fmt=\".3f\", cmap=cmap, ax=ax,\n",
    "                xticklabels=layers, yticklabels=[pt.upper() for pt in probe_keys],\n",
    "                linewidths=.5)\n",
    "\n",
    "    ax.set_title(title, fontsize=VisualConfig.TITLE_SIZE)\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('Probe Type')\n",
    "    \n",
    "    output_manager.save_figure(fig, f'performance_heatmap_{model_name}_{task}')\n",
    "    plt.show()\n",
    "\n",
    "def create_task_layer_group_plot(experiment: ExperimentResult, output_manager: OutputManager):\n",
    "    \"\"\"\n",
    "    Analyzes and plots performance across early, middle, and late layer groups for any task.\n",
    "    \"\"\"\n",
    "    task = experiment.task\n",
    "    model_name = experiment.model\n",
    "    results_data = experiment.results.get('results', {})\n",
    "\n",
    "    if 'viewpoint' in task:\n",
    "        metric_key, y_label = 'mae', 'Mean Absolute Error (MAE)'\n",
    "        probe_keys = ['linear', 'mlp']\n",
    "    elif 'voxel' in task:\n",
    "        metric_key, y_label = 'voxel_iou', 'Mean Intersection over Union (IoU)'\n",
    "        probe_keys = ['voxel']\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    layer_results = []\n",
    "    for lk, ld in results_data.items():\n",
    "        if not lk.startswith('layer_'): continue\n",
    "        layer = int(lk.split('_')[1])\n",
    "        for pt, pd in ld.items():\n",
    "            if pt in probe_keys:\n",
    "                metric_val = pd.get('test_metrics', {}).get(metric_key)\n",
    "                if metric_val is not None:\n",
    "                    layer_results.append({'layer': layer, 'probe': pt, 'metric': metric_val})\n",
    "    \n",
    "    if not layer_results: return\n",
    "\n",
    "    layers = sorted(set(r['layer'] for r in layer_results))\n",
    "    n = len(layers)\n",
    "    if n < 3: return \n",
    "    early, middle, late = layers[:n//3], layers[n//3 : 2*n//3], layers[2*n//3:]\n",
    "    groups = {'Early': early, 'Middle': middle, 'Late': late}\n",
    "\n",
    "    group_stats = {g: {pt: [] for pt in probe_keys} for g in groups}\n",
    "    for r in layer_results:\n",
    "        for group_name, group_layers in groups.items():\n",
    "            if r['layer'] in group_layers:\n",
    "                group_stats[group_name][r['probe']].append(r['metric'])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    x_pos = np.arange(len(probe_keys))\n",
    "    width = 0.25\n",
    "    group_means = {g: [np.mean(stats[pt]) if stats[pt] else 0 for pt in probe_keys] for g, stats in group_stats.items()}\n",
    "\n",
    "    ax.bar(x_pos - width, group_means['Early'], width, label='Early Layers', color='#4E79A7', alpha=0.8)\n",
    "    ax.bar(x_pos, group_means['Middle'], width, label='Middle Layers', color='#F28E2B', alpha=0.8)\n",
    "    ax.bar(x_pos + width, group_means['Late'], width, label='Late Layers', color='#59A14F', alpha=0.8)\n",
    "\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(f'Layer Group Performance - {model_name.upper()}', fontsize=VisualConfig.TITLE_SIZE)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels([pt.upper() for pt in probe_keys])\n",
    "    ax.legend()\n",
    "    ax.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "    output_manager.save_figure(fig, f'layer_group_trends_{model_name}_{task}')\n",
    "    plt.show()\n",
    "\n",
    "for exp in all_experiments.values():\n",
    "    create_task_performance_heatmap(exp, output_manager)\n",
    "    create_task_layer_group_plot(exp, output_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f14115c",
   "metadata": {},
   "source": [
    "## 6. Analyzing Intrinsic Latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9862b",
   "metadata": {},
   "source": [
    "### 6.1 Cross-Model Latent Space Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76099ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_features = {}\n",
    "def create_latent_space_analysis(experiments: List[ExperimentResult], loader: ExperimentLoader, output_manager: OutputManager):\n",
    "    \"\"\"\n",
    "    Creates a side-by-side comparison of latent spaces via PCA / t-SNE plots for each model\n",
    "    \"\"\"\n",
    "    for exp in experiments:\n",
    "        best_layer, _ = exp.get_best_layer()\n",
    "        if best_layer is None: continue\n",
    "        \n",
    "        feature_files = loader.get_feature_files(exp)\n",
    "        if best_layer in feature_files and 'test' in feature_files[best_layer]:\n",
    "            with open(feature_files[best_layer]['test'], 'rb') as f: data = pickle.load(f)\n",
    "            if 'voxel' in exp.task and 'view_data' in data:\n",
    "                feature_source = data['view_data']\n",
    "            elif 'features' in data:\n",
    "                feature_source = data['features']\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            if hasattr(feature_source, 'ndim') and feature_source.ndim == 3:\n",
    "                n_samples, n_patches, n_dims = feature_source.shape\n",
    "                feature_source = feature_source.reshape(n_samples, n_patches * n_dims)\n",
    "\n",
    "            if not isinstance(feature_source, torch.Tensor):\n",
    "                feature_source = torch.from_numpy(feature_source)\n",
    "                \n",
    "            n_samples = min(2000, len(feature_source))\n",
    "            indices = np.random.choice(len(feature_source), n_samples, replace=False)\n",
    "            \n",
    "            model_features[exp.model] = {\n",
    "                'features': feature_source[indices],\n",
    "                'targets': data['targets'][indices] if 'targets' in data else None,\n",
    "                'metadata': {k: [v[i] for i in indices] for k, v in data.get('metadata', {}).items()},\n",
    "                'layer': best_layer\n",
    "            }\n",
    "\n",
    "    if not model_features:\n",
    "        print(\"No valid features found for latent space analysis.\")\n",
    "        return\n",
    "\n",
    "    n_models = len(model_features)\n",
    "    fig, axes = plt.subplots(3, 2 * n_models, figsize=(9 * n_models, 9), squeeze=False)\n",
    "    \n",
    "    for idx, (model_name, data) in enumerate(model_features.items()):\n",
    "        print(f\"--- Analyzing latent space for: {model_name.upper()} ---\")\n",
    "        features, targets, metadata = data['features'], data['targets'], data['metadata']\n",
    "        \n",
    "        print(\"  > Running PCA...\")\n",
    "        pca = PCA(n_components=2)\n",
    "        features_pca = pca.fit_transform(features)\n",
    "        \n",
    "        print(\"  > Running t-SNE...\")\n",
    "        tsne = TSNE(n_components=2, perplexity=30, n_iter=350, random_state=42, init='pca', learning_rate='auto')\n",
    "        features_tsne = tsne.fit_transform(features)\n",
    "        \n",
    "        print(\"  > Running UMAP...\")\n",
    "        reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "        features_umap = reducer.fit_transform(features)\n",
    "\n",
    "        col_start = idx * 2\n",
    "        ax_pca_cat = axes[0, col_start]\n",
    "        ax_pca_azi = axes[0, col_start + 1]\n",
    "        ax_tsne_cat = axes[1, col_start]\n",
    "        ax_tsne_azi = axes[1, col_start + 1]\n",
    "        ax_umap_cat = axes[2, col_start]\n",
    "        ax_umap_azi= axes[2, col_start + 1]\n",
    "\n",
    "        \n",
    "        azimuth = (targets[:, 0] + 1) * 180 if targets is not None else None\n",
    "        categories = metadata.get('categories')\n",
    "        cat_indices = None\n",
    "        if categories:\n",
    "            unique_cats = sorted(list(set(categories)))\n",
    "            cat_to_idx = {cat: i for i, cat in enumerate(unique_cats)}\n",
    "            cat_indices = [cat_to_idx[cat] for cat in categories]\n",
    "\n",
    "        # PCA plots\n",
    "        if cat_indices:\n",
    "            ax_pca_cat.scatter(features_pca[:, 0], features_pca[:, 1], c=cat_indices, cmap='tab20', alpha=0.8, s=10)\n",
    "        ax_pca_cat.set_title(f'{model_name.upper()} L{data[\"layer\"]}: PCA by Category')\n",
    "        ax_pca_cat.set_xlabel('PC 1'); ax_pca_cat.set_ylabel('PC 2')\n",
    "        \n",
    "        if azimuth is not None:\n",
    "            scatter_pca = ax_pca_azi.scatter(features_pca[:, 0], features_pca[:, 1], c=azimuth, cmap='hsv', alpha=0.7, s=10)\n",
    "            fig.colorbar(scatter_pca, ax=ax_pca_azi, label='Azimuth')\n",
    "        ax_pca_azi.set_title(f'PCA by Viewpoint')\n",
    "        ax_pca_azi.set_xlabel('PC 1'); ax_pca_azi.set_ylabel('PC 2')\n",
    "\n",
    "        # t-SNE plots\n",
    "        if cat_indices:\n",
    "            ax_tsne_cat.scatter(features_tsne[:, 0], features_tsne[:, 1], c=cat_indices, cmap='tab20', alpha=0.8, s=10)\n",
    "        ax_tsne_cat.set_title(f't-SNE by Category')\n",
    "        ax_tsne_cat.set_xlabel('t-SNE 1'); ax_tsne_cat.set_ylabel('t-SNE 2')\n",
    "\n",
    "        if azimuth is not None:\n",
    "            scatter_tsne = ax_tsne_azi.scatter(features_tsne[:, 0], features_tsne[:, 1], c=azimuth, cmap='hsv', alpha=0.7, s=10)\n",
    "            fig.colorbar(scatter_tsne, ax=ax_tsne_azi, label='Azimuth')\n",
    "        ax_tsne_azi.set_title(f't-SNE by Viewpoint')\n",
    "        ax_tsne_azi.set_xlabel('t-SNE 1'); ax_tsne_azi.set_ylabel('t-SNE 2')\n",
    "        \n",
    "        # u-map plot\n",
    "        if cat_indices:\n",
    "            ax_umap_cat.scatter(features_umap[:, 0], features_umap[:, 1], c=cat_indices, cmap='tab20', alpha=0.8, s=10)\n",
    "        ax_umap_cat.set_title(f'uMAP by Category')\n",
    "        ax_umap_cat.set_xlabel('uMAP 1'); ax_umap_cat.set_ylabel('uMAP 2')            \n",
    "        \n",
    "        if azimuth is not None:\n",
    "            scatter_umap = ax_umap_azi.scatter(features_umap[:, 0], features_umap[:, 1], c=azimuth, cmap='hsv', alpha=0.7, s=10)\n",
    "            fig.colorbar(scatter_umap, ax=ax_umap_azi, label='Azimuth')\n",
    "        ax_umap_azi.set_title(f'uMAP by Viewpoint')\n",
    "        ax_umap_azi.set_xlabel('uMAP 1'); ax_tsne_azi.set_ylabel('uMAP 2')\n",
    "        \n",
    "        for ax in [ax_pca_cat, ax_pca_azi, ax_tsne_cat, ax_tsne_azi, ax_umap_cat, ax_umap_azi]:\n",
    "             ax.grid(True, alpha=0.4)\n",
    "\n",
    "    fig.suptitle('Comprehensive Latent Space Analysis', fontsize=VisualConfig.TITLE_SIZE + 4, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    output_manager.save_figure(fig, 'latent_space_comprehensive_comparison')\n",
    "    plt.show()\n",
    "\n",
    "create_latent_space_analysis(list(all_experiments.values()), loader, output_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfa9306",
   "metadata": {},
   "source": [
    "#### 6.1.1 Silhouette Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a93e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_silhouette_scores(model_features: Dict) -> Dict:\n",
    "    \"\"\"Calculates the Silhouette Score for object categories in each model's latent space\"\"\"\n",
    "    scores = {}\n",
    "    for model_name, data in model_features.items():\n",
    "        features = data['features']\n",
    "        metadata = data['metadata']\n",
    "        \n",
    "        if 'categories' in metadata and len(set(metadata['categories'])) > 1:\n",
    "            unique_cats = sorted(list(set(metadata['categories'])))\n",
    "            cat_to_idx = {cat: i for i, cat in enumerate(unique_cats)}\n",
    "            labels = [cat_to_idx[cat] for cat in metadata['categories']]\n",
    "            \n",
    "            score = silhouette_score(features, labels)\n",
    "            scores[model_name] = score\n",
    "            print(f\"  > {model_name}: {score:.4f}\")\n",
    "        else:\n",
    "            print(f\"  > Skipping {model_name}: Not enough data\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def plot_scores(scores: Dict, title: str, output_manager: OutputManager):\n",
    "    \"\"\"Creates a bar plot for the scores.\"\"\"\n",
    "    if not scores: return\n",
    "    \n",
    "    models = list(scores.keys())\n",
    "    values = list(scores.values())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    colors = [VisualConfig.MODEL_COLORS.get(m, '#333') for m in models]\n",
    "    \n",
    "    bars = ax.bar(models, values, color=colors)\n",
    "    ax.bar_label(bars, fmt='%.3f')\n",
    "    \n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title(title, fontsize=VisualConfig.TITLE_SIZE)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    ax.grid(True, axis='y', alpha=0.5)\n",
    "    \n",
    "    output_manager.save_figure(fig, title.lower().replace(' ', '_'))\n",
    "    plt.show()\n",
    "\n",
    "silhouette_results = calculate_silhouette_scores(model_features)\n",
    "plot_scores(silhouette_results, \"Category Cluster Separation (Silhouette Score)\", output_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26904415",
   "metadata": {},
   "source": [
    "#### 6.1.2: CKA Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5223ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inter_layer_cka_matrix(layer_features: List[torch.Tensor], device: torch.device) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the CKA matrix between a list of layer feature tensors.\n",
    "    \"\"\"\n",
    "    n_layers = len(layer_features)\n",
    "    cka_matrix = np.zeros((n_layers, n_layers))\n",
    "    \n",
    "    print(\"Computing Inter-Layer CKA Matrix... (this may take a moment)\")\n",
    "    \n",
    "    centered_kernels = []\n",
    "    for features_tensor in layer_features:\n",
    "        kernel = torch.from_numpy(rbf_kernel(features_tensor.cpu().numpy())).to(device)\n",
    "        centered_kernels.append(center_gram_torch(kernel))\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        for j in range(i, n_layers):\n",
    "            if i == j:\n",
    "                cka_matrix[i, j] = 1.0\n",
    "                continue\n",
    "            \n",
    "            K_i_centered = centered_kernels[i]\n",
    "            K_j_centered = centered_kernels[j]\n",
    "            \n",
    "            numerator = torch.sum(K_i_centered * K_j_centered)\n",
    "            denominator = torch.sqrt(torch.sum(K_i_centered**2) * torch.sum(K_j_centered**2))\n",
    "            cka = (numerator / denominator).cpu().item()\n",
    "            \n",
    "            cka_matrix[i, j] = cka\n",
    "            cka_matrix[j, i] = cka\n",
    "            \n",
    "    return cka_matrix\n",
    "\n",
    "def create_inter_layer_cka_similarity(experiment: ExperimentResult, loader: ExperimentLoader, output_manager: OutputManager):\n",
    "    \"\"\"\n",
    "    Loads all layer features for a model and visualizes their CKA similarity matrix.\n",
    "    \"\"\"\n",
    "    feature_files = loader.get_feature_files(experiment)\n",
    "    if not feature_files:\n",
    "        print(f\"No feature files found for {experiment.name} to run CKA analysis.\")\n",
    "        return\n",
    "\n",
    "    layer_indices = sorted(feature_files.keys())\n",
    "    all_layer_features = []\n",
    "    \n",
    "    print(f\"Loading features for {experiment.model} ({experiment.task})...\")\n",
    "    for layer in layer_indices:\n",
    "        if 'test' in feature_files[layer]:\n",
    "            with open(feature_files[layer]['test'], 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            \n",
    "            if 'voxel' in experiment.task and 'view_data' in data:\n",
    "                feature_source = data['view_data']\n",
    "            elif 'features' in data:\n",
    "                feature_source = data['features']\n",
    "            else:\n",
    "                print(f\"Warning: Could not find a valid feature key in layer {layer} for {experiment.name}. Skipping layer.\")\n",
    "                continue\n",
    "            \n",
    "            if hasattr(feature_source, 'ndim') and feature_source.ndim == 3:\n",
    "                print(f\"  > Reshaping 3D features in layer {layer} from {feature_source.shape} to 2D.\")\n",
    "                n_samples, n_patches, n_dims = feature_source.shape\n",
    "                feature_source = feature_source.reshape(n_samples, n_patches * n_dims)\n",
    "\n",
    "            if not isinstance(feature_source, torch.Tensor):\n",
    "                feature_source = torch.from_numpy(feature_source)\n",
    "\n",
    "            n_samples_to_use = min(1000, len(feature_source))\n",
    "            indices = np.random.choice(len(feature_source), n_samples_to_use, replace=False)\n",
    "            all_layer_features.append(feature_source[indices])\n",
    "\n",
    "    if len(all_layer_features) < 2:\n",
    "        print(f\"Not enough layer features to compare for {experiment.name}.\")\n",
    "        return\n",
    "\n",
    "    cka_matrix = get_inter_layer_cka_matrix(all_layer_features, device)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(cka_matrix, annot=True, fmt=\".2f\", cmap=\"magma\", ax=ax,\n",
    "                xticklabels=layer_indices, yticklabels=layer_indices)\n",
    "    \n",
    "    ax.set_title(f'Inter-Layer CKA Similarity - {experiment.model.upper()}', fontsize=VisualConfig.TITLE_SIZE)\n",
    "    ax.set_xlabel('Layer'); ax.set_ylabel('Layer')\n",
    "    \n",
    "    output_manager.save_figure(fig, f'cka_inter_layer_{experiment.model}_{experiment.task}')\n",
    "    output_manager.save_data({'model': experiment.model, 'task': experiment.task, 'layers': layer_indices, 'cka_matrix': cka_matrix.tolist()}, f'cka_inter_layer_{experiment.model}_{experiment.task}_data')\n",
    "    plt.show()\n",
    "\n",
    "for exp in all_experiments.values():\n",
    "    create_inter_layer_cka_similarity(exp, loader, output_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e46037e",
   "metadata": {},
   "source": [
    "### 6.2 Attention Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3847eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import traceback\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def visualize_attention_for_model(\n",
    "    image_tensor: torch.Tensor,\n",
    "    model_name: str,\n",
    "    layer: int,\n",
    "    method: str,\n",
    "    device: torch.device,\n",
    "    axes: np.ndarray\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize the attntion scores using some fanccyyyyy stuff!\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model, _ = load_model_and_preprocessor(model_name, ckpt_path=None, device=device, cache_dir=None)\n",
    "        model.to(device).eval()\n",
    "        if image_tensor.dim() == 3:\n",
    "            image_tensor = image_tensor.unsqueeze(0)\n",
    "        inputs_on_device = image_tensor.to(device)\n",
    "\n",
    "        all_attentions = []\n",
    "        with torch.no_grad():\n",
    "            if 'timm' not in model_name and model_name != 'ijepa':\n",
    "                outputs = model(inputs_on_device, output_attentions=True)\n",
    "                all_attentions = outputs.attentions\n",
    "            else:\n",
    "                attention_maps_by_layer = {}\n",
    "                hook_handles = []\n",
    "                original_fused_attn_states = {}\n",
    "                def get_hook(layer_idx):\n",
    "                    def hook_fn(module, input_args, output_tensor):\n",
    "                        attention_maps_by_layer[layer_idx] = input_args[0].detach()\n",
    "                    return hook_fn\n",
    "\n",
    "                if not hasattr(model, 'blocks') or not model.blocks: return\n",
    "                for i, block in enumerate(model.blocks):\n",
    "                    attn_module = block.attn\n",
    "                    if hasattr(attn_module, 'fused_attn'):\n",
    "                        original_fused_attn_states[i] = attn_module.fused_attn\n",
    "                        attn_module.fused_attn = False\n",
    "                    handle = attn_module.attn_drop.register_forward_hook(get_hook(i))\n",
    "                    hook_handles.append(handle)\n",
    "                \n",
    "                _ = model(inputs_on_device)\n",
    "                for handle in hook_handles: handle.remove()\n",
    "                for i, state in original_fused_attn_states.items(): model.blocks[i].attn.fused_attn = state\n",
    "                \n",
    "                num_heads = model.blocks[0].attn.num_heads\n",
    "                batch_size = inputs_on_device.shape[0]\n",
    "                for i in sorted(attention_maps_by_layer.keys()):\n",
    "                    attn_probs = attention_maps_by_layer[i]\n",
    "                    if attn_probs.ndim == 3:\n",
    "                        num_tokens = attn_probs.shape[-1]\n",
    "                        attn_probs = attn_probs.view(batch_size, num_heads, num_tokens, num_tokens)\n",
    "                    all_attentions.append(attn_probs)\n",
    "\n",
    "        if not all_attentions: return\n",
    "\n",
    "        attn_from_layer = all_attentions[layer]\n",
    "        attn_to_vis = attn_from_layer.mean(dim=1)\n",
    "        \n",
    "        if method == 'cls':\n",
    "            attention_vector = attn_to_vis[0, 0, 1:]\n",
    "        elif method == 'aggregate':\n",
    "            patch_attn = attn_to_vis[0, 1:, 1:]\n",
    "            attention_vector = patch_attn.sum(dim=0)\n",
    "        \n",
    "        num_patch_tokens = attention_vector.shape[-1]\n",
    "        patch_grid_size = int(np.sqrt(num_patch_tokens))\n",
    "        \n",
    "        if patch_grid_size * patch_grid_size == num_patch_tokens:\n",
    "            attention_map_reshaped = attention_vector.reshape(patch_grid_size, patch_grid_size)\n",
    "        elif hasattr(model, 'patch_embed') and hasattr(model.patch_embed, 'grid_size'):\n",
    "            grid_h, grid_w = model.patch_embed.grid_size\n",
    "            expected_tokens = grid_h * grid_w\n",
    "            if num_patch_tokens != expected_tokens:\n",
    "                padding_needed = expected_tokens - num_patch_tokens\n",
    "                if padding_needed > 0:\n",
    "                    padded_attention = F.pad(attention_vector, (0, padding_needed), value=0)\n",
    "                    attention_map_reshaped = padded_attention.reshape(grid_h, grid_w)\n",
    "                else:\n",
    "                    attention_map_reshaped = attention_vector[:expected_tokens].reshape(grid_h, grid_w)\n",
    "            else:\n",
    "                attention_map_reshaped = attention_vector.reshape(grid_h, grid_w)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Cannot determine grid size for {model_name} with {num_patch_tokens} tokens.\")\n",
    "\n",
    "        attention_map_reshaped = attention_map_reshaped.cpu().numpy()\n",
    "        attention_map_reshaped = (attention_map_reshaped - attention_map_reshaped.min()) / (attention_map_reshaped.max() - attention_map_reshaped.min() + 1e-6)\n",
    "        \n",
    "        attn_resized = F.interpolate(\n",
    "            torch.tensor(attention_map_reshaped).unsqueeze(0).unsqueeze(0),\n",
    "            size=(image_tensor.shape[-2], image_tensor.shape[-1]),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        ).squeeze().cpu().numpy()\n",
    "\n",
    "        img_for_show = image_tensor[0].permute(1, 2, 0).cpu().numpy()\n",
    "        min_val, max_val = img_for_show.min(), img_for_show.max()\n",
    "        img_for_show = (img_for_show - min_val) / (max_val - min_val + 1e-6)\n",
    "        \n",
    "        ax1, ax2, ax3 = axes\n",
    "        title = f\"{model_name.upper()}\\nL{layer} ({method})\"\n",
    "        \n",
    "        ax1.imshow(img_for_show); ax1.set_title(title); ax1.axis('off')\n",
    "        im = ax2.imshow(attn_resized, cmap='hot'); ax2.set_title('Attention Map'); ax2.axis('off')\n",
    "        ax3.imshow(img_for_show); ax3.imshow(attn_resized, cmap='hot', alpha=0.5); ax3.set_title('Overlay'); ax3.axis('off')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while visualizing attention for {model_name}: {e}\")\n",
    "\n",
    "def run_comparative_attention_analysis(experiments: List[ExperimentResult], output_manager: OutputManager):\n",
    "    \"\"\"Manages the process of creating a side-by-side attention visualization.\"\"\"\n",
    "    image = None\n",
    "    try:\n",
    "        dataset_config = OmegaConf.load('../configs/datasets/shapenet_3dr2n2.yaml')\n",
    "        dataset_config.categories = ['chair']\n",
    "        print(\"Loading data.... be patient!\")\n",
    "        _, _, test_loader = create_3dr2n2_dataloaders(dataset_config, batch_size=1, num_workers=0)\n",
    "        image = next(iter(test_loader))['image'][0]\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal: Could not load sample image. Aborting. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    model_configs = {}\n",
    "    for exp in experiments:\n",
    "        if exp.task == 'viewpoint_estimation':\n",
    "            best_layer, _ = exp.get_best_layer()\n",
    "            if best_layer is not None:\n",
    "                method = 'aggregate' if 'ijepa' in exp.model.lower() else 'cls'\n",
    "                model_configs[exp.model] = {'layer': best_layer, 'method': method}\n",
    "    \n",
    "    n_models = len(model_configs)\n",
    "    if n_models == 0: return\n",
    "        \n",
    "    fig, axes = plt.subplots(3, n_models, figsize=(7 * n_models, 10), squeeze=False)\n",
    "    \n",
    "    for idx, (model_name, config) in enumerate(model_configs.items()):\n",
    "        print(f\"Processing: {model_name.upper()} (Layer: {config['layer']}, Method: {config['method']})\")\n",
    "        visualize_attention_for_model(\n",
    "            image_tensor=image, model_name=model_name,\n",
    "            layer=config['layer'], method=config['method'],\n",
    "            device=device, axes=axes[:, idx]\n",
    "        )\n",
    "\n",
    "    fig.suptitle('Comparative Attention Analysis (Best Viewpoint Layers)', fontsize=VisualConfig.TITLE_SIZE + 2, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    output_manager.save_figure(fig, 'comparative_attention_analysis_final')\n",
    "    plt.show()\n",
    "\n",
    "viewpoint_exps = categorized_experiments.get('viewpoint_estimation', [])\n",
    "if viewpoint_exps:\n",
    "    run_comparative_attention_analysis(viewpoint_exps, output_manager)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LatentInvestigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
