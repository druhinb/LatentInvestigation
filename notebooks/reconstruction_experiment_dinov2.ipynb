{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a05e8b5",
   "metadata": {},
   "source": [
    "# Reconstruction Experiment on SSL Models (DINOv2)\n",
    "\n",
    "This notebook adapts the probing experiment framework to focus on 3D voxel reconstruction using `VoxelProbe`. It will:\n",
    "- Load a pre-trained DINOv2 model.\n",
    "- Extract features from specified layers.\n",
    "- Train `VoxelProbe` instances on these features to predict 3D voxel occupancy.\n",
    "- Evaluate performance using IoU, Precision, Recall, and F1-score.\n",
    "- Analyze and visualize results to determine which layers are best for reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb671f5",
   "metadata": {},
   "source": [
    "### Imports, Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95849e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables before imports\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "# Imports\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import wandb\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Model and dataset imports\n",
    "from src.models.reconstruction_feature_extractor import ReconstructionFeatureExtractor, load_image_feature_extractor \n",
    "from src.datasets.shapenet_voxel_meshes import create_3dr2n2_reconstruction_dataloaders \n",
    "\n",
    "# Probing imports using new modular structure\n",
    "from src.probing import (\n",
    "    create_probe, \n",
    "    ProbeTrainer,\n",
    "    ReconstructionPipeline,\n",
    "    ReconstructionDataset,\n",
    "    compute_voxel_metrics,\n",
    "    MetricsTracker,\n",
    ")\n",
    "from src.analysis.layer_analysis import LayerWiseAnalyzer\n",
    "\n",
    "# Fix duplicate logging issue in Jupyter notebooks\n",
    "# Clear any existing handlers to prevent duplicates\n",
    "root_logger = logging.getLogger()\n",
    "for handler in root_logger.handlers[:]:\n",
    "    root_logger.removeHandler(handler)\n",
    "\n",
    "# Configure logging fresh\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    force=True  # This forces reconfiguration in newer Python versions\n",
    ")\n",
    "\n",
    "# Get the notebook logger\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f3059",
   "metadata": {},
   "source": [
    "### Reconstruction Experiment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e724005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructionExperiment:\n",
    "    \"\"\"Orchestrates 3D reconstruction experiments using VoxelProbes\"\"\"\n",
    "\n",
    "    def __init__(self, config: DictConfig):\n",
    "        self.config = config\n",
    "        device_to_use = config.get(\"device\", config.get(\"device\"))\n",
    "        if device_to_use:\n",
    "            self.device = device_to_use\n",
    "        else:\n",
    "            self.device = (\n",
    "                \"cuda\"\n",
    "                if torch.cuda.is_available()\n",
    "                else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "            )\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "\n",
    "        if config.get(\"wandb\", {}).get(\"enabled\", False):\n",
    "            wandb.init(\n",
    "                project=config.wandb.project,\n",
    "                entity=config.wandb.get(\"entity\"),\n",
    "                name=config.experiment.name + \"_reconstruction\", \n",
    "                config=OmegaConf.to_container(config, resolve=True),\n",
    "            )\n",
    "\n",
    "        self.results_dir = Path(config.get(\"results_dir\", \"./results\")) / (config.experiment.name)\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.cache_dir = Path(config.get(\"cache_dir\", \"./cache\")) / (config.experiment.name)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.probe_save_dir = self.cache_dir / \"probes\"\n",
    "        self.probe_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.analyzer = LayerWiseAnalyzer(self.results_dir)\n",
    "\n",
    "    def load_source_dataset(self) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        \"\"\"Load the source ShapeNet dataset with images and voxels.\"\"\"\n",
    "        subset_percentage = self.config.datasets.get(\"subset_percentage\", None)\n",
    "        return create_3dr2n2_reconstruction_dataloaders( \n",
    "            self.config.datasets, \n",
    "            batch_size=self.config.datasets.get(\"source_batch_size\", 16), \n",
    "            num_workers=self.config.get(\"num_workers\", 4),\n",
    "            subset_percentage=subset_percentage\n",
    "        )\n",
    "\n",
    "    def load_reconstruction_feature_extractor(self) -> ReconstructionFeatureExtractor:\n",
    "        \"\"\"Load and setup ReconstructionFeatureExtractor\"\"\"\n",
    "        model_config = self.config.models\n",
    "        model_config_dict = OmegaConf.to_container(model_config, resolve=True)\n",
    "        model_config_dict[\"device\"] = self.device\n",
    "        model_config_dict[\"cache_dir\"] = str(self.cache_dir / \"models\")\n",
    "        \n",
    "        feature_extractor = load_image_feature_extractor(model_config_dict)\n",
    "        logger.info(f\"Loaded {model_config_dict.get('model_name')} reconstruction feature extractor\")\n",
    "        return feature_extractor\n",
    "\n",
    "    def prepare_reconstruction_input_datasets_for_layer(\n",
    "        self,\n",
    "        reconstruction_pipeline: ReconstructionPipeline,\n",
    "        train_source_loader: DataLoader,\n",
    "        val_source_loader: DataLoader,\n",
    "        test_source_loader: DataLoader,\n",
    "        layer: int,\n",
    "        image_feature_type: str,\n",
    "    ) -> Tuple[ReconstructionDataset, ReconstructionDataset, ReconstructionDataset]:\n",
    "        \"\"\"\n",
    "        Uses ReconstructionPipeline to extract image features for a specific layer\n",
    "        and combine them with processed camera parameters, then creates datasets.\n",
    "        \"\"\"\n",
    "        experiment_id = f\"{self.config.models.model_name}_{self.config.experiment.name}_layer_{layer}\"\n",
    "        \n",
    "        # Create datasets using the new API\n",
    "        train_input_dataset = reconstruction_pipeline.create_dataset(\n",
    "            dataloader=train_source_loader,\n",
    "            layers=[layer],\n",
    "            feature_type=image_feature_type,\n",
    "            cache_key=f\"{experiment_id}_train\",\n",
    "            force_recompute=self.config.get(\"force_recompute_processed_data\", False)\n",
    "        )\n",
    "        \n",
    "        val_input_dataset = reconstruction_pipeline.create_dataset(\n",
    "            dataloader=val_source_loader,\n",
    "            layers=[layer],\n",
    "            feature_type=image_feature_type,\n",
    "            cache_key=f\"{experiment_id}_val\",\n",
    "            force_recompute=self.config.get(\"force_recompute_processed_data\", False)\n",
    "        )\n",
    "        \n",
    "        test_input_dataset = reconstruction_pipeline.create_dataset(\n",
    "            dataloader=test_source_loader,\n",
    "            layers=[layer],\n",
    "            feature_type=image_feature_type,\n",
    "            cache_key=f\"{experiment_id}_test\",\n",
    "            force_recompute=self.config.get(\"force_recompute_processed_data\", False)\n",
    "        )\n",
    "        \n",
    "        return train_input_dataset, val_input_dataset, test_input_dataset\n",
    "\n",
    "    def run_voxel_probe_experiment(\n",
    "        self,\n",
    "        train_processed_loader: DataLoader, \n",
    "        val_processed_loader: DataLoader,\n",
    "        test_processed_loader: DataLoader,\n",
    "        feature_dim: int,\n",
    "        layer: int,\n",
    "    ) -> Dict:\n",
    "        \"\"\"Run a single VoxelProbe experiment\"\"\"\n",
    "        probe_type = \"voxel\"\n",
    "        logger.info(\n",
    "            f\"Running VoxelProbe on layer {layer} (input_feature_dim: {feature_dim})\"\n",
    "        )\n",
    "\n",
    "        probe_config = self.config.probing\n",
    "        \n",
    "        probe_config[\"input_dim\"] = feature_dim\n",
    "        probe_config[\"task_type\"] = \"voxel_reconstruction\" \n",
    "        \n",
    "        self.device = probe_config.get(\"device\", self.device)\n",
    "\n",
    "        probe = create_probe(probe_config)\n",
    "        \n",
    "        metrics_tracker = MetricsTracker()\n",
    "        trainer = ProbeTrainer(\n",
    "            probe, device=self.device, MetricsTracker=metrics_tracker\n",
    "        )\n",
    "\n",
    "        training_config = self.config.probing.get(\"training\", {}) \n",
    "        optimizer_specific_config = probe_config.get(\"optimizer\", training_config.get(\"optimizer\", {}))\n",
    "        scheduler_specific_config = probe_config.get(\"scheduler\", training_config.get(\"scheduler\", {}))\n",
    "\n",
    "        optimizer = self.create_optimizer(probe, optimizer_specific_config)\n",
    "        scheduler = self.create_scheduler(optimizer, scheduler_specific_config)\n",
    "\n",
    "        epochs = training_config.get(\"epochs\", 50) # Potentially more epochs for reconstruction\n",
    "        early_stopping_patience = training_config.get(\"early_stopping_patience\", 10)\n",
    "        wandb_enabled = self.config.get(\"wandb\", {}).get(\"enabled\", False)\n",
    "\n",
    "        best_model_state_dict, best_val_loss = trainer.train(\n",
    "            epochs,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            early_stopping_patience,\n",
    "            train_processed_loader,\n",
    "            val_processed_loader,\n",
    "            probe_type=probe_type, # Pass \"voxel\"\n",
    "            layer=layer,\n",
    "            wandb_enabled=wandb_enabled,\n",
    "        )\n",
    "        \n",
    "        probe_filename = f\"{self.config.models.model_name}_{probe_type}_layer_{layer}_probe.pth\"\n",
    "        probe_save_path = self.probe_save_dir / probe_filename\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': best_model_state_dict,\n",
    "            'probe_config': probe_config, \n",
    "            'layer': layer,\n",
    "            'probe_type': probe_type,\n",
    "            'experiment_name': self.config.experiment.name,\n",
    "            'model_name': self.config.models.model_name,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'input_feature_dim': feature_dim \n",
    "        }, probe_save_path)\n",
    "        logger.info(f\"Saved VoxelProbe for layer {layer} to {probe_save_path}\")\n",
    "\n",
    "        probe.load_state_dict(best_model_state_dict)\n",
    "       \n",
    "        test_metrics = trainer.evaluate(\n",
    "            test_loader=test_processed_loader, \n",
    "            wandb_enabled=wandb_enabled, \n",
    "            probe_type=probe_type, \n",
    "            layer=layer\n",
    "        )\n",
    "\n",
    "        detailed_metrics = self._compute_detailed_metrics(probe, test_processed_loader) \n",
    "\n",
    "        total_epochs_trained = len(metrics_tracker.get_history(\"train\"))\n",
    "\n",
    "        results = {\n",
    "            \"train_history\": metrics_tracker.get_history(\"train\"),\n",
    "            \"val_history\": metrics_tracker.get_history(\"val\"),\n",
    "            \"test_metrics\": test_metrics, \n",
    "            \"detailed_metrics\": detailed_metrics, \n",
    "            \"best_epoch\": metrics_tracker.best_epoch,\n",
    "            \"total_epochs\": total_epochs_trained,\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def create_optimizer(\n",
    "        self, model: nn.Module, optimizer_config: Dict\n",
    "    ) -> torch.optim.Optimizer:\n",
    "        from hydra.utils import instantiate\n",
    "        opt_config_copy = OmegaConf.create(optimizer_config) \n",
    "        if \"_target_\" not in opt_config_copy: \n",
    "             raise ValueError(\"Optimizer config must have a _target_ field\")\n",
    "\n",
    "        return instantiate(opt_config_copy, params=model.parameters())\n",
    "\n",
    "\n",
    "    def create_scheduler(\n",
    "        self, optimizer: torch.optim.Optimizer, scheduler_config: Dict\n",
    "    ):\n",
    "        if not scheduler_config or not scheduler_config.get(\"_target_\"): \n",
    "            return None\n",
    "        from hydra.utils import instantiate\n",
    "        sched_config_copy = OmegaConf.create(scheduler_config)\n",
    "        return instantiate(sched_config_copy, optimizer=optimizer)\n",
    "\n",
    "    def _compute_detailed_metrics( \n",
    "        self, probe: nn.Module, test_loader: DataLoader \n",
    "    ) -> Dict:\n",
    "        \n",
    "        probe.to(self.device)\n",
    "        probe.eval()\n",
    "        all_predictions = [] \n",
    "        all_targets = []   \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Computing detailed metrics\"):\n",
    "                features = batch[\"processed_views\"].to(self.device) \n",
    "                targets = batch[\"target_voxels\"].to(self.device) \n",
    "\n",
    "                if probe.task_type == \"voxel_reconstruction\":\n",
    "                    features = features.view(features.size(0), -1)\n",
    "\n",
    "                outputs = probe(features) # Voxel logits: [B, 1, D, H, W]\n",
    "\n",
    "                all_predictions.append(outputs.cpu()) \n",
    "                all_targets.append(targets.cpu())\n",
    "\n",
    "        predictions_cat = torch.cat(all_predictions, dim=0)\n",
    "        targets_cat = torch.cat(all_targets, dim=0)\n",
    "        \n",
    "        metrics = {}\n",
    "        voxel_eval_metrics = compute_voxel_metrics(predictions_cat, targets_cat)\n",
    "        metrics.update(voxel_eval_metrics)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def save_results(self, results: Dict) -> str:\n",
    "        import json\n",
    "        results_file = self.results_dir / \"reconstruction_results.json\"\n",
    "        serializable_results = self.make_json_serializable(results)\n",
    "        combined_results = {\n",
    "            \"config\": OmegaConf.to_container(self.config, resolve=True),\n",
    "            \"results\": serializable_results,\n",
    "        }\n",
    "        with open(results_file, \"w\") as f:\n",
    "            json.dump(combined_results, f, indent=2)\n",
    "        logger.info(f\"Reconstruction results saved to {results_file}\")\n",
    "        return str(results_file)\n",
    "\n",
    "    def make_json_serializable(self, obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: self.make_json_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self.make_json_serializable(v) for v in obj]\n",
    "        elif isinstance(obj, (torch.Tensor, np.ndarray)):\n",
    "            return obj.tolist() if hasattr(obj, \"tolist\") else float(obj)\n",
    "        elif isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, Path):\n",
    "            return str(obj)\n",
    "        else:\n",
    "            return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd6ad5",
   "metadata": {},
   "source": [
    "### Hydra Configuration Loading / Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca7937ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 23:29:45,859 - __main__ - INFO - Initializing Hydra with config_path: '../configs' relative to /Users/druhi/Documents/+Programming/GitHub/LatentInvestigation/notebooks\n",
      "2025-06-04 23:29:46,019 - __main__ - INFO - Composing configuration with config_name: 'experiment_config_new'\n",
      "2025-06-04 23:29:46,019 - __main__ - INFO - Composing configuration with config_name: 'experiment_config_new'\n",
      "2025-06-04 23:29:46,080 - __main__ - INFO - Hydra configuration loaded successfully for reconstruction experiment.\n",
      "2025-06-04 23:29:46,081 - __main__ - INFO - Experiment name: phase1_dinov2_voxel_reconstruction\n",
      "2025-06-04 23:29:46,081 - __main__ - INFO - Task type: voxel_reconstruction\n",
      "2025-06-04 23:29:46,081 - __main__ - INFO - Probe types: ['voxel']\n",
      "2025-06-04 23:29:46,080 - __main__ - INFO - Hydra configuration loaded successfully for reconstruction experiment.\n",
      "2025-06-04 23:29:46,081 - __main__ - INFO - Experiment name: phase1_dinov2_voxel_reconstruction\n",
      "2025-06-04 23:29:46,081 - __main__ - INFO - Task type: voxel_reconstruction\n",
      "2025-06-04 23:29:46,081 - __main__ - INFO - Probe types: ['voxel']\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "import os \n",
    "from pathlib import Path\n",
    "\n",
    "CONFIG_PATH = \"../configs\"\n",
    "CONFIG_NAME = \"experiment_config_new\"  # Use the new restructured config with defaults\n",
    "\n",
    "cfg: Optional[DictConfig] = None\n",
    "\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "    logger.info(\"Clearing existing Hydra global state.\")\n",
    "    GlobalHydra.instance().clear()\n",
    "\n",
    "try:\n",
    "    project_root = Path(os.getcwd()).parent \n",
    "    data_dir_abs = project_root / \"data\" \n",
    "    os.environ[\"DATA_DIR\"] = str(data_dir_abs)\n",
    "    \n",
    "    logger.info(f\"Initializing Hydra with config_path: '{CONFIG_PATH}' relative to {os.getcwd()}\")\n",
    "    initialize(version_base=None, config_path=CONFIG_PATH, job_name=\"reconstruction_experiment\")\n",
    "    \n",
    "    logger.info(f\"Composing configuration with config_name: '{CONFIG_NAME}'\")\n",
    "    cfg = compose(config_name=CONFIG_NAME) \n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing Hydra or loading configuration: {e}\", exc_info=True)\n",
    "    cfg = None \n",
    "\n",
    "if cfg:\n",
    "    logger.info(\"Hydra configuration loaded successfully for reconstruction experiment.\")\n",
    "    logger.info(f\"Experiment name: {cfg.experiment.name}\")\n",
    "    logger.info(f\"Task type: {cfg.probing.task_type}\")\n",
    "    logger.info(f\"Probe types: {cfg.probing.probe_types}\")\n",
    "\n",
    "else:\n",
    "    logger.error(\"Failed to load Hydra configuration. Please check paths and config files.\")\n",
    "\n",
    "# Quick check for critical reconstruction settings\n",
    "if cfg and cfg.probing.task_type != \"voxel_reconstruction\":\n",
    "    logger.warning(f\"Configured task_type is '{cfg.probing.task_type}', expected 'voxel_reconstruction' for this notebook.\")\n",
    "if cfg and \"voxel\" not in cfg.probing.probe_types:\n",
    "    logger.warning(f\"Configured probe_types are '{cfg.probing.probe_types}', 'voxel' probe might not run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b7188d",
   "metadata": {},
   "source": [
    "## Running the Reconstruction Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "003360ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 23:29:46,091 - __main__ - INFO - Starting reconstruction experiment execution\n",
      "2025-06-04 23:29:46,091 - __main__ - INFO - Using device: mps\n",
      "2025-06-04 23:29:46,091 - __main__ - INFO - Using device: mps\n"
     ]
    }
   ],
   "source": [
    "reconstruction_results = None\n",
    "if cfg:\n",
    "    logger.info(\"Starting reconstruction experiment execution\")\n",
    "    experiment = ReconstructionExperiment(cfg)\n",
    "else:\n",
    "    logger.error(\"Configuration not loaded. Cannot start experiment.\")\n",
    "    experiment = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363288db",
   "metadata": {},
   "source": [
    "### Load Source Data and Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dbee6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 23:29:46,106 - src.models.model_loader - INFO - Loading HuggingFace model 'facebook/dinov2-base'\n",
      "2025-06-04 23:29:46,838 - src.models.reconstruction_feature_extractor - INFO - Loaded and froze dinov2 on mps.\n",
      "2025-06-04 23:29:46,839 - __main__ - INFO - Loaded dinov2 reconstruction feature extractor\n",
      "2025-06-04 23:29:46,838 - src.models.reconstruction_feature_extractor - INFO - Loaded and froze dinov2 on mps.\n",
      "2025-06-04 23:29:46,839 - __main__ - INFO - Loaded dinov2 reconstruction feature extractor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train DataLoader with 153 samples, batch size 32.\n",
      "Created val DataLoader with 32 samples, batch size 32.\n",
      "Created test DataLoader with 32 samples, batch size 32.\n"
     ]
    }
   ],
   "source": [
    "if experiment:\n",
    "    image_feature_extractor = experiment.load_reconstruction_feature_extractor()\n",
    "    \n",
    "    extraction_config = cfg.models.get(\"feature_extraction\", {})\n",
    "    layers_to_probe = extraction_config.get(\"layers\", [0, 2, 5, 8, 11]) \n",
    "    image_feature_type = extraction_config.get(\"feature_type\", \"cls_token\")\n",
    "\n",
    "    train_source_loader, val_source_loader, test_source_loader = experiment.load_source_dataset()\n",
    "    \n",
    "    # Use the new ReconstructionPipeline API\n",
    "    reconstruction_pipeline = ReconstructionPipeline(\n",
    "        image_pipeline=image_feature_extractor,\n",
    "        device=experiment.device,\n",
    "        cache_dir=str(experiment.cache_dir / \"processed_reconstruction_data\")\n",
    "    )\n",
    "else:\n",
    "    logger.error(\"Experiment not initialized. Skipping feature extractor and dataset loading.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75494f",
   "metadata": {},
   "source": [
    "### Process Data and Train VoxelProbes for Each Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d19051df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 23:29:46,863 - __main__ - INFO - Will train VoxelProbes for layers: [2]\n",
      "Processing Layers:   0%|          | 0/1 [00:00<?, ?it/s]2025-06-04 23:29:46,879 - __main__ - INFO - Processing layer 2 for reconstruction...\n",
      "2025-06-04 23:29:46,881 - src.probing.base_pipeline - INFO - Loading cached data from cache/phase1_dinov2_voxel_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_2_train.pkl\n",
      "2025-06-04 23:29:46,888 - src.probing.base_pipeline - INFO - Loading cached data from cache/phase1_dinov2_voxel_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_2_val.pkl\n",
      "Processing Layers:   0%|          | 0/1 [00:00<?, ?it/s]2025-06-04 23:29:46,879 - __main__ - INFO - Processing layer 2 for reconstruction...\n",
      "2025-06-04 23:29:46,881 - src.probing.base_pipeline - INFO - Loading cached data from cache/phase1_dinov2_voxel_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_2_train.pkl\n",
      "2025-06-04 23:29:46,888 - src.probing.base_pipeline - INFO - Loading cached data from cache/phase1_dinov2_voxel_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_2_val.pkl\n",
      "2025-06-04 23:29:46,892 - src.probing.base_pipeline - INFO - Loading cached data from cache/phase1_dinov2_voxel_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_2_test.pkl\n",
      "2025-06-04 23:29:46,895 - __main__ - INFO - Layer 2: Train Input Dataset size: 128\n",
      "2025-06-04 23:29:46,895 - __main__ - INFO - Layer 2: Val Input Dataset size: 32\n",
      "2025-06-04 23:29:46,895 - __main__ - INFO - Layer 2: Test Input Dataset size: 32\n",
      "2025-06-04 23:29:46,896 - __main__ - INFO - Input feature dimension for VoxelProbe at layer 2: 18624 (Shape of sample: torch.Size([18624]))\n",
      "2025-06-04 23:29:46,896 - __main__ - INFO - Running VoxelProbe on layer 2...\n",
      "2025-06-04 23:29:46,897 - __main__ - INFO - Running VoxelProbe on layer 2 (input_feature_dim: 18624)\n",
      "2025-06-04 23:29:46,892 - src.probing.base_pipeline - INFO - Loading cached data from cache/phase1_dinov2_voxel_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_2_test.pkl\n",
      "2025-06-04 23:29:46,895 - __main__ - INFO - Layer 2: Train Input Dataset size: 128\n",
      "2025-06-04 23:29:46,895 - __main__ - INFO - Layer 2: Val Input Dataset size: 32\n",
      "2025-06-04 23:29:46,895 - __main__ - INFO - Layer 2: Test Input Dataset size: 32\n",
      "2025-06-04 23:29:46,896 - __main__ - INFO - Input feature dimension for VoxelProbe at layer 2: 18624 (Shape of sample: torch.Size([18624]))\n",
      "2025-06-04 23:29:46,896 - __main__ - INFO - Running VoxelProbe on layer 2...\n",
      "2025-06-04 23:29:46,897 - __main__ - INFO - Running VoxelProbe on layer 2 (input_feature_dim: 18624)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 [Train Lyr:2 Prb:voxel]:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A2025-06-04 23:30:03,510 - src.probing.probes - INFO - Epoch 1/1 Lyr:2 Prb:voxel - Train Loss: 0.5852, Val Loss: 0.6904\n",
      "2025-06-04 23:30:03,510 - src.probing.probes - INFO - Epoch 1/1 Lyr:2 Prb:voxel - Train Loss: 0.5852, Val Loss: 0.6904\n",
      "2025-06-04 23:30:04,664 - __main__ - INFO - Saved VoxelProbe for layer 2 to cache/phase1_dinov2_voxel_reconstruction/probes/dinov2_voxel_layer_2_probe.pth\n",
      "2025-06-04 23:30:04,664 - __main__ - INFO - Saved VoxelProbe for layer 2 to cache/phase1_dinov2_voxel_reconstruction/probes/dinov2_voxel_layer_2_probe.pth\n",
      "2025-06-04 23:30:08,302 - src.probing.probes - INFO - Test Metrics (Lyr:2 Prb:voxel): {'loss': 0.6903156638145447, 'voxel_iou': 0.06938595324754715, 'voxel_precision': 0.08503496646881104, 'voxel_recall': 0.35935384035110474, 'voxel_f1': 0.12702718377113342}\n",
      "2025-06-04 23:30:08,302 - src.probing.probes - INFO - Test Metrics (Lyr:2 Prb:voxel): {'loss': 0.6903156638145447, 'voxel_iou': 0.06938595324754715, 'voxel_precision': 0.08503496646881104, 'voxel_recall': 0.35935384035110474, 'voxel_f1': 0.12702718377113342}\n",
      "Computing detailed metrics: 100%|██████████| 1/1 [00:03<00:00,  3.65s/it]\n",
      "Processing Layers: 100%|██████████| 1/1 [00:25<00:00, 25.15s/it]\n",
      "\n",
      "Processing Layers: 100%|██████████| 1/1 [00:25<00:00, 25.15s/it]\n"
     ]
    }
   ],
   "source": [
    "if experiment:\n",
    "    reconstruction_results = {}\n",
    "\n",
    "    if \"voxel\" not in cfg.probing.probe_types:\n",
    "        logger.error(f\"'voxel' not in configured probe_types: {cfg.probing.probe_types}. VoxelProbes will not be trained.\")\n",
    "    else:\n",
    "        logger.info(f\"Will train VoxelProbes for layers: {layers_to_probe}\")\n",
    "\n",
    "        for layer_idx in tqdm(layers_to_probe, desc=\"Processing Layers\"):\n",
    "            logger.info(f\"Processing layer {layer_idx} for reconstruction...\")\n",
    "\n",
    "            # Use the updated method with new API\n",
    "            train_input_ds, val_input_ds, test_input_ds = experiment.prepare_reconstruction_input_datasets_for_layer(\n",
    "                reconstruction_pipeline=reconstruction_pipeline,\n",
    "                train_source_loader=train_source_loader,\n",
    "                val_source_loader=val_source_loader,\n",
    "                test_source_loader=test_source_loader,\n",
    "                layer=layer_idx,\n",
    "                image_feature_type=image_feature_type,\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Layer {layer_idx}: Train Input Dataset size: {len(train_input_ds)}\")\n",
    "            logger.info(f\"Layer {layer_idx}: Val Input Dataset size: {len(val_input_ds)}\")\n",
    "            logger.info(f\"Layer {layer_idx}: Test Input Dataset size: {len(test_input_ds)}\")\n",
    "\n",
    "            # Create DataLoaders from the datasets\n",
    "            processed_batch_size = cfg.training.get(\"batch_size\", 32)\n",
    "            \n",
    "            train_processed_loader = DataLoader(\n",
    "                train_input_ds,\n",
    "                batch_size=processed_batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=cfg.get(\"num_workers\", 4),\n",
    "                pin_memory=True\n",
    "            )\n",
    "            \n",
    "            val_processed_loader = DataLoader(\n",
    "                val_input_ds,\n",
    "                batch_size=processed_batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=cfg.get(\"num_workers\", 4),\n",
    "                pin_memory=True\n",
    "            )\n",
    "            \n",
    "            test_processed_loader = DataLoader(\n",
    "                test_input_ds,\n",
    "                batch_size=processed_batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=cfg.get(\"num_workers\", 4),\n",
    "                pin_memory=True\n",
    "            )\n",
    "\n",
    "            # Get feature dimension from sample\n",
    "            sample_processed_data = train_input_ds[0][\"processed_views\"]\n",
    "            input_feature_dim_for_probe = sample_processed_data.numel() \n",
    "            logger.info(f\"Input feature dimension for VoxelProbe at layer {layer_idx}: {input_feature_dim_for_probe} (Shape of sample: {sample_processed_data.shape})\")\n",
    "\n",
    "            logger.info(f\"Running VoxelProbe on layer {layer_idx}...\")\n",
    "            probe_run_results = experiment.run_voxel_probe_experiment(\n",
    "                train_processed_loader=train_processed_loader,\n",
    "                val_processed_loader=val_processed_loader,\n",
    "                test_processed_loader=test_processed_loader,\n",
    "                feature_dim=input_feature_dim_for_probe,\n",
    "                layer=layer_idx,\n",
    "            )\n",
    "            reconstruction_results[f\"layer_{layer_idx}\"] = {\"voxel\": probe_run_results}\n",
    "\n",
    "else:\n",
    "    logger.error(\"Experiment not initialized. Skipping layer processing and probe training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203594fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 23:30:12,045 - __main__ - INFO - Saving reconstruction experiment results...\n",
      "2025-06-04 23:30:12,050 - __main__ - INFO - Reconstruction results saved to results/phase1_dinov2_voxel_reconstruction/reconstruction_results.json\n",
      "2025-06-04 23:30:12,051 - __main__ - INFO - Results saved to: results/phase1_dinov2_voxel_reconstruction/reconstruction_results.json\n",
      "2025-06-04 23:30:12,050 - __main__ - INFO - Reconstruction results saved to results/phase1_dinov2_voxel_reconstruction/reconstruction_results.json\n",
      "2025-06-04 23:30:12,051 - __main__ - INFO - Results saved to: results/phase1_dinov2_voxel_reconstruction/reconstruction_results.json\n"
     ]
    }
   ],
   "source": [
    "if experiment and reconstruction_results:\n",
    "    logger.info(\"Saving reconstruction experiment results...\")\n",
    "    result_file_path = experiment.save_results(reconstruction_results)\n",
    "    logger.info(f\"Results saved to: {result_file_path}\")\n",
    "else:\n",
    "    logger.warning(\"No results to save or experiment not run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0bcdeb",
   "metadata": {},
   "source": [
    "### Analyze and Visualize Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb12f33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 23:30:12,073 - __main__ - INFO - Analyzing reconstruction results...\n",
      "2025-06-04 23:30:13,845 - src.analysis.layer_analysis - INFO - Analysis report saved to results/phase1_dinov2_voxel_reconstruction/layer_analysis_report.json\n",
      "2025-06-04 23:30:13,845 - src.analysis.layer_analysis - INFO - Analysis report saved to results/phase1_dinov2_voxel_reconstruction/layer_analysis_report.json\n"
     ]
    }
   ],
   "source": [
    "from src.analysis.layer_analysis import analyze_experiment_results\n",
    "\n",
    "if experiment and reconstruction_results and 'result_file_path' in locals():\n",
    "    logger.info(\"Analyzing reconstruction results...\")\n",
    "\n",
    "    analyze_experiment_results(\n",
    "        results_file=result_file_path,\n",
    "        output_dir=Path(result_file_path).parent\n",
    "    )\n",
    "else:\n",
    "    logger.warning(\"No results to analyze or result file path not available.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LatentInvestigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
