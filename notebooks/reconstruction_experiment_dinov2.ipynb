{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a05e8b5",
   "metadata": {},
   "source": [
    "# Reconstruction Experiment on SSL Models (DINOv2)\n",
    "\n",
    "This notebook adapts the probing experiment framework to focus on 3D voxel reconstruction using `VoxelProbe`. It will:\n",
    "- Load a pre-trained DINOv2 model.\n",
    "- Extract features from specified layers.\n",
    "- Train `VoxelProbe` instances on these features to predict 3D voxel occupancy.\n",
    "- Evaluate performance using IoU, Precision, Recall, and F1-score.\n",
    "- Analyze and visualize results to determine which layers are best for reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb671f5",
   "metadata": {},
   "source": [
    "### Imports, Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95849e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables before imports\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "# Imports\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import wandb\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "from src.models.reconstruction_feature_extractor import ReconstructionFeatureExtractor, load_image_feature_extractor\n",
    "from src.datasets.shapenet_voxel_meshes import create_3dr2n2_reconstruction_dataloaders\n",
    "from src.probing.reconstruction_pipeline import ReconstructionPipeline, ReconstructionDataset\n",
    "from src.probing.probes import create_probe, ProbeTrainer\n",
    "from src.probing.metrics import (\n",
    "    compute_voxel_metrics,\n",
    "    MetricsTracker,\n",
    ")\n",
    "from src.analysis.layer_analysis import LayerWiseAnalyzer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f3059",
   "metadata": {},
   "source": [
    "### Reconstruction Experiment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e724005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructionExperiment:\n",
    "    \"\"\"Orchestrates 3D reconstruction experiments using VoxelProbes\"\"\"\n",
    "\n",
    "    def __init__(self, config: DictConfig):\n",
    "        self.config = config\n",
    "        device_to_use = config.get(\"device\", config.get(\"device\"))\n",
    "        if device_to_use:\n",
    "            self.device = device_to_use\n",
    "        else:\n",
    "            self.device = (\n",
    "                \"cuda\"\n",
    "                if torch.cuda.is_available()\n",
    "                else \"cpu\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "            )\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "\n",
    "        if config.get(\"wandb\", {}).get(\"enabled\", False):\n",
    "            wandb.init(\n",
    "                project=config.wandb.project,\n",
    "                entity=config.wandb.get(\"entity\"),\n",
    "                name=config.experiment.name + \"_reconstruction\", \n",
    "                config=OmegaConf.to_container(config, resolve=True),\n",
    "            )\n",
    "\n",
    "        self.results_dir = Path(config.get(\"results_dir\", \"./results\")) / (config.experiment.name)\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.cache_dir = Path(config.get(\"cache_dir\", \"./cache\")) / (config.experiment.name)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.probe_save_dir = self.cache_dir / \"probes\"\n",
    "        self.probe_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.analyzer = LayerWiseAnalyzer(self.results_dir)\n",
    "\n",
    "    def load_source_dataset(self) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        \"\"\"Load the source ShapeNet dataset with images and voxels.\"\"\"\n",
    "        subset_percentage = self.config.datasets.get(\"subset_percentage\", None)\n",
    "        return create_3dr2n2_reconstruction_dataloaders( \n",
    "            self.config.datasets, \n",
    "            batch_size=self.config.datasets.get(\"source_batch_size\", 16), \n",
    "            num_workers=self.config.get(\"num_workers\", 4),\n",
    "            subset_percentage=subset_percentage\n",
    "        )\n",
    "\n",
    "    def load_reconstruction_feature_extractor(self) -> ReconstructionFeatureExtractor:\n",
    "        \"\"\"Load and setup ReconstructionFeatureExtractor\"\"\"\n",
    "        model_config = self.config.models\n",
    "        model_config_dict = OmegaConf.to_container(model_config, resolve=True)\n",
    "        model_config_dict[\"device\"] = self.device\n",
    "        model_config_dict[\"cache_dir\"] = str(self.cache_dir / \"models\")\n",
    "        \n",
    "        feature_extractor = load_image_feature_extractor(model_config_dict)\n",
    "        logger.info(f\"Loaded {model_config_dict.get('model_name')} reconstruction feature extractor\")\n",
    "        return feature_extractor\n",
    "\n",
    "    def prepare_reconstruction_input_datasets_for_layer(\n",
    "        self,\n",
    "        reconstruction_pipeline: ReconstructionPipeline,\n",
    "        train_source_loader: DataLoader,\n",
    "        val_source_loader: DataLoader,\n",
    "        test_source_loader: DataLoader,\n",
    "        layer: int,\n",
    "        image_feature_type: str,\n",
    "    ) -> Tuple[ReconstructionInputDataset, ReconstructionInputDataset, ReconstructionInputDataset]:\n",
    "        \"\"\"\n",
    "        Uses ReconstructionPipeline to extract image features for a specific layer\n",
    "        and combine them with processed camera parameters, then creates datasets.\n",
    "        \"\"\"\n",
    "        experiment_id = f\"{self.config.models.model_name}_{self.config.experiment.name}_layer_{layer}\"\n",
    "        \n",
    "        train_input_dataset, val_input_dataset, test_input_dataset = reconstruction_pipeline.create_reconstruction_datasets(\n",
    "            train_source_loader=train_source_loader,\n",
    "            val_source_loader=val_source_loader,\n",
    "            test_source_loader=test_source_loader,\n",
    "            image_feat_layers=[layer],\n",
    "            img_feat_type=image_feature_type,\n",
    "            experiment_name=experiment_id, \n",
    "            force_recompute_data=self.config.get(\"force_recompute_processed_data\", False)\n",
    "        )\n",
    "        return train_input_dataset, val_input_dataset, test_input_dataset\n",
    "\n",
    "    def run_voxel_probe_experiment(\n",
    "        self,\n",
    "        train_processed_loader: DataLoader, \n",
    "        val_processed_loader: DataLoader,\n",
    "        test_processed_loader: DataLoader,\n",
    "        feature_dim: int,\n",
    "        layer: int,\n",
    "    ) -> Dict:\n",
    "        \"\"\"Run a single VoxelProbe experiment\"\"\"\n",
    "        probe_type = \"voxel\"\n",
    "        logger.info(\n",
    "            f\"Running VoxelProbe on layer {layer} (input_feature_dim: {feature_dim})\"\n",
    "        )\n",
    "\n",
    "        probe_config_orig = self.config.probing.get(probe_type, {})\n",
    "        probe_config = OmegaConf.to_container(probe_config_orig, resolve=True)\n",
    "        \n",
    "        probe_config[\"input_dim\"] = feature_dim\n",
    "        probe_config[\"task_type\"] = \"voxel_reconstruction\" \n",
    "\n",
    "        probe = create_probe(probe_config)\n",
    "        \n",
    "        metrics_tracker = MetricsTracker()\n",
    "        trainer = ProbeTrainer(\n",
    "            probe, device=self.device, MetricsTracker=metrics_tracker\n",
    "        )\n",
    "\n",
    "        training_config = self.config.probing.get(\"training\", {}) \n",
    "        optimizer_specific_config = probe_config.get(\"optimizer\", training_config.get(\"optimizer\", {}))\n",
    "        scheduler_specific_config = probe_config.get(\"scheduler\", training_config.get(\"scheduler\", {}))\n",
    "\n",
    "        optimizer = self.create_optimizer(probe, optimizer_specific_config)\n",
    "        scheduler = self.create_scheduler(optimizer, scheduler_specific_config)\n",
    "\n",
    "        epochs = training_config.get(\"epochs\", 50) # Potentially more epochs for reconstruction\n",
    "        early_stopping_patience = training_config.get(\"early_stopping_patience\", 10)\n",
    "        wandb_enabled = self.config.get(\"wandb\", {}).get(\"enabled\", False)\n",
    "\n",
    "        best_model_state_dict, best_val_loss = trainer.train(\n",
    "            epochs,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            early_stopping_patience,\n",
    "            train_processed_loader,\n",
    "            val_processed_loader,\n",
    "            probe_type=probe_type, # Pass \"voxel\"\n",
    "            layer=layer,\n",
    "            wandb_enabled=wandb_enabled,\n",
    "        )\n",
    "        \n",
    "        probe_filename = f\"{self.config.models.model_name}_{probe_type}_layer_{layer}_probe.pth\"\n",
    "        probe_save_path = self.probe_save_dir / probe_filename\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': best_model_state_dict,\n",
    "            'probe_config': probe_config, \n",
    "            'layer': layer,\n",
    "            'probe_type': probe_type,\n",
    "            'experiment_name': self.config.experiment.name,\n",
    "            'model_name': self.config.models.model_name,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'input_feature_dim': feature_dim \n",
    "        }, probe_save_path)\n",
    "        logger.info(f\"Saved VoxelProbe for layer {layer} to {probe_save_path}\")\n",
    "\n",
    "        probe.load_state_dict(best_model_state_dict)\n",
    "       \n",
    "        test_metrics = trainer.evaluate(\n",
    "            test_loader=test_processed_loader, \n",
    "            wandb_enabled=wandb_enabled, \n",
    "            probe_type=probe_type, \n",
    "            layer=layer\n",
    "        )\n",
    "\n",
    "  \n",
    "        detailed_metrics = self._compute_detailed_metrics(probe, test_processed_loader) \n",
    "\n",
    "        total_epochs_trained = len(metrics_tracker.get_history(\"train\"))\n",
    "\n",
    "        results = {\n",
    "            \"train_history\": metrics_tracker.get_history(\"train\"),\n",
    "            \"val_history\": metrics_tracker.get_history(\"val\"),\n",
    "            \"test_metrics\": test_metrics, \n",
    "            \"detailed_metrics\": detailed_metrics, \n",
    "            \"best_epoch\": metrics_tracker.best_epoch,\n",
    "            \"total_epochs\": total_epochs_trained,\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def create_optimizer(\n",
    "        self, model: nn.Module, optimizer_config: Dict\n",
    "    ) -> torch.optim.Optimizer:\n",
    "        from hydra.utils import instantiate\n",
    "        opt_config_copy = OmegaConf.create(optimizer_config) \n",
    "        if \"_target_\" not in opt_config_copy: \n",
    "             raise ValueError(\"Optimizer config must have a _target_ field\")\n",
    "\n",
    "        return instantiate(opt_config_copy, params=model.parameters())\n",
    "\n",
    "\n",
    "    def create_scheduler(\n",
    "        self, optimizer: torch.optim.Optimizer, scheduler_config: Dict\n",
    "    ):\n",
    "        if not scheduler_config or not scheduler_config.get(\"_target_\"): \n",
    "            return None\n",
    "        from hydra.utils import instantiate\n",
    "        sched_config_copy = OmegaConf.create(scheduler_config)\n",
    "        return instantiate(sched_config_copy, optimizer=optimizer)\n",
    "\n",
    "    def _compute_detailed_metrics( \n",
    "        self, probe: nn.Module, test_loader: DataLoader \n",
    "    ) -> Dict:\n",
    "        probe.to(self.device)\n",
    "        probe.eval()\n",
    "        all_predictions = [] \n",
    "        all_targets = []   \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Computing detailed metrics\"):\n",
    "                features = batch[\"processed_views\"].to(self.device) \n",
    "                targets = batch[\"target_voxels\"].to(self.device) \n",
    "\n",
    "                if probe.task_type == \"voxel_reconstruction\":\n",
    "                    features = features.view(features.size(0), -1)\n",
    "\n",
    "                outputs = probe(features) # Voxel logits: [B, 1, D, H, W]\n",
    "\n",
    "                all_predictions.append(outputs.cpu()) \n",
    "                all_targets.append(targets.cpu())\n",
    "\n",
    "        predictions_cat = torch.cat(all_predictions, dim=0)\n",
    "        targets_cat = torch.cat(all_targets, dim=0)\n",
    "        \n",
    "        metrics = {}\n",
    "        voxel_eval_metrics = compute_voxel_metrics(predictions_cat, targets_cat)\n",
    "        metrics.update(voxel_eval_metrics)\n",
    "        \n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def save_results(self, results: Dict) -> str:\n",
    "        import json\n",
    "        results_file = self.results_dir / \"reconstruction_results.json\"\n",
    "        serializable_results = self.make_json_serializable(results)\n",
    "        combined_results = {\n",
    "            \"config\": OmegaConf.to_container(self.config, resolve=True),\n",
    "            \"results\": serializable_results,\n",
    "        }\n",
    "        with open(results_file, \"w\") as f:\n",
    "            json.dump(combined_results, f, indent=2)\n",
    "        logger.info(f\"Reconstruction results saved to {results_file}\")\n",
    "        return str(results_file)\n",
    "\n",
    "    def make_json_serializable(self, obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: self.make_json_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self.make_json_serializable(v) for v in obj]\n",
    "        elif isinstance(obj, (torch.Tensor, np.ndarray)):\n",
    "            return obj.tolist() if hasattr(obj, \"tolist\") else float(obj)\n",
    "        elif isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, Path):\n",
    "            return str(obj)\n",
    "        else:\n",
    "            return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd6ad5",
   "metadata": {},
   "source": [
    "### Hydra Configuration Loading / Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca7937ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:38:08,363 - __main__ - INFO - Initializing Hydra with config_path: '../configs' relative to /Users/druhi/Documents/+Programming/GitHub/LatentInvestigation/notebooks\n",
      "2025-06-03 22:38:08,544 - __main__ - INFO - Composing configuration with config_name: 'experiment_config'\n",
      "2025-06-03 22:38:08,544 - __main__ - INFO - Composing configuration with config_name: 'experiment_config'\n",
      "2025-06-03 22:38:08,603 - __main__ - INFO - Hydra configuration loaded successfully for reconstruction experiment.\n",
      "2025-06-03 22:38:08,604 - __main__ - INFO - Experiment name: phase1_dinov2_voxel_reconstruction\n",
      "2025-06-03 22:38:08,604 - __main__ - INFO - Task type: voxel_reconstruction\n",
      "2025-06-03 22:38:08,604 - __main__ - INFO - Probe types: ['voxel']\n",
      "2025-06-03 22:38:08,603 - __main__ - INFO - Hydra configuration loaded successfully for reconstruction experiment.\n",
      "2025-06-03 22:38:08,604 - __main__ - INFO - Experiment name: phase1_dinov2_voxel_reconstruction\n",
      "2025-06-03 22:38:08,604 - __main__ - INFO - Task type: voxel_reconstruction\n",
      "2025-06-03 22:38:08,604 - __main__ - INFO - Probe types: ['voxel']\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "import os \n",
    "from pathlib import Path\n",
    "\n",
    "CONFIG_PATH = \"../configs\"\n",
    "CONFIG_NAME = \"experiment_config\" # This should now point to a config suitable for reconstruction\n",
    "\n",
    "cfg: Optional[DictConfig] = None\n",
    "\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "    logger.info(\"Clearing existing Hydra global state.\")\n",
    "    GlobalHydra.instance().clear()\n",
    "\n",
    "try:\n",
    "    project_root = Path(os.getcwd()).parent \n",
    "    data_dir_abs = project_root / \"data\" \n",
    "    os.environ[\"DATA_DIR\"] = str(data_dir_abs)\n",
    "    \n",
    "    logger.info(f\"Initializing Hydra with config_path: '{CONFIG_PATH}' relative to {os.getcwd()}\")\n",
    "    initialize(version_base=None, config_path=CONFIG_PATH, job_name=\"reconstruction_experiment\")\n",
    "    \n",
    "    logger.info(f\"Composing configuration with config_name: '{CONFIG_NAME}'\")\n",
    "    cfg = compose(config_name=CONFIG_NAME) \n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing Hydra or loading configuration: {e}\", exc_info=True)\n",
    "    cfg = None \n",
    "\n",
    "if cfg:\n",
    "    logger.info(\"Hydra configuration loaded successfully for reconstruction experiment.\")\n",
    "    logger.info(f\"Experiment name: {cfg.experiment.name}\")\n",
    "    logger.info(f\"Task type: {cfg.probing.task_type}\")\n",
    "    logger.info(f\"Probe types: {cfg.probing.probe_types}\")\n",
    "\n",
    "else:\n",
    "    logger.error(\"Failed to load Hydra configuration. Please check paths and config files.\")\n",
    "\n",
    "# Quick check for critical reconstruction settings\n",
    "if cfg and cfg.probing.task_type != \"voxel_reconstruction\":\n",
    "    logger.warning(f\"Configured task_type is '{cfg.probing.task_type}', expected 'voxel_reconstruction' for this notebook.\")\n",
    "if cfg and \"voxel\" not in cfg.probing.probe_types:\n",
    "    logger.warning(f\"Configured probe_types are '{cfg.probing.probe_types}', 'voxel' probe might not run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b7188d",
   "metadata": {},
   "source": [
    "## Running the Reconstruction Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "003360ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:38:08,609 - __main__ - INFO - Starting reconstruction experiment execution\n",
      "2025-06-03 22:38:08,609 - __main__ - INFO - Using device: cpu\n",
      "2025-06-03 22:38:08,609 - __main__ - INFO - Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "reconstruction_results = None\n",
    "if cfg:\n",
    "    logger.info(\"Starting reconstruction experiment execution\")\n",
    "    experiment = ReconstructionExperiment(cfg)\n",
    "else:\n",
    "    logger.error(\"Configuration not loaded. Cannot start experiment.\")\n",
    "    experiment = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363288db",
   "metadata": {},
   "source": [
    "### Load Source Data and Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dbee6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:38:08,614 - src.models.reconstruction_feature_extractor - INFO - Loading model and processor for: dinov2\n",
      "2025-06-03 22:38:08,615 - src.models.reconstruction_feature_extractor - INFO - Attempting to load HuggingFace model: facebook/dinov2-base\n",
      "2025-06-03 22:38:08,615 - src.models.reconstruction_feature_extractor - INFO - Attempting to load HuggingFace model: facebook/dinov2-base\n",
      "2025-06-03 22:38:09,219 - src.models.reconstruction_feature_extractor - INFO - Loaded HuggingFace model: facebook/dinov2-base\n",
      "2025-06-03 22:38:09,222 - src.models.reconstruction_feature_extractor - INFO - Loaded and froze dinov2 on cpu.\n",
      "2025-06-03 22:38:09,222 - __main__ - INFO - Loaded dinov2 reconstruction feature extractor\n",
      "2025-06-03 22:38:09,219 - src.models.reconstruction_feature_extractor - INFO - Loaded HuggingFace model: facebook/dinov2-base\n",
      "2025-06-03 22:38:09,222 - src.models.reconstruction_feature_extractor - INFO - Loaded and froze dinov2 on cpu.\n",
      "2025-06-03 22:38:09,222 - __main__ - INFO - Loaded dinov2 reconstruction feature extractor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30648/30648 [00:03<00:00, 8206.69it/s] \n",
      "100%|██████████| 30648/30648 [00:03<00:00, 8206.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 30647 valid samples for split train.\n",
      "Using 0.50% of train data: 153 samples.\n",
      "Created train DataLoader with 153 samples, batch size 32.\n",
      "Preparing samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6567/6567 [00:00<00:00, 9832.03it/s] \n",
      "100%|██████████| 6567/6567 [00:00<00:00, 9832.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 6567 valid samples for split val.\n",
      "Using 0.50% of val data: 32 samples.\n",
      "Created val DataLoader with 32 samples, batch size 32.\n",
      "Preparing samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6569/6569 [00:00<00:00, 18063.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 6569 valid samples for split test.\n",
      "Using 0.50% of test data: 32 samples.\n",
      "Created test DataLoader with 32 samples, batch size 32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if experiment:\n",
    "    image_feature_extractor = experiment.load_reconstruction_feature_extractor()\n",
    "    \n",
    "    extraction_config = cfg.models.get(\"feature_extraction\", {})\n",
    "    layers_to_probe = extraction_config.get(\"layers\", [0, 2, 5, 8, 11]) \n",
    "    image_feature_type = extraction_config.get(\"feature_type\", \"cls_token\")\n",
    "\n",
    "    train_source_loader, val_source_loader, test_source_loader = experiment.load_source_dataset()\n",
    "    \n",
    "    reconstruction_pipeline = ReconstructionPipeline(\n",
    "        image_feature_extractor=image_feature_extractor,\n",
    "        device=experiment.device,\n",
    "        cache_dir=str(experiment.cache_dir / \"processed_reconstruction_data\")\n",
    "    )\n",
    "else:\n",
    "    logger.error(\"Experiment not initialized. Skipping feature extractor and dataset loading.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75494f",
   "metadata": {},
   "source": [
    "### Process Data and Train VoxelProbes for Each Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19051df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:38:14,030 - __main__ - INFO - Will train VoxelProbes for layers: [2, 11]\n",
      "Processing Layers:   0%|          | 0/2 [00:00<?, ?it/s]2025-06-03 22:38:14,031 - __main__ - INFO - Processing layer 2 for reconstruction...\n",
      "2025-06-03 22:38:14,038 - src.probing.reconstruction_processing_pipeline - INFO - Loaded cached reconstruction data from cache/phase1_dinov2_voxel_reconstruction_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_2_train_reconstruction_prepared_data.pkl\n",
      "2025-06-03 22:38:14,039 - src.probing.reconstruction_processing_pipeline - INFO - TRAIN ReconstructionInputDataset created with 128 samples.\n",
      "Processing Layers:   0%|          | 0/2 [00:00<?, ?it/s]2025-06-03 22:38:14,031 - __main__ - INFO - Processing layer 2 for reconstruction...\n",
      "2025-06-03 22:38:14,038 - src.probing.reconstruction_processing_pipeline - INFO - Loaded cached reconstruction data from cache/phase1_dinov2_voxel_reconstruction_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_2_train_reconstruction_prepared_data.pkl\n",
      "2025-06-03 22:38:14,039 - src.probing.reconstruction_processing_pipeline - INFO - TRAIN ReconstructionInputDataset created with 128 samples.\n",
      "2025-06-03 22:38:14,042 - src.probing.reconstruction_processing_pipeline - INFO - Loaded cached reconstruction data from cache/phase1_dinov2_voxel_reconstruction_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_2_val_reconstruction_prepared_data.pkl\n",
      "2025-06-03 22:38:14,042 - src.probing.reconstruction_processing_pipeline - INFO - VAL ReconstructionInputDataset created with 32 samples.\n",
      "2025-06-03 22:38:14,045 - src.probing.reconstruction_processing_pipeline - INFO - Loaded cached reconstruction data from cache/phase1_dinov2_voxel_reconstruction_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_2_test_reconstruction_prepared_data.pkl\n",
      "2025-06-03 22:38:14,045 - src.probing.reconstruction_processing_pipeline - INFO - TEST ReconstructionInputDataset created with 32 samples.\n",
      "2025-06-03 22:38:14,046 - __main__ - INFO - Layer 2: Train Input Dataset size: 128\n",
      "2025-06-03 22:38:14,046 - __main__ - INFO - Layer 2: Val Input Dataset size: 32\n",
      "2025-06-03 22:38:14,046 - __main__ - INFO - Layer 2: Test Input Dataset size: 32\n",
      "2025-06-03 22:38:14,047 - __main__ - INFO - Input feature dimension for VoxelProbe at layer 2: 18624 (Shape of sample: torch.Size([24, 776]))\n",
      "2025-06-03 22:38:14,047 - __main__ - INFO - Running VoxelProbe on layer 2...\n",
      "2025-06-03 22:38:14,047 - __main__ - INFO - Running VoxelProbe on layer 2 (input_feature_dim: 18624)\n",
      "2025-06-03 22:38:14,042 - src.probing.reconstruction_processing_pipeline - INFO - Loaded cached reconstruction data from cache/phase1_dinov2_voxel_reconstruction_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_2_val_reconstruction_prepared_data.pkl\n",
      "2025-06-03 22:38:14,042 - src.probing.reconstruction_processing_pipeline - INFO - VAL ReconstructionInputDataset created with 32 samples.\n",
      "2025-06-03 22:38:14,045 - src.probing.reconstruction_processing_pipeline - INFO - Loaded cached reconstruction data from cache/phase1_dinov2_voxel_reconstruction_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_2_test_reconstruction_prepared_data.pkl\n",
      "2025-06-03 22:38:14,045 - src.probing.reconstruction_processing_pipeline - INFO - TEST ReconstructionInputDataset created with 32 samples.\n",
      "2025-06-03 22:38:14,046 - __main__ - INFO - Layer 2: Train Input Dataset size: 128\n",
      "2025-06-03 22:38:14,046 - __main__ - INFO - Layer 2: Val Input Dataset size: 32\n",
      "2025-06-03 22:38:14,046 - __main__ - INFO - Layer 2: Test Input Dataset size: 32\n",
      "2025-06-03 22:38:14,047 - __main__ - INFO - Input feature dimension for VoxelProbe at layer 2: 18624 (Shape of sample: torch.Size([24, 776]))\n",
      "2025-06-03 22:38:14,047 - __main__ - INFO - Running VoxelProbe on layer 2...\n",
      "2025-06-03 22:38:14,047 - __main__ - INFO - Running VoxelProbe on layer 2 (input_feature_dim: 18624)\n",
      "2025-06-03 22:38:37,064 - src.probing.probes - INFO - Epoch 1/1 Lyr:2 Prb:voxel - Train Loss: 1.9193, Val Loss: 1.0729\n",
      "2025-06-03 22:38:37,064 - src.probing.probes - INFO - Epoch 1/1 Lyr:2 Prb:voxel - Train Loss: 1.9193, Val Loss: 1.0729\n",
      "2025-06-03 22:38:38,303 - __main__ - INFO - Saved VoxelProbe for layer 2 to cache/phase1_dinov2_voxel_reconstruction_reconstruction/probes/dinov2_voxel_layer_2_probe.pth\n",
      "2025-06-03 22:38:38,303 - __main__ - INFO - Saved VoxelProbe for layer 2 to cache/phase1_dinov2_voxel_reconstruction_reconstruction/probes/dinov2_voxel_layer_2_probe.pth\n",
      "2025-06-03 22:38:41,580 - src.probing.probes - INFO - Test Metrics (Lyr:2 Prb:voxel): {'loss': 1.0916023254394531, 'voxel_iou': 0.08292428404092789, 'voxel_precision': 0.0917893722653389, 'voxel_recall': 0.5655761361122131, 'voxel_f1': 0.1486997902393341}\n",
      "2025-06-03 22:38:41,580 - src.probing.probes - INFO - Test Metrics (Lyr:2 Prb:voxel): {'loss': 1.0916023254394531, 'voxel_iou': 0.08292428404092789, 'voxel_precision': 0.0917893722653389, 'voxel_recall': 0.5655761361122131, 'voxel_f1': 0.1486997902393341}\n",
      "Computing detailed metrics: 100%|██████████| 1/1 [00:03<00:00,  3.25s/it]\n",
      "Processing Layers:  50%|█████     | 1/2 [00:30<00:30, 30.81s/it]2025-06-03 22:38:44,843 - __main__ - INFO - Processing layer 11 for reconstruction...\n",
      "\n",
      "Processing Layers:  50%|█████     | 1/2 [00:30<00:30, 30.81s/it]2025-06-03 22:38:44,843 - __main__ - INFO - Processing layer 11 for reconstruction...\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Preparing reconstruction data: 100%|██████████| 4/4 [03:26<00:00, 51.54s/it]\n",
      "2025-06-03 22:42:11,043 - src.probing.reconstruction_processing_pipeline - INFO - Cached reconstruction data to cache/phase1_dinov2_voxel_reconstruction_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_11_train_reconstruction_prepared_data.pkl\n",
      "2025-06-03 22:42:11,053 - src.probing.reconstruction_processing_pipeline - INFO - TRAIN ReconstructionInputDataset created with 128 samples.\n",
      "\n",
      "2025-06-03 22:42:11,043 - src.probing.reconstruction_processing_pipeline - INFO - Cached reconstruction data to cache/phase1_dinov2_voxel_reconstruction_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_11_train_reconstruction_prepared_data.pkl\n",
      "2025-06-03 22:42:11,053 - src.probing.reconstruction_processing_pipeline - INFO - TRAIN ReconstructionInputDataset created with 128 samples.\n",
      "Preparing reconstruction data: 100%|██████████| 1/1 [00:54<00:00, 54.98s/it]\n",
      "2025-06-03 22:43:06,039 - src.probing.reconstruction_processing_pipeline - INFO - Cached reconstruction data to cache/phase1_dinov2_voxel_reconstruction_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_11_val_reconstruction_prepared_data.pkl\n",
      "Preparing reconstruction data: 100%|██████████| 1/1 [00:54<00:00, 54.98s/it]\n",
      "2025-06-03 22:43:06,039 - src.probing.reconstruction_processing_pipeline - INFO - Cached reconstruction data to cache/phase1_dinov2_voxel_reconstruction_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_11_val_reconstruction_prepared_data.pkl\n",
      "2025-06-03 22:43:06,047 - src.probing.reconstruction_processing_pipeline - INFO - VAL ReconstructionInputDataset created with 32 samples.\n",
      "2025-06-03 22:43:06,047 - src.probing.reconstruction_processing_pipeline - INFO - VAL ReconstructionInputDataset created with 32 samples.\n",
      "Preparing reconstruction data: 100%|██████████| 1/1 [00:56<00:00, 56.49s/it]\n",
      "2025-06-03 22:44:02,545 - src.probing.reconstruction_processing_pipeline - INFO - Cached reconstruction data to cache/phase1_dinov2_voxel_reconstruction_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_11_test_reconstruction_prepared_data.pkl\n",
      "Preparing reconstruction data: 100%|██████████| 1/1 [00:56<00:00, 56.49s/it]\n",
      "2025-06-03 22:44:02,545 - src.probing.reconstruction_processing_pipeline - INFO - Cached reconstruction data to cache/phase1_dinov2_voxel_reconstruction_reconstruction/processed_reconstruction_data/dinov2_phase1_dinov2_voxel_reconstruction_layer_11_test_reconstruction_prepared_data.pkl\n",
      "2025-06-03 22:44:02,555 - src.probing.reconstruction_processing_pipeline - INFO - TEST ReconstructionInputDataset created with 32 samples.\n",
      "2025-06-03 22:44:02,555 - __main__ - INFO - Layer 11: Train Input Dataset size: 128\n",
      "2025-06-03 22:44:02,555 - __main__ - INFO - Layer 11: Val Input Dataset size: 32\n",
      "2025-06-03 22:44:02,556 - __main__ - INFO - Layer 11: Test Input Dataset size: 32\n",
      "2025-06-03 22:44:02,558 - __main__ - INFO - Input feature dimension for VoxelProbe at layer 11: 18624 (Shape of sample: torch.Size([24, 776]))\n",
      "2025-06-03 22:44:02,558 - __main__ - INFO - Running VoxelProbe on layer 11...\n",
      "2025-06-03 22:44:02,558 - __main__ - INFO - Running VoxelProbe on layer 11 (input_feature_dim: 18624)\n",
      "2025-06-03 22:44:02,555 - src.probing.reconstruction_processing_pipeline - INFO - TEST ReconstructionInputDataset created with 32 samples.\n",
      "2025-06-03 22:44:02,555 - __main__ - INFO - Layer 11: Train Input Dataset size: 128\n",
      "2025-06-03 22:44:02,555 - __main__ - INFO - Layer 11: Val Input Dataset size: 32\n",
      "2025-06-03 22:44:02,556 - __main__ - INFO - Layer 11: Test Input Dataset size: 32\n",
      "2025-06-03 22:44:02,558 - __main__ - INFO - Input feature dimension for VoxelProbe at layer 11: 18624 (Shape of sample: torch.Size([24, 776]))\n",
      "2025-06-03 22:44:02,558 - __main__ - INFO - Running VoxelProbe on layer 11...\n",
      "2025-06-03 22:44:02,558 - __main__ - INFO - Running VoxelProbe on layer 11 (input_feature_dim: 18624)\n",
      "2025-06-03 22:44:26,928 - src.probing.probes - INFO - Epoch 1/1 Lyr:11 Prb:voxel - Train Loss: 1.4014, Val Loss: 1.0866\n",
      "2025-06-03 22:44:26,928 - src.probing.probes - INFO - Epoch 1/1 Lyr:11 Prb:voxel - Train Loss: 1.4014, Val Loss: 1.0866\n",
      "2025-06-03 22:44:28,436 - __main__ - INFO - Saved VoxelProbe for layer 11 to cache/phase1_dinov2_voxel_reconstruction_reconstruction/probes/dinov2_voxel_layer_11_probe.pth\n",
      "2025-06-03 22:44:28,436 - __main__ - INFO - Saved VoxelProbe for layer 11 to cache/phase1_dinov2_voxel_reconstruction_reconstruction/probes/dinov2_voxel_layer_11_probe.pth\n",
      "2025-06-03 22:44:32,145 - src.probing.probes - INFO - Test Metrics (Lyr:11 Prb:voxel): {'loss': 0.9978461265563965, 'voxel_iou': 0.06024046987295151, 'voxel_precision': 0.08234992623329163, 'voxel_recall': 0.2457302212715149, 'voxel_f1': 0.11184008419513702}\n",
      "2025-06-03 22:44:32,145 - src.probing.probes - INFO - Test Metrics (Lyr:11 Prb:voxel): {'loss': 0.9978461265563965, 'voxel_iou': 0.06024046987295151, 'voxel_precision': 0.08234992623329163, 'voxel_recall': 0.2457302212715149, 'voxel_f1': 0.11184008419513702}\n",
      "Computing detailed metrics: 100%|██████████| 1/1 [00:03<00:00,  3.53s/it]\n",
      "Processing Layers: 100%|██████████| 2/2 [06:21<00:00, 190.87s/it]\n",
      "\n",
      "Processing Layers: 100%|██████████| 2/2 [06:21<00:00, 190.87s/it]\n"
     ]
    }
   ],
   "source": [
    "if experiment:\n",
    "    reconstruction_results = {}\n",
    "\n",
    "    if \"voxel\" not in cfg.probing.probe_types:\n",
    "        logger.error(f\"'voxel' not in configured probe_types: {cfg.probing.probe_types}. VoxelProbes will not be trained.\")\n",
    "    else:\n",
    "        logger.info(f\"Will train VoxelProbes for layers: {layers_to_probe}\")\n",
    "\n",
    "        for layer_idx in tqdm(layers_to_probe, desc=\"Processing Layers\"):\n",
    "            logger.info(f\"Processing layer {layer_idx} for reconstruction...\")\n",
    "\n",
    "            train_input_ds, val_input_ds, test_input_ds = experiment.prepare_reconstruction_input_datasets_for_layer(\n",
    "                reconstruction_pipeline=reconstruction_pipeline,\n",
    "                train_source_loader=train_source_loader,\n",
    "                val_source_loader=val_source_loader,\n",
    "                test_source_loader=test_source_loader,\n",
    "                layer=layer_idx,\n",
    "                image_feature_type=image_feature_type,\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Layer {layer_idx}: Train Input Dataset size: {len(train_input_ds)}\")\n",
    "            logger.info(f\"Layer {layer_idx}: Val Input Dataset size: {len(val_input_ds)}\")\n",
    "            logger.info(f\"Layer {layer_idx}: Test Input Dataset size: {len(test_input_ds)}\")\n",
    "\n",
    "            # Prepare DataLoaders from processed datasets\n",
    "            processed_batch_size = cfg.probing.get(\"training\", {}).get(\"batch_size\", 32)\n",
    "            train_processed_loader = DataLoader(\n",
    "                train_input_ds,\n",
    "                batch_size=processed_batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=cfg.get(\"num_workers\", 4),\n",
    "                pin_memory=True,\n",
    "            )\n",
    "            val_processed_loader = DataLoader(\n",
    "                val_input_ds,\n",
    "                batch_size=processed_batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=cfg.get(\"num_workers\", 4),\n",
    "                pin_memory=True,\n",
    "            )\n",
    "            test_processed_loader = DataLoader(\n",
    "                test_input_ds,\n",
    "                batch_size=processed_batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=cfg.get(\"num_workers\", 4),\n",
    "                pin_memory=True,\n",
    "            )\n",
    "\n",
    "            # Inspect a sample to determine input feature dimension\n",
    "            sample_processed_data = train_input_ds[0][\"processed_views\"]\n",
    "            input_feature_dim_for_probe = sample_processed_data.numel()\n",
    "            logger.info(f\"Input feature dimension for VoxelProbe at layer {layer_idx}: {input_feature_dim_for_probe} (Shape of sample: {sample_processed_data.shape})\")\n",
    "\n",
    "            logger.info(f\"Running VoxelProbe on layer {layer_idx}...\")\n",
    "            probe_run_results = experiment.run_voxel_probe_experiment(\n",
    "                train_processed_loader=train_processed_loader,\n",
    "                val_processed_loader=val_processed_loader,\n",
    "                test_processed_loader=test_processed_loader,\n",
    "                feature_dim=input_feature_dim_for_probe,\n",
    "                layer=layer_idx,\n",
    "            )\n",
    "            reconstruction_results[f\"layer_{layer_idx}\"] = {\"voxel\": probe_run_results}\n",
    "\n",
    "else:\n",
    "    logger.error(\"Experiment not initialized. Skipping layer processing and probe training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203594fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:44:35,768 - __main__ - INFO - Saving reconstruction experiment results...\n",
      "2025-06-03 22:44:35,773 - __main__ - INFO - Reconstruction results saved to results/phase1_dinov2_voxel_reconstruction_reconstruction/reconstruction_results.json\n",
      "2025-06-03 22:44:35,773 - __main__ - INFO - Results saved to: results/phase1_dinov2_voxel_reconstruction_reconstruction/reconstruction_results.json\n",
      "2025-06-03 22:44:35,773 - __main__ - INFO - Reconstruction results saved to results/phase1_dinov2_voxel_reconstruction_reconstruction/reconstruction_results.json\n",
      "2025-06-03 22:44:35,773 - __main__ - INFO - Results saved to: results/phase1_dinov2_voxel_reconstruction_reconstruction/reconstruction_results.json\n"
     ]
    }
   ],
   "source": [
    "if experiment and reconstruction_results:\n",
    "    logger.info(\"Saving reconstruction experiment results...\")\n",
    "    result_file_path = experiment.save_results(reconstruction_results)\n",
    "    logger.info(f\"Results saved to: {result_file_path}\")\n",
    "else:\n",
    "    logger.warning(\"No results to save or experiment not run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0bcdeb",
   "metadata": {},
   "source": [
    "### Analyze and Visualize Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb12f33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:47:26,444 - __main__ - INFO - Analyzing reconstruction results...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'NoneType' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m experiment \u001b[38;5;129;01mand\u001b[39;00m reconstruction_results \u001b[38;5;129;01mand\u001b[39;00m result_file_path:\n\u001b[1;32m      4\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing reconstruction results...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[43manalyze_experiment_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresults_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Example metrics\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/+Programming/GitHub/LatentInvestigation/src/analysis/layer_analysis.py:508\u001b[0m, in \u001b[0;36manalyze_experiment_results\u001b[0;34m(results_file, output_dir)\u001b[0m\n\u001b[1;32m    504\u001b[0m plot_files \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39mcreate_plots(layer_results, output_dir)\n\u001b[1;32m    505\u001b[0m analysis[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplot_files\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m plot_files]\n\u001b[1;32m    507\u001b[0m report_file \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39msave_analysis_report(\n\u001b[0;32m--> 508\u001b[0m     layer_results, \u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer_analysis_report.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    509\u001b[0m )\n\u001b[1;32m    510\u001b[0m analysis[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreport_file\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(report_file)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m analysis\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'NoneType' and 'str'"
     ]
    }
   ],
   "source": [
    "from src.analysis.layer_analysis import analyze_experiment_results\n",
    "\n",
    "if experiment and reconstruction_results and result_file_path:\n",
    "    logger.info(\"Analyzing reconstruction results...\")\n",
    "    \n",
    "    analyze_experiment_results(\n",
    "        results_file=result_file_path, \n",
    "         # Example metrics\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LatentInvestigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
