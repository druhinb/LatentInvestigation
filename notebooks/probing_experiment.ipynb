{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baad35b8",
   "metadata": {},
   "source": [
    "# Probing Experiment on SSL Models\n",
    "\n",
    "This is effectively a notebook-ized version of the old experiment runner script. It compartmentalizes everything so we don't lose state between small errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22bdeb2",
   "metadata": {},
   "source": [
    "### Imports, Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f0a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables before imports\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "# Imports\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import wandb\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from src.models.feature_extractor import FeatureExtractor, load_feature_extractor\n",
    "from src.datasets.shapenet_3dr2n2 import create_3dr2n2_dataloaders\n",
    "from src.probing.probes import create_probe, ProbeTrainer\n",
    "from src.probing.data_preprocessing import (\n",
    "    FeatureExtractorPipeline,\n",
    "    create_probing_dataloaders,\n",
    "    ProbingDataset,\n",
    ")\n",
    "from src.probing.metrics import (\n",
    "    compute_regression_metrics,\n",
    "    compute_viewpoint_specific_metrics,\n",
    "    MetricsTracker,\n",
    ")\n",
    "from src.analysis.layer_analysis import LayerWiseAnalyzer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa8a3bb",
   "metadata": {},
   "source": [
    "### Probing Setup\n",
    "This class is the overarching \"manager\" that is responsible for the entire experiment. It contains all the functionalities required to:\n",
    "\n",
    "- Create & setup dataloaders \n",
    "- Extract features from the frozen layers of the ViT models \n",
    "- Train MLP & Linear probes on those layers \n",
    "- Summarize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3034b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbingExperiment:\n",
    "    \"\"\"Orchestrates probing experiments\"\"\"\n",
    "\n",
    "    def __init__(self, config: DictConfig):\n",
    "        self.config = config\n",
    "        # Determine device: prioritize models.device, then top-level device, then auto-detect\n",
    "        device_to_use = config.models.get(\"device\", config.get(\"device\"))\n",
    "        if device_to_use:\n",
    "            self.device = device_to_use\n",
    "        else:\n",
    "            self.device = (\n",
    "                \"cuda\"\n",
    "                if torch.cuda.is_available()\n",
    "                else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "            )\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Initialize wandb\n",
    "        if config.get(\"wandb\", {}).get(\"enabled\", False):\n",
    "            wandb.init(\n",
    "                project=config.wandb.project,\n",
    "                entity=config.wandb.get(\"entity\"),\n",
    "                name=config.experiment.name,\n",
    "                config=OmegaConf.to_container(config, resolve=True),\n",
    "            )\n",
    "\n",
    "        # Setup paths\n",
    "        self.results_dir = Path(config.get(\"results_dir\", \"./results\"))\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.cache_dir = Path(config.get(\"cache_dir\", \"./cache\"))\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Setup probe save directory\n",
    "        self.probe_save_dir = self.cache_dir / \"probes\" / self.config.experiment.name\n",
    "        self.probe_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Initialize analyzer\n",
    "        self.analyzer = LayerWiseAnalyzer(self.results_dir / config.experiment.name)\n",
    "\n",
    "   \n",
    "    def load_dataset(self) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        \"\"\"Load the dataset\"\"\"\n",
    "        subset_percentage = self.config.datasets.get(\"subset_percentage\", None)\n",
    "        return create_3dr2n2_dataloaders(\n",
    "            self.config.datasets, subset_percentage=subset_percentage\n",
    "        )\n",
    "\n",
    "    def load_feature_extractor(self) -> FeatureExtractor:\n",
    "        \"\"\"Load and setup feature extractor\"\"\"\n",
    "        model_config = self.config.models\n",
    "        model_config.device = self.device\n",
    "        model_config.cache_dir = str(self.cache_dir / \"models\")\n",
    "\n",
    "        feature_extractor = load_feature_extractor(OmegaConf.to_container(model_config))\n",
    "        logger.info(f\"Loaded {model_config.model_name} feature extractor\")\n",
    "        return feature_extractor\n",
    "\n",
    "    def extract_features_for_layer(\n",
    "        self,\n",
    "        feature_extractor: FeatureExtractor,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        test_loader: DataLoader,\n",
    "        layer: int,\n",
    "        feature_type: str,\n",
    "        task_type: str,\n",
    "    ) -> Tuple[ProbingDataset, ProbingDataset, ProbingDataset]:\n",
    "        \"\"\"Extract features for a specific layer\"\"\"\n",
    "        pipeline = FeatureExtractorPipeline(\n",
    "            feature_extractor=feature_extractor,\n",
    "            device=self.device,\n",
    "            batch_size=self.config.get(\"extraction_batch_size\", 32),\n",
    "            cache_dir=str(self.cache_dir / \"features\"),\n",
    "        )\n",
    "\n",
    "        experiment_name = f\"{self.config.models.model_name}_{self.config.experiment.name}_layer_{layer}\"\n",
    "\n",
    "        return pipeline.create_probing_datasets(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "            layers=[layer],\n",
    "            feature_type=feature_type,\n",
    "            task_type=task_type,\n",
    "            experiment_name=experiment_name,\n",
    "        )\n",
    "\n",
    "    def run_probe_experiment(\n",
    "        self,\n",
    "        probe_type: str,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        test_loader: DataLoader,\n",
    "        feature_dim: int,\n",
    "        layer: int,\n",
    "    ) -> Dict:\n",
    "        \"\"\"Run a single probe experiment\"\"\"\n",
    "\n",
    "        logger.info(\n",
    "            f\"Running {probe_type} probe on layer {layer} (feature_dim: {feature_dim})\"\n",
    "        )\n",
    "\n",
    "        # Get probe configuration\n",
    "        probe_config = self.config.probing.get(probe_type, {})\n",
    "        # Make a mutable copy for modification\n",
    "        probe_config = OmegaConf.to_container(probe_config, resolve=True)\n",
    "\n",
    "        # Create probe\n",
    "        probe_config[\"input_dim\"] = feature_dim\n",
    "        probe_config[\"output_dim\"] = self.config.probing.get(\"output_dim\", 2)\n",
    "\n",
    "        main_task_type = self.config.probing.get(\"task_type\", \"regression\")\n",
    "        if main_task_type == \"viewpoint_regression\":\n",
    "            probe_config[\"task_type\"] = \"regression\"\n",
    "        elif main_task_type == \"view_classification\":\n",
    "            probe_config[\"task_type\"] = \"classification\"\n",
    "        else:\n",
    "            probe_config[\"task_type\"] = main_task_type\n",
    "\n",
    "        probe = create_probe(probe_config)\n",
    "\n",
    "        # Setup trainer\n",
    "        trainer = ProbeTrainer(probe, device=self.device)\n",
    "\n",
    "        # Setup optimizer and scheduler\n",
    "        training_config = probe_config.get(\"training\", {})\n",
    "        optimizer = self.create_optimizer(probe, training_config.get(\"optimizer\", {}))\n",
    "        scheduler = self.create_scheduler(\n",
    "            optimizer, training_config.get(\"scheduler\", {})\n",
    "        )\n",
    "\n",
    "        # Training parameters\"results/phase1_dinov2_viewpoint_probing/results.json\"\n",
    "        epochs = training_config.get(\"epochs\", 30)\n",
    "        early_stopping_patience = training_config.get(\"early_stopping_patience\", 15)\n",
    "\n",
    "        metrics_tracker = MetricsTracker()\n",
    "        trainer = ProbeTrainer(\n",
    "            probe, device=self.device, MetricsTracker=metrics_tracker\n",
    "        )\n",
    "\n",
    "        # Check if wandb is enabled\n",
    "        wandb_enabled = self.config.get(\"wandb\", {}).get(\"enabled\", False)\n",
    "\n",
    "        best_model, best_val_loss = trainer.train(\n",
    "            epochs,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            early_stopping_patience,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            probe_type=probe_type,\n",
    "            layer=layer,\n",
    "            wandb_enabled=wandb_enabled,\n",
    "        )\n",
    "        \n",
    "        # Save the trained probe\n",
    "        probe_save_dir = self.cache_dir / \"probes\" / self.config.experiment.name\n",
    "        probe_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        probe_filename = f\"{probe_type}_layer_{layer}_probe.pth\"\n",
    "        probe_save_path = probe_save_dir / probe_filename\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': best_model,  # best_model is already a state_dict\n",
    "            'probe_config': probe_config,\n",
    "            'layer': layer,\n",
    "            'probe_type': probe_type,\n",
    "            'experiment_name': self.config.experiment.name,\n",
    "            'model_name': self.config.models.model_name,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'feature_dim': feature_dim\n",
    "        }, probe_save_path)\n",
    "        \n",
    "        logger.info(f\"Saved {probe_type} probe for layer {layer} to {probe_save_path}\")\n",
    "\n",
    "        test_metrics = trainer.evaluate(test_loader)\n",
    "\n",
    "        detailed_metrics = self.compute_detailed_metrics(probe, test_loader)\n",
    "\n",
    "        total_epochs = len(metrics_tracker.get_history(\"train\"))\n",
    "\n",
    "        results = {\n",
    "            \"train_history\": metrics_tracker.get_history(\"train\"),\n",
    "            \"val_history\": metrics_tracker.get_history(\"val\"),\n",
    "            \"test_metrics\": test_metrics,\n",
    "            \"detailed_metrics\": detailed_metrics,\n",
    "            \"best_epoch\": metrics_tracker.best_epoch,\n",
    "            \"total_epochs\": total_epochs,\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_probe(self, probe: nn.Module, probe_type: str, layer: int, probe_config: Dict):\n",
    "        \"\"\"Save the trained probe model and its configuration\"\"\"\n",
    "        import json\n",
    "        \n",
    "        # Create filename with model name, probe type, and layer\n",
    "        model_name = self.config.models.model_name\n",
    "        filename = f\"{model_name}_{probe_type}_layer_{layer}.pth\"\n",
    "        probe_path = self.probe_save_dir / filename\n",
    "        \n",
    "        # Save the probe state dict\n",
    "        torch.save({\n",
    "            'model_state_dict': probe.state_dict(),\n",
    "            'probe_config': probe_config,\n",
    "            'model_name': model_name,\n",
    "            'probe_type': probe_type,\n",
    "            'layer': layer,\n",
    "            'experiment_name': self.config.experiment.name\n",
    "        }, probe_path)\n",
    "        \n",
    "        # Also save the config as JSON\n",
    "        config_filename = f\"{model_name}_{probe_type}_layer_{layer}_config.json\"\n",
    "        config_path = self.probe_save_dir / config_filename\n",
    "        \n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'probe_config': probe_config,\n",
    "                'model_name': model_name,\n",
    "                'probe_type': probe_type,\n",
    "                'layer': layer,\n",
    "                'experiment_name': self.config.experiment.name\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Probe saved to {probe_path}\")\n",
    "        logger.info(f\"Probe config saved to {config_path}\")\n",
    "\n",
    "    def load_probe(self, probe_type: str, layer: int, device: Optional[str] = None) -> nn.Module:\n",
    "        \"\"\"Load a previously saved probe\"\"\"\n",
    "        if device is None:\n",
    "            device = self.device\n",
    "            \n",
    "        model_name = self.config.models.model_name\n",
    "        filename = f\"{model_name}_{probe_type}_layer_{layer}.pth\"\n",
    "        probe_path = self.probe_save_dir / filename\n",
    "        \n",
    "        if not probe_path.exists():\n",
    "            raise FileNotFoundError(f\"Probe not found at {probe_path}\")\n",
    "        \n",
    "        # Load the saved data\n",
    "        saved_data = torch.load(probe_path, map_location=device)\n",
    "        \n",
    "        # Recreate the probe using the saved config\n",
    "        probe_config = saved_data['probe_config']\n",
    "        probe = create_probe(probe_config)\n",
    "        \n",
    "        # Load the state dict\n",
    "        probe.load_state_dict(saved_data['model_state_dict'])\n",
    "        probe.to(device)\n",
    "        \n",
    "        logger.info(f\"Probe loaded from {probe_path}\")\n",
    "        return probe\n",
    "\n",
    "    def create_optimizer(\n",
    "        self, model: nn.Module, optimizer_config: Dict\n",
    "    ) -> torch.optim.Optimizer:\n",
    "        \"\"\"Create optimizer from config using Hydra instantiate\"\"\"\n",
    "        from hydra.utils import instantiate\n",
    "\n",
    "        # Create a copy of config and add model parameters\n",
    "        optimizer_config = optimizer_config.copy()\n",
    "        optimizer_config[\"params\"] = model.parameters()\n",
    "\n",
    "        return instantiate(optimizer_config)\n",
    "\n",
    "    def create_scheduler(\n",
    "        self, optimizer: torch.optim.Optimizer, scheduler_config: Dict\n",
    "    ):\n",
    "        \"\"\"Create learning rate scheduler from config using Hydra instantiate\"\"\"\n",
    "        if not scheduler_config:\n",
    "            return None\n",
    "\n",
    "        from hydra.utils import instantiate\n",
    "\n",
    "        scheduler_config = scheduler_config.copy()\n",
    "        scheduler_config[\"optimizer\"] = optimizer\n",
    "\n",
    "        return instantiate(scheduler_config)\n",
    "\n",
    "    def compute_detailed_metrics(\n",
    "        self, probe: nn.Module, test_loader: DataLoader\n",
    "    ) -> Dict:\n",
    "        \"\"\"Compute alles metrics\"\"\"\n",
    "        probe.eval()\n",
    "\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        all_categories = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                features = batch[\"features\"].to(self.device)\n",
    "                targets = batch[\"targets\"]\n",
    "\n",
    "                outputs = probe(features)\n",
    "\n",
    "                all_predictions.append(outputs.cpu())\n",
    "                all_targets.append(targets)\n",
    "\n",
    "                # Get categories if available\n",
    "                if \"categories\" in batch:\n",
    "                    all_categories.extend(batch[\"categories\"])\n",
    "\n",
    "        predictions = torch.cat(all_predictions, dim=0)\n",
    "        targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "        # Basic regression metrics\n",
    "        metrics = compute_regression_metrics(predictions, targets, return_per_dim=True)\n",
    "\n",
    "        # Viewpoint-specific metrics\n",
    "        if predictions.shape[1] == 2:\n",
    "            viewpoint_metrics = compute_viewpoint_specific_metrics(\n",
    "                azimuth_pred=predictions[:, 0],\n",
    "                elevation_pred=predictions[:, 1],\n",
    "                azimuth_target=targets[:, 0],\n",
    "                elevation_target=targets[:, 1],\n",
    "            )\n",
    "            metrics.update(viewpoint_metrics)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def save_results(self, results: Dict) -> str:\n",
    "        \"\"\"Save results to disk\"\"\"\n",
    "        import json\n",
    "\n",
    "        # Create experiment directory\n",
    "        exp_dir = self.results_dir / self.config.experiment.name\n",
    "        exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save results\n",
    "        results_file = exp_dir / \"results.json\"\n",
    "\n",
    "        # Convert tensors to lists for JSON serialization\n",
    "        serializable_results = self.make_json_serializable(results)\n",
    "\n",
    "        combined_results = {\n",
    "            \"config\": OmegaConf.to_container(self.config, resolve=True),\n",
    "            \"results\": serializable_results,\n",
    "        }\n",
    "\n",
    "        with open(results_file, \"w\") as f:\n",
    "            json.dump(combined_results, f, indent=2)\n",
    "\n",
    "        logger.info(f\"Results saved to {results_file}\")\n",
    "        return results_file\n",
    "\n",
    "    def make_json_serializable(self, obj):\n",
    "        \"\"\"Convert object to JSON-serializable format\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: self.make_json_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self.make_json_serializable(v) for v in obj]\n",
    "        elif isinstance(obj, (torch.Tensor, np.ndarray)):\n",
    "            return obj.tolist() if hasattr(obj, \"tolist\") else float(obj)\n",
    "        elif isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        else:\n",
    "            return obj\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb324d",
   "metadata": {},
   "source": [
    "### Hydra Configuration Loading / Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3296c249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 20:37:26,470 - __main__ - INFO - Initializing Hydra with config_path: '../configs'\n",
      "2025-06-01 20:37:26,656 - __main__ - INFO - Composing configuration with config_name: 'experiment_config'\n",
      "2025-06-01 20:37:26,716 - __main__ - INFO - Hydra configuration loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "import os \n",
    "from pathlib import Path #\n",
    "\n",
    "CONFIG_PATH = \"../configs\"\n",
    "CONFIG_NAME = \"experiment_config\"\n",
    "\n",
    "cfg: Optional[DictConfig] = None\n",
    "\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "    logger.info(\"Clearing existing Hydra global state.\")\n",
    "    GlobalHydra.instance().clear()\n",
    "\n",
    "try:\n",
    "    project_root = Path(os.getcwd()).parent \n",
    "    data_dir_abs = project_root / \"data\"\n",
    "    \n",
    "    os.environ[\"DATA_DIR\"] = str(data_dir_abs)\n",
    "\n",
    "    logger.info(f\"Initializing Hydra with config_path: '{CONFIG_PATH}'\")\n",
    "    \n",
    "    initialize(version_base=None, config_path=CONFIG_PATH)\n",
    "    \n",
    "    logger.info(f\"Composing configuration with config_name: '{CONFIG_NAME}'\")\n",
    "    \n",
    "    cfg = compose(config_name=CONFIG_NAME)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing Hydra or loading configuration: {e}\", exc_info=True)\n",
    "\n",
    "if cfg:\n",
    "    logger.info(\"Hydra configuration loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d9168",
   "metadata": {},
   "source": [
    "## Running the Experiment\n",
    "The following code uses the above configurations and utility functions to run the actual experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69efd063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 20:37:26,724 - __main__ - INFO - Starting experiment execution\n",
      "2025-06-01 20:37:26,725 - __main__ - INFO - Using device: mps\n",
      "2025-06-01 20:37:27,176 - wandb.jupyter - ERROR - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdruhin-bhowal\u001b[0m (\u001b[33mcse493g1_drn\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/druhi/Documents/+Programming/GitHub/LatentInvestigation/notebooks/wandb/run-20250601_203728-b3874460</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cse493g1_drn/latent-investigation/runs/b3874460' target=\"_blank\">phase1_dinov2_viewpoint_probing</a></strong> to <a href='https://wandb.ai/cse493g1_drn/latent-investigation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cse493g1_drn/latent-investigation' target=\"_blank\">https://wandb.ai/cse493g1_drn/latent-investigation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cse493g1_drn/latent-investigation/runs/b3874460' target=\"_blank\">https://wandb.ai/cse493g1_drn/latent-investigation/runs/b3874460</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = None\n",
    "logger.info(\"Starting experiment execution\")\n",
    "experiment = ProbingExperiment(cfg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117dade8",
   "metadata": {},
   "source": [
    "### Load the Feature Extractor & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df34a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 20:37:31,287 - src.models.feature_extractor - INFO - Loaded dinov2 model on mps\n",
      "2025-06-01 20:37:31,287 - __main__ - INFO - Loaded dinov2 feature extractor\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = experiment.load_feature_extractor()\n",
    "extraction_config = cfg.models.get(\"feature_extraction\", {})\n",
    "layers = extraction_config.get(\"layers\", [11])\n",
    "feature_type = extraction_config.get(\"feature_type\", \"cls_token\")\n",
    "task_type = cfg.probing.get(\"task_type\", \"viewpoint_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "713ca504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30648/30648 [00:30<00:00, 995.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0.05% of train data: 367 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6567/6567 [00:06<00:00, 1000.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0.05% of val data: 78 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6569/6569 [00:06<00:00, 1012.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0.05% of test data: 78 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = experiment.load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642e0c9",
   "metadata": {},
   "source": [
    "### Train the Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c89e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]2025-06-01 20:38:15,277 - __main__ - INFO - Processing layer 2...\n",
      "2025-06-01 20:38:15,279 - src.probing.data_preprocessing - INFO - Loading cached features from cache/features/dinov2_phase1_dinov2_viewpoint_probing_layer_2_train.pkl\n",
      "2025-06-01 20:38:15,283 - src.probing.data_preprocessing - INFO - TRAIN dataset: 352 samples\n",
      "2025-06-01 20:38:15,285 - src.probing.data_preprocessing - INFO - Loading cached features from cache/features/dinov2_phase1_dinov2_viewpoint_probing_layer_2_val.pkl\n",
      "2025-06-01 20:38:15,287 - src.probing.data_preprocessing - INFO - VAL dataset: 78 samples\n",
      "2025-06-01 20:38:15,288 - src.probing.data_preprocessing - INFO - Loading cached features from cache/features/dinov2_phase1_dinov2_viewpoint_probing_layer_2_test.pkl\n",
      "2025-06-01 20:38:15,292 - src.probing.data_preprocessing - INFO - TEST dataset: 78 samples\n",
      "2025-06-01 20:38:15,292 - __main__ - INFO - Running linear probe on layer 2...\n",
      "2025-06-01 20:38:15,293 - __main__ - INFO - Running linear probe on layer 2 (feature_dim: 768)\n",
      "Training 1/1: 100%|██████████| 1/1 [00:03<00:00,  3.59s/it]\n",
      "2025-06-01 20:38:21,315 - __main__ - INFO - Saved linear probe for layer 2 to cache/probes/phase1_dinov2_viewpoint_probing/linear_layer_2_probe.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss=0.2296, val_loss=0.2422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 20:38:26,097 - __main__ - INFO - Running mlp probe on layer 2...\n",
      "2025-06-01 20:38:26,098 - __main__ - INFO - Running mlp probe on layer 2 (feature_dim: 768)\n",
      "Training 1/1: 100%|██████████| 1/1 [00:02<00:00,  2.40s/it]\n",
      "2025-06-01 20:38:30,786 - __main__ - INFO - Saved mlp probe for layer 2 to cache/probes/phase1_dinov2_viewpoint_probing/mlp_layer_2_probe.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss=0.2306, val_loss=0.2035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:20<00:20, 20.03s/it]2025-06-01 20:38:35,304 - __main__ - INFO - Processing layer 11...\n",
      "2025-06-01 20:38:35,306 - src.probing.data_preprocessing - INFO - Loading cached features from cache/features/dinov2_phase1_dinov2_viewpoint_probing_layer_11_train.pkl\n",
      "2025-06-01 20:38:35,308 - src.probing.data_preprocessing - INFO - TRAIN dataset: 352 samples\n",
      "2025-06-01 20:38:35,309 - src.probing.data_preprocessing - INFO - Loading cached features from cache/features/dinov2_phase1_dinov2_viewpoint_probing_layer_11_val.pkl\n",
      "2025-06-01 20:38:35,311 - src.probing.data_preprocessing - INFO - VAL dataset: 78 samples\n",
      "2025-06-01 20:38:35,311 - src.probing.data_preprocessing - INFO - Loading cached features from cache/features/dinov2_phase1_dinov2_viewpoint_probing_layer_11_test.pkl\n",
      "2025-06-01 20:38:35,312 - src.probing.data_preprocessing - INFO - TEST dataset: 78 samples\n",
      "2025-06-01 20:38:35,313 - __main__ - INFO - Running linear probe on layer 11...\n",
      "2025-06-01 20:38:35,313 - __main__ - INFO - Running linear probe on layer 11 (feature_dim: 768)\n",
      "Training 1/1: 100%|██████████| 1/1 [00:02<00:00,  2.16s/it]\n",
      "2025-06-01 20:38:39,715 - __main__ - INFO - Saved linear probe for layer 11 to cache/probes/phase1_dinov2_viewpoint_probing/linear_layer_11_probe.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss=0.7528, val_loss=0.5066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 20:38:44,199 - __main__ - INFO - Running mlp probe on layer 11...\n",
      "2025-06-01 20:38:44,200 - __main__ - INFO - Running mlp probe on layer 11 (feature_dim: 768)\n",
      "Training 1/1: 100%|██████████| 1/1 [00:02<00:00,  2.28s/it]\n",
      "2025-06-01 20:38:48,747 - __main__ - INFO - Saved mlp probe for layer 11 to cache/probes/phase1_dinov2_viewpoint_probing/mlp_layer_11_probe.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss=0.8335, val_loss=1.2173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:37<00:00, 18.99s/it]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for layer in tqdm(layers):\n",
    "    logger.info(f\"Processing layer {layer}...\")\n",
    "\n",
    "    # Extract features for this layer\n",
    "    train_dataset, val_dataset, test_dataset = experiment.extract_features_for_layer(\n",
    "        feature_extractor,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        layer,\n",
    "        feature_type,\n",
    "        task_type,\n",
    "    )\n",
    "\n",
    "    # Create probing dataloaders\n",
    "    probe_train_loader, probe_val_loader, probe_test_loader = (\n",
    "       create_probing_dataloaders(\n",
    "            train_dataset,\n",
    "            val_dataset,\n",
    "            test_dataset,\n",
    "            batch_size=cfg.probing.get(\"training\", {}).get(\n",
    "                \"batch_size\", 64\n",
    "            ),\n",
    "            num_workers=cfg.get(\"num_workers\", 4),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Run probing experiments for each probe type\n",
    "    layer_results = {}\n",
    "    for probe_type in cfg.probing.probe_types:\n",
    "        logger.info(f\"Running {probe_type} probe on layer {layer}...\")\n",
    "        probe_results = experiment.run_probe_experiment(\n",
    "            probe_type,\n",
    "            probe_train_loader,\n",
    "            probe_val_loader,\n",
    "            probe_test_loader,\n",
    "            train_dataset.features.shape[1],\n",
    "            layer,\n",
    "        )\n",
    "        layer_results[probe_type] = probe_results\n",
    "\n",
    "    results[f\"layer_{layer}\"] = layer_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "829f1822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 20:38:53,268 - __main__ - INFO - Saving results...\n",
      "2025-06-01 20:38:53,272 - __main__ - INFO - Results saved to results/phase1_dinov2_viewpoint_probing/results.json\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Saving results...\")\n",
    "result_path = experiment.save_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706a05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 20:38:53,288 - __main__ - INFO - Creating analysis and visualizations...\n",
      "2025-06-01 20:38:55,227 - src.analysis.layer_analysis - INFO - Analysis report saved to analysis_results/layer_analysis_report.json\n",
      "2025-06-01 20:38:55,228 - __main__ - INFO - Results analyzed! Please see the results and analysis_results folders for the outcomes.\n"
     ]
    }
   ],
   "source": [
    "from src.analysis.layer_analysis import analyze_experiment_results\n",
    "\n",
    "logger.info(\"Creating analysis and visualizations...\")\n",
    "analyze_experiment_results(result_path, output_dir=result_path.parent)\n",
    "\n",
    "logger.info(\"Results analyzed! Please see the results and analysis_results folders for the outcomes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LatentInvestigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
